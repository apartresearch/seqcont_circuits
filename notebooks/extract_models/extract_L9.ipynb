{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["DcZG9rm2IAiA","OLkInsdjyHMx","-cGX7o8ZpXIH","wHV6r-JTFPRh"],"machine_shape":"hm","gpuType":"T4","authorship_tag":"ABX9TyMc89pBzsEaioxhvdz44kCI"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"b5a832faf53040a193929f5f345922cb":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0d2a5621b21a4ec79ee197d5f587dce7","IPY_MODEL_3a6bbd20249247ba83121be80fc66acb","IPY_MODEL_cead0244853e43a489cba71bec0f8aeb"],"layout":"IPY_MODEL_2cb1d73fd408451f8cf449863450803b"}},"0d2a5621b21a4ec79ee197d5f587dce7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7649a3d236f842f1acd8d1ddab59e7a8","placeholder":"​","style":"IPY_MODEL_a599632bef994519817ce2b5de350268","value":"Downloading (…)lve/main/config.json: 100%"}},"3a6bbd20249247ba83121be80fc66acb":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_02496c327c904410a46c5dc64cc9abd8","max":665,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e1a5dbaaae1848889a135b1f6c2bc42b","value":665}},"cead0244853e43a489cba71bec0f8aeb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6a3d7e5322be436e89cd67aff42f0c37","placeholder":"​","style":"IPY_MODEL_bb0f52c7e10c454b96a3cd62392f9e0f","value":" 665/665 [00:00&lt;00:00, 52.1kB/s]"}},"2cb1d73fd408451f8cf449863450803b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7649a3d236f842f1acd8d1ddab59e7a8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a599632bef994519817ce2b5de350268":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"02496c327c904410a46c5dc64cc9abd8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e1a5dbaaae1848889a135b1f6c2bc42b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6a3d7e5322be436e89cd67aff42f0c37":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bb0f52c7e10c454b96a3cd62392f9e0f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"410eabe330234f7a96767b0c0572474f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_714762a589d34ef687171800050ffc0e","IPY_MODEL_5191de2f5fa045adb9e201b658d70dfa","IPY_MODEL_14363dca02714b96a0194a87f20e900d"],"layout":"IPY_MODEL_5d75369794a343618337f50835a1ea1c"}},"714762a589d34ef687171800050ffc0e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2a698b84478840f2932bdc361f0023c9","placeholder":"​","style":"IPY_MODEL_dc5ac7807dd447669295d06b71b0cadc","value":"Downloading model.safetensors: 100%"}},"5191de2f5fa045adb9e201b658d70dfa":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_88641295b608453bb46b8c1738a96127","max":548105171,"min":0,"orientation":"horizontal","style":"IPY_MODEL_250ae5aadbc749338c027c1ea761634f","value":548105171}},"14363dca02714b96a0194a87f20e900d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f550ab4994144463a93e8604eee4b5e7","placeholder":"​","style":"IPY_MODEL_ad59ddeee53a4009b890ee4920ca5146","value":" 548M/548M [00:01&lt;00:00, 423MB/s]"}},"5d75369794a343618337f50835a1ea1c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2a698b84478840f2932bdc361f0023c9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dc5ac7807dd447669295d06b71b0cadc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"88641295b608453bb46b8c1738a96127":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"250ae5aadbc749338c027c1ea761634f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f550ab4994144463a93e8604eee4b5e7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ad59ddeee53a4009b890ee4920ca5146":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"81a654ddef954fef87620fe2780926bd":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_57e8d6d28a1d4401bd7c3b3f919bc318","IPY_MODEL_51f68aa5caea48d89fa32dcd834393c2","IPY_MODEL_dba37d2e75e04478bcda6fd4fdce0ab3"],"layout":"IPY_MODEL_243516e1d15840d19c32a3bd305e5c76"}},"57e8d6d28a1d4401bd7c3b3f919bc318":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c6bf049689c6470cb5999d18e652f053","placeholder":"​","style":"IPY_MODEL_744b86a4691442e79ab2357c41a56455","value":"Downloading (…)neration_config.json: 100%"}},"51f68aa5caea48d89fa32dcd834393c2":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_fca913b610a0466c90c2a13c3f2b6757","max":124,"min":0,"orientation":"horizontal","style":"IPY_MODEL_219d84a5bb9f4ae289d8d54e659b8716","value":124}},"dba37d2e75e04478bcda6fd4fdce0ab3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_68304dcb56464d99bc8cdc40b400fc71","placeholder":"​","style":"IPY_MODEL_fb4657de28954b0caca5b5e4a9ebf686","value":" 124/124 [00:00&lt;00:00, 7.73kB/s]"}},"243516e1d15840d19c32a3bd305e5c76":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c6bf049689c6470cb5999d18e652f053":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"744b86a4691442e79ab2357c41a56455":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fca913b610a0466c90c2a13c3f2b6757":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"219d84a5bb9f4ae289d8d54e659b8716":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"68304dcb56464d99bc8cdc40b400fc71":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fb4657de28954b0caca5b5e4a9ebf686":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6117cbc508b64d93a6e090af43c5abee":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a1508e16d67f4ece96064386a908ac7a","IPY_MODEL_ccfc134c3185462b8bd3eb97e6851027","IPY_MODEL_e1c2e1dbfcf043b8ba25e77feed13646"],"layout":"IPY_MODEL_69ed6dcc37394f5fb7a115412dde27f7"}},"a1508e16d67f4ece96064386a908ac7a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_353508c2ea1b4421875e5d696d1163b6","placeholder":"​","style":"IPY_MODEL_48624782bebe44bfb90ebd49dc8213db","value":"Downloading (…)olve/main/vocab.json: 100%"}},"ccfc134c3185462b8bd3eb97e6851027":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3b2008e3c29d437494ce4f19d7f47144","max":1042301,"min":0,"orientation":"horizontal","style":"IPY_MODEL_463a163353794c7fbf20cf2ada338cc7","value":1042301}},"e1c2e1dbfcf043b8ba25e77feed13646":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dab2cafa9ea14571afc50f52a47b1028","placeholder":"​","style":"IPY_MODEL_5ee661b9c841457a870fc2364e2656c0","value":" 1.04M/1.04M [00:00&lt;00:00, 3.22MB/s]"}},"69ed6dcc37394f5fb7a115412dde27f7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"353508c2ea1b4421875e5d696d1163b6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"48624782bebe44bfb90ebd49dc8213db":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3b2008e3c29d437494ce4f19d7f47144":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"463a163353794c7fbf20cf2ada338cc7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"dab2cafa9ea14571afc50f52a47b1028":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5ee661b9c841457a870fc2364e2656c0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c5fa55e3d4fb493c916e4226e7355aba":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_88bbf8b2e475416fbcc6e562452b8675","IPY_MODEL_6bb7af85068246178e046e92b53770da","IPY_MODEL_7053b48b744b438ca893e01c102bbc31"],"layout":"IPY_MODEL_3b46008e5bca44e6a32c365a2bd3ba58"}},"88bbf8b2e475416fbcc6e562452b8675":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d549ff7af825419191218fc57adedf1e","placeholder":"​","style":"IPY_MODEL_8d5f9660f71c45a5a36c201e3e17b079","value":"Downloading (…)olve/main/merges.txt: 100%"}},"6bb7af85068246178e046e92b53770da":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0f9dce8a927644afad721b41946c1036","max":456318,"min":0,"orientation":"horizontal","style":"IPY_MODEL_613cf9674273438bbee12ab754208ca7","value":456318}},"7053b48b744b438ca893e01c102bbc31":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5a9d4a1b2e0c4bcd9cb190e597cf2a42","placeholder":"​","style":"IPY_MODEL_8fa601ecb1224b3c966f875172db6a6d","value":" 456k/456k [00:00&lt;00:00, 18.4MB/s]"}},"3b46008e5bca44e6a32c365a2bd3ba58":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d549ff7af825419191218fc57adedf1e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8d5f9660f71c45a5a36c201e3e17b079":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0f9dce8a927644afad721b41946c1036":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"613cf9674273438bbee12ab754208ca7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5a9d4a1b2e0c4bcd9cb190e597cf2a42":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8fa601ecb1224b3c966f875172db6a6d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a243c60c0d41497ba05ca1873e917ee0":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_969f960c9b524449984d2adec1fad898","IPY_MODEL_689356fe124f4314acb4897c22b38d73","IPY_MODEL_fb5da7a8fa914b2dbb27dad514c547d4"],"layout":"IPY_MODEL_66deb99b8c174922bd0fc3b4c341811d"}},"969f960c9b524449984d2adec1fad898":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cc3d920647f4459eba06c7453968e2ba","placeholder":"​","style":"IPY_MODEL_02b938acf6db412aa2705fb0c387a4dc","value":"Downloading (…)/main/tokenizer.json: 100%"}},"689356fe124f4314acb4897c22b38d73":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_87bcec01777d4bf29734f3d141c63851","max":1355256,"min":0,"orientation":"horizontal","style":"IPY_MODEL_011778661989497aabbd9a02bfe13727","value":1355256}},"fb5da7a8fa914b2dbb27dad514c547d4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_855b10e6eaea444484da514c65c4799f","placeholder":"​","style":"IPY_MODEL_cdf4eea66d33479c838a9a418a16b340","value":" 1.36M/1.36M [00:00&lt;00:00, 16.4MB/s]"}},"66deb99b8c174922bd0fc3b4c341811d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cc3d920647f4459eba06c7453968e2ba":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"02b938acf6db412aa2705fb0c387a4dc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"87bcec01777d4bf29734f3d141c63851":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"011778661989497aabbd9a02bfe13727":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"855b10e6eaea444484da514c65c4799f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cdf4eea66d33479c838a9a418a16b340":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"b13177b7"},"source":["<a href=\"https://colab.research.google.com/github/wlg1/numseqcont_circuit_expms/blob/main/nb_templates/headFNs_expms_template.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" align=\"left\"/></a>&nbsp;or in a local notebook."]},{"cell_type":"markdown","metadata":{"id":"DcZG9rm2IAiA"},"source":["# Setup\n","(No need to change anything)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rMcpSDdjIAiA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1689554561696,"user_tz":240,"elapsed":51019,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"19ea5af1-4973-497a-a13b-4fd0bad8d676"},"outputs":[{"output_type":"stream","name":"stdout","text":["Running as a Colab notebook\n","Collecting git+https://github.com/neelnanda-io/TransformerLens.git\n","  Cloning https://github.com/neelnanda-io/TransformerLens.git to /tmp/pip-req-build-nmajzgh3\n","  Running command git clone --filter=blob:none --quiet https://github.com/neelnanda-io/TransformerLens.git /tmp/pip-req-build-nmajzgh3\n","  Resolved https://github.com/neelnanda-io/TransformerLens.git to commit 218ebd6f491f47f5e2f64e4c4327548b60a093eb\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Collecting datasets>=2.7.1 (from transformer-lens==0.0.0)\n","  Downloading datasets-2.13.1-py3-none-any.whl (486 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m486.2/486.2 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting einops>=0.6.0 (from transformer-lens==0.0.0)\n","  Downloading einops-0.6.1-py3-none-any.whl (42 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting fancy-einsum>=0.0.3 (from transformer-lens==0.0.0)\n","  Downloading fancy_einsum-0.0.3-py3-none-any.whl (6.2 kB)\n","Collecting jaxtyping>=0.2.11 (from transformer-lens==0.0.0)\n","  Downloading jaxtyping-0.2.20-py3-none-any.whl (24 kB)\n","Collecting numpy>=1.23 (from transformer-lens==0.0.0)\n","  Downloading numpy-1.25.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m54.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.10/dist-packages (from transformer-lens==0.0.0) (1.5.3)\n","Requirement already satisfied: rich>=12.6.0 in /usr/local/lib/python3.10/dist-packages (from transformer-lens==0.0.0) (13.4.2)\n","Requirement already satisfied: torch>=1.10 in /usr/local/lib/python3.10/dist-packages (from transformer-lens==0.0.0) (2.0.1+cu118)\n","Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.10/dist-packages (from transformer-lens==0.0.0) (4.65.0)\n","Collecting transformers>=4.25.1 (from transformer-lens==0.0.0)\n","  Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m88.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting typeguard<4.0.0,>=3.0.2 (from transformer-lens==0.0.0)\n","  Downloading typeguard-3.0.2-py3-none-any.whl (30 kB)\n","Collecting wandb>=0.13.5 (from transformer-lens==0.0.0)\n","  Downloading wandb-0.15.5-py3-none-any.whl (2.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m82.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer-lens==0.0.0) (9.0.0)\n","Collecting dill<0.3.7,>=0.3.0 (from datasets>=2.7.1->transformer-lens==0.0.0)\n","  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer-lens==0.0.0) (2.27.1)\n","Collecting xxhash (from datasets>=2.7.1->transformer-lens==0.0.0)\n","  Downloading xxhash-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.5/212.5 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting multiprocess (from datasets>=2.7.1->transformer-lens==0.0.0)\n","  Downloading multiprocess-0.70.14-py310-none-any.whl (134 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer-lens==0.0.0) (2023.6.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer-lens==0.0.0) (3.8.4)\n","Collecting huggingface-hub<1.0.0,>=0.11.0 (from datasets>=2.7.1->transformer-lens==0.0.0)\n","  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer-lens==0.0.0) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer-lens==0.0.0) (6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.1 in /usr/local/lib/python3.10/dist-packages (from jaxtyping>=0.2.11->transformer-lens==0.0.0) (4.7.1)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->transformer-lens==0.0.0) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->transformer-lens==0.0.0) (2022.7.1)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=12.6.0->transformer-lens==0.0.0) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=12.6.0->transformer-lens==0.0.0) (2.14.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer-lens==0.0.0) (3.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer-lens==0.0.0) (1.11.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer-lens==0.0.0) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer-lens==0.0.0) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer-lens==0.0.0) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10->transformer-lens==0.0.0) (3.25.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10->transformer-lens==0.0.0) (16.0.6)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.1->transformer-lens==0.0.0) (2022.10.31)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers>=4.25.1->transformer-lens==0.0.0)\n","  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m113.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting safetensors>=0.3.1 (from transformers>=4.25.1->transformer-lens==0.0.0)\n","  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m83.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer-lens==0.0.0) (8.1.4)\n","Collecting GitPython!=3.1.29,>=1.0.0 (from wandb>=0.13.5->transformer-lens==0.0.0)\n","  Downloading GitPython-3.1.32-py3-none-any.whl (188 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.5/188.5 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer-lens==0.0.0) (5.9.5)\n","Collecting sentry-sdk>=1.0.0 (from wandb>=0.13.5->transformer-lens==0.0.0)\n","  Downloading sentry_sdk-1.28.1-py2.py3-none-any.whl (214 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m214.7/214.7 kB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb>=0.13.5->transformer-lens==0.0.0)\n","  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n","Collecting pathtools (from wandb>=0.13.5->transformer-lens==0.0.0)\n","  Downloading pathtools-0.1.2.tar.gz (11 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting setproctitle (from wandb>=0.13.5->transformer-lens==0.0.0)\n","  Downloading setproctitle-1.3.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer-lens==0.0.0) (67.7.2)\n","Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer-lens==0.0.0) (1.4.4)\n","Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer-lens==0.0.0) (3.20.3)\n","Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb>=0.13.5->transformer-lens==0.0.0) (1.16.0)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer-lens==0.0.0) (23.1.0)\n","Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer-lens==0.0.0) (2.0.12)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer-lens==0.0.0) (6.0.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer-lens==0.0.0) (4.0.2)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer-lens==0.0.0) (1.9.2)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer-lens==0.0.0) (1.3.3)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer-lens==0.0.0) (1.3.1)\n","Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer-lens==0.0.0)\n","  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=12.6.0->transformer-lens==0.0.0) (0.1.2)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=2.7.1->transformer-lens==0.0.0) (1.26.16)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=2.7.1->transformer-lens==0.0.0) (2023.5.7)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=2.7.1->transformer-lens==0.0.0) (3.4)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10->transformer-lens==0.0.0) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10->transformer-lens==0.0.0) (1.3.0)\n","Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer-lens==0.0.0)\n","  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n","Building wheels for collected packages: transformer-lens, pathtools\n","  Building wheel for transformer-lens (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for transformer-lens: filename=transformer_lens-0.0.0-py3-none-any.whl size=102649 sha256=50c080ec07ff30a7505584a395fd49616111ced9aa28a8e802f22cc9fe7f2e4c\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-gog9uydh/wheels/8a/1e/37/ffb9c15454a1725b13a9d9f5e74fb91725048884ad734b8c1f\n","  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8791 sha256=796e392823843808d645d52818a11ddf1f1eba9261e7350b9770abc4b285a1c3\n","  Stored in directory: /root/.cache/pip/wheels/e7/f3/22/152153d6eb222ee7a56ff8617d80ee5207207a8c00a7aab794\n","Successfully built transformer-lens pathtools\n","Installing collected packages: tokenizers, safetensors, pathtools, xxhash, typeguard, smmap, setproctitle, sentry-sdk, numpy, fancy-einsum, einops, docker-pycreds, dill, multiprocess, jaxtyping, huggingface-hub, gitdb, transformers, GitPython, wandb, datasets, transformer-lens\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 1.22.4\n","    Uninstalling numpy-1.22.4:\n","      Successfully uninstalled numpy-1.22.4\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","numba 0.56.4 requires numpy<1.24,>=1.18, but you have numpy 1.25.1 which is incompatible.\n","tensorflow 2.12.0 requires numpy<1.24,>=1.22, but you have numpy 1.25.1 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed GitPython-3.1.32 datasets-2.13.1 dill-0.3.6 docker-pycreds-0.4.0 einops-0.6.1 fancy-einsum-0.0.3 gitdb-4.0.10 huggingface-hub-0.16.4 jaxtyping-0.2.20 multiprocess-0.70.14 numpy-1.25.1 pathtools-0.1.2 safetensors-0.3.1 sentry-sdk-1.28.1 setproctitle-1.3.2 smmap-5.0.0 tokenizers-0.13.3 transformer-lens-0.0.0 transformers-4.30.2 typeguard-3.0.2 wandb-0.15.5 xxhash-3.2.0\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["numpy"]}}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","## Installing the NodeSource Node.js 16.x repo...\n","\n","\n","## Populating apt-get cache...\n","\n","+ apt-get update\n","Hit:1 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu focal InRelease\n","Hit:2 http://archive.ubuntu.com/ubuntu focal InRelease\n","Get:3 http://archive.ubuntu.com/ubuntu focal-updates InRelease [114 kB]\n","Get:4 https://cloud.r-project.org/bin/linux/ubuntu focal-cran40/ InRelease [3,622 B]\n","Hit:5 http://ppa.launchpad.net/cran/libgit2/ubuntu focal InRelease\n","Hit:6 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal InRelease\n","Get:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  InRelease [1,581 B]\n","Get:8 http://archive.ubuntu.com/ubuntu focal-backports InRelease [108 kB]\n","Hit:9 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu focal InRelease\n","Hit:10 http://ppa.launchpad.net/ubuntugis/ppa/ubuntu focal InRelease\n","Get:11 http://security.ubuntu.com/ubuntu focal-security InRelease [114 kB]\n","Get:12 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 Packages [3,347 kB]\n","Get:13 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 Packages [1,372 kB]\n","Get:14 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  Packages [1,084 kB]\n","Fetched 6,145 kB in 1s (4,767 kB/s)\n","Reading package lists... Done\n","\n","## Confirming \"focal\" is supported...\n","\n","+ curl -sLf -o /dev/null 'https://deb.nodesource.com/node_16.x/dists/focal/Release'\n","\n","## Adding the NodeSource signing key to your keyring...\n","\n","+ curl -s https://deb.nodesource.com/gpgkey/nodesource.gpg.key | gpg --dearmor | tee /usr/share/keyrings/nodesource.gpg >/dev/null\n","\n","## Creating apt sources list file for the NodeSource Node.js 16.x repo...\n","\n","+ echo 'deb [signed-by=/usr/share/keyrings/nodesource.gpg] https://deb.nodesource.com/node_16.x focal main' > /etc/apt/sources.list.d/nodesource.list\n","+ echo 'deb-src [signed-by=/usr/share/keyrings/nodesource.gpg] https://deb.nodesource.com/node_16.x focal main' >> /etc/apt/sources.list.d/nodesource.list\n","\n","## Running `apt-get update` for you...\n","\n","+ apt-get update\n","Hit:1 http://archive.ubuntu.com/ubuntu focal InRelease\n","Hit:2 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu focal InRelease\n","Hit:3 https://cloud.r-project.org/bin/linux/ubuntu focal-cran40/ InRelease\n","Hit:4 http://archive.ubuntu.com/ubuntu focal-updates InRelease\n","Hit:5 http://archive.ubuntu.com/ubuntu focal-backports InRelease\n","Hit:6 http://ppa.launchpad.net/cran/libgit2/ubuntu focal InRelease\n","Get:7 https://deb.nodesource.com/node_16.x focal InRelease [4,583 B]\n","Hit:8 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal InRelease\n","Hit:9 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  InRelease\n","Hit:10 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu focal InRelease\n","Hit:11 http://ppa.launchpad.net/ubuntugis/ppa/ubuntu focal InRelease\n","Hit:12 http://security.ubuntu.com/ubuntu focal-security InRelease\n","Get:13 https://deb.nodesource.com/node_16.x focal/main amd64 Packages [775 B]\n","Fetched 5,358 B in 1s (5,193 B/s)\n","Reading package lists... Done\n","\n","## Run `\u001b[1msudo apt-get install -y nodejs\u001b[m` to install Node.js 16.x and npm\n","## You may also need development tools to build native addons:\n","     sudo apt-get install gcc g++ make\n","## To install the Yarn package manager, run:\n","     curl -sL https://dl.yarnpkg.com/debian/pubkey.gpg | gpg --dearmor | sudo tee /usr/share/keyrings/yarnkey.gpg >/dev/null\n","     echo \"deb [signed-by=/usr/share/keyrings/yarnkey.gpg] https://dl.yarnpkg.com/debian stable main\" | sudo tee /etc/apt/sources.list.d/yarn.list\n","     sudo apt-get update && sudo apt-get install yarn\n","\n","\n","Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","The following NEW packages will be installed:\n","  nodejs\n","0 upgraded, 1 newly installed, 0 to remove and 15 not upgraded.\n","Need to get 27.2 MB of archives.\n","After this operation, 128 MB of additional disk space will be used.\n","Get:1 https://deb.nodesource.com/node_16.x focal/main amd64 nodejs amd64 16.20.1-deb-1nodesource1 [27.2 MB]\n","Fetched 27.2 MB in 0s (83.5 MB/s)\n","debconf: unable to initialize frontend: Dialog\n","debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 1.)\n","debconf: falling back to frontend: Readline\n","debconf: unable to initialize frontend: Readline\n","debconf: (This frontend requires a controlling tty.)\n","debconf: falling back to frontend: Teletype\n","dpkg-preconfigure: unable to re-open stdin: \n","Selecting previously unselected package nodejs.\n","(Reading database ... 123105 files and directories currently installed.)\n","Preparing to unpack .../nodejs_16.20.1-deb-1nodesource1_amd64.deb ...\n","Unpacking nodejs (16.20.1-deb-1nodesource1) ...\n","Setting up nodejs (16.20.1-deb-1nodesource1) ...\n","Processing triggers for man-db (2.9.1-1) ...\n","Collecting git+https://github.com/neelnanda-io/PySvelte.git\n","  Cloning https://github.com/neelnanda-io/PySvelte.git to /tmp/pip-req-build-6_ocu8m_\n","  Running command git clone --filter=blob:none --quiet https://github.com/neelnanda-io/PySvelte.git /tmp/pip-req-build-6_ocu8m_\n","  Resolved https://github.com/neelnanda-io/PySvelte.git to commit 6f5d971a148d40fb7481d400ae74551b37340e83\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from PySvelte==1.0.0) (0.6.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from PySvelte==1.0.0) (1.25.1)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from PySvelte==1.0.0) (2.0.1+cu118)\n","Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (from PySvelte==1.0.0) (2.13.1)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from PySvelte==1.0.0) (4.30.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from PySvelte==1.0.0) (4.65.0)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from PySvelte==1.0.0) (1.5.3)\n","Collecting typeguard~=2.0 (from PySvelte==1.0.0)\n","  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n","Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->PySvelte==1.0.0) (9.0.0)\n","Requirement already satisfied: dill<0.3.7,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets->PySvelte==1.0.0) (0.3.6)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets->PySvelte==1.0.0) (2.27.1)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->PySvelte==1.0.0) (3.2.0)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets->PySvelte==1.0.0) (0.70.14)\n","Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets->PySvelte==1.0.0) (2023.6.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->PySvelte==1.0.0) (3.8.4)\n","Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from datasets->PySvelte==1.0.0) (0.16.4)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets->PySvelte==1.0.0) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets->PySvelte==1.0.0) (6.0)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->PySvelte==1.0.0) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->PySvelte==1.0.0) (2022.7.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->PySvelte==1.0.0) (3.12.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->PySvelte==1.0.0) (4.7.1)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->PySvelte==1.0.0) (1.11.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->PySvelte==1.0.0) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->PySvelte==1.0.0) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->PySvelte==1.0.0) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->PySvelte==1.0.0) (3.25.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->PySvelte==1.0.0) (16.0.6)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->PySvelte==1.0.0) (2022.10.31)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers->PySvelte==1.0.0) (0.13.3)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers->PySvelte==1.0.0) (0.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->PySvelte==1.0.0) (23.1.0)\n","Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->PySvelte==1.0.0) (2.0.12)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->PySvelte==1.0.0) (6.0.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->PySvelte==1.0.0) (4.0.2)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->PySvelte==1.0.0) (1.9.2)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->PySvelte==1.0.0) (1.3.3)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->PySvelte==1.0.0) (1.3.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->PySvelte==1.0.0) (1.16.0)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets->PySvelte==1.0.0) (1.26.16)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets->PySvelte==1.0.0) (2023.5.7)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets->PySvelte==1.0.0) (3.4)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->PySvelte==1.0.0) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->PySvelte==1.0.0) (1.3.0)\n","Building wheels for collected packages: PySvelte\n","  Building wheel for PySvelte (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for PySvelte: filename=PySvelte-1.0.0-py3-none-any.whl size=158308 sha256=c98644f83a07d06d74e3751c77a36697820a5483ae194c714a78367d25e427e4\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-w7vx2u5l/wheels/fa/f6/f2/673ef7aeb78d7503b6e3e42387132822fdc38d3ee283d3e5b4\n","Successfully built PySvelte\n","Installing collected packages: typeguard, PySvelte\n","  Attempting uninstall: typeguard\n","    Found existing installation: typeguard 3.0.2\n","    Uninstalling typeguard-3.0.2:\n","      Successfully uninstalled typeguard-3.0.2\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","transformer-lens 0.0.0 requires typeguard<4.0.0,>=3.0.2, but you have typeguard 2.13.3 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed PySvelte-1.0.0 typeguard-2.13.3\n"]}],"source":["# Janky code to do different setup when run in a Colab notebook vs VSCode\n","DEBUG_MODE = False\n","try:\n","    import google.colab\n","    IN_COLAB = True\n","    print(\"Running as a Colab notebook\")\n","    %pip install git+https://github.com/neelnanda-io/TransformerLens.git\n","    # Install another version of node that makes PySvelte work way faster\n","    !curl -fsSL https://deb.nodesource.com/setup_16.x | sudo -E bash -; sudo apt-get install -y nodejs\n","    %pip install git+https://github.com/neelnanda-io/PySvelte.git\n","except:\n","    IN_COLAB = False\n","    print(\"Running as a Jupyter notebook - intended for development only!\")\n","    from IPython import get_ipython\n","\n","    ipython = get_ipython()\n","    # Code to automatically update the HookedTransformer code as its edited without restarting the kernel\n","    ipython.magic(\"load_ext autoreload\")\n","    ipython.magic(\"autoreload 2\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xKoTs7VBIAiD"},"outputs":[],"source":["# Plotly needs a different renderer for VSCode/Notebooks vs Colab argh\n","import plotly.io as pio\n","\n","if IN_COLAB or not DEBUG_MODE:\n","    # Thanks to annoying rendering issues, Plotly graphics will either show up in colab OR Vscode depending on the renderer - this is bad for developing demos! Thus creating a debug mode.\n","    pio.renderers.default = \"colab\"\n","else:\n","    pio.renderers.default = \"png\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z6b1n2tvIAiD"},"outputs":[],"source":["# Import stuff\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import numpy as np\n","import einops\n","from fancy_einsum import einsum\n","import tqdm.notebook as tqdm\n","import random\n","from pathlib import Path\n","import plotly.express as px\n","from torch.utils.data import DataLoader\n","\n","from jaxtyping import Float, Int\n","from typing import List, Union, Optional\n","from functools import partial\n","import copy\n","\n","import itertools\n","from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer\n","import dataclasses\n","import datasets\n","from IPython.display import HTML"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zuhzYxbsIAiE"},"outputs":[],"source":["import pysvelte\n","\n","import transformer_lens\n","import transformer_lens.utils as utils\n","from transformer_lens.hook_points import (\n","    HookedRootModule,\n","    HookPoint,\n",")  # Hooking utilities\n","from transformer_lens import HookedTransformer, HookedTransformerConfig, FactoredMatrix, ActivationCache"]},{"cell_type":"markdown","metadata":{"id":"hccba0v-IAiF"},"source":["We turn automatic differentiation off, to save GPU memory, as this notebook focuses on model inference not model training."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cFMTUcQiIAiF","outputId":"4d10a7d5-864e-44c4-efec-f2a3de52182a","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1689554568829,"user_tz":240,"elapsed":15,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch.autograd.grad_mode.set_grad_enabled at 0x78fdbc80ea40>"]},"metadata":{},"execution_count":5}],"source":["torch.set_grad_enabled(False)"]},{"cell_type":"markdown","metadata":{"id":"zyKb4C51IAiG"},"source":["Plotting helper functions:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KFs9BrbzIAiH"},"outputs":[],"source":["def imshow(tensor, renderer=None, **kwargs):\n","    px.imshow(utils.to_numpy(tensor), color_continuous_midpoint=0.0, color_continuous_scale=\"RdBu\", **kwargs).show(renderer)\n","\n","def line(tensor, renderer=None, **kwargs):\n","    px.line(y=utils.to_numpy(tensor), **kwargs).show(renderer)\n","\n","def scatter(x, y, xaxis=\"\", yaxis=\"\", caxis=\"\", renderer=None, **kwargs):\n","    x = utils.to_numpy(x)\n","    y = utils.to_numpy(y)\n","    px.scatter(y=y, x=x, labels={\"x\":xaxis, \"y\":yaxis, \"color\":caxis}, **kwargs).show(renderer)"]},{"cell_type":"markdown","source":["# Load Model"],"metadata":{"id":"OLkInsdjyHMx"}},{"cell_type":"markdown","source":["Decide which model to use (eg. gpt2-small vs -medium)"],"metadata":{"id":"ssJgoKr2yI8O"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"xLwDyosvIAiJ","executionInfo":{"status":"ok","timestamp":1689554585502,"user_tz":240,"elapsed":16685,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"colab":{"base_uri":"https://localhost:8080/","referenced_widgets":["b5a832faf53040a193929f5f345922cb","0d2a5621b21a4ec79ee197d5f587dce7","3a6bbd20249247ba83121be80fc66acb","cead0244853e43a489cba71bec0f8aeb","2cb1d73fd408451f8cf449863450803b","7649a3d236f842f1acd8d1ddab59e7a8","a599632bef994519817ce2b5de350268","02496c327c904410a46c5dc64cc9abd8","e1a5dbaaae1848889a135b1f6c2bc42b","6a3d7e5322be436e89cd67aff42f0c37","bb0f52c7e10c454b96a3cd62392f9e0f","410eabe330234f7a96767b0c0572474f","714762a589d34ef687171800050ffc0e","5191de2f5fa045adb9e201b658d70dfa","14363dca02714b96a0194a87f20e900d","5d75369794a343618337f50835a1ea1c","2a698b84478840f2932bdc361f0023c9","dc5ac7807dd447669295d06b71b0cadc","88641295b608453bb46b8c1738a96127","250ae5aadbc749338c027c1ea761634f","f550ab4994144463a93e8604eee4b5e7","ad59ddeee53a4009b890ee4920ca5146","81a654ddef954fef87620fe2780926bd","57e8d6d28a1d4401bd7c3b3f919bc318","51f68aa5caea48d89fa32dcd834393c2","dba37d2e75e04478bcda6fd4fdce0ab3","243516e1d15840d19c32a3bd305e5c76","c6bf049689c6470cb5999d18e652f053","744b86a4691442e79ab2357c41a56455","fca913b610a0466c90c2a13c3f2b6757","219d84a5bb9f4ae289d8d54e659b8716","68304dcb56464d99bc8cdc40b400fc71","fb4657de28954b0caca5b5e4a9ebf686","6117cbc508b64d93a6e090af43c5abee","a1508e16d67f4ece96064386a908ac7a","ccfc134c3185462b8bd3eb97e6851027","e1c2e1dbfcf043b8ba25e77feed13646","69ed6dcc37394f5fb7a115412dde27f7","353508c2ea1b4421875e5d696d1163b6","48624782bebe44bfb90ebd49dc8213db","3b2008e3c29d437494ce4f19d7f47144","463a163353794c7fbf20cf2ada338cc7","dab2cafa9ea14571afc50f52a47b1028","5ee661b9c841457a870fc2364e2656c0","c5fa55e3d4fb493c916e4226e7355aba","88bbf8b2e475416fbcc6e562452b8675","6bb7af85068246178e046e92b53770da","7053b48b744b438ca893e01c102bbc31","3b46008e5bca44e6a32c365a2bd3ba58","d549ff7af825419191218fc57adedf1e","8d5f9660f71c45a5a36c201e3e17b079","0f9dce8a927644afad721b41946c1036","613cf9674273438bbee12ab754208ca7","5a9d4a1b2e0c4bcd9cb190e597cf2a42","8fa601ecb1224b3c966f875172db6a6d","a243c60c0d41497ba05ca1873e917ee0","969f960c9b524449984d2adec1fad898","689356fe124f4314acb4897c22b38d73","fb5da7a8fa914b2dbb27dad514c547d4","66deb99b8c174922bd0fc3b4c341811d","cc3d920647f4459eba06c7453968e2ba","02b938acf6db412aa2705fb0c387a4dc","87bcec01777d4bf29734f3d141c63851","011778661989497aabbd9a02bfe13727","855b10e6eaea444484da514c65c4799f","cdf4eea66d33479c838a9a418a16b340"]},"outputId":"0c2e10b8-3a31-447e-8c68-ff2c37fdd8d6"},"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading (…)lve/main/config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b5a832faf53040a193929f5f345922cb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"410eabe330234f7a96767b0c0572474f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)neration_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"81a654ddef954fef87620fe2780926bd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)olve/main/vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6117cbc508b64d93a6e090af43c5abee"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c5fa55e3d4fb493c916e4226e7355aba"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a243c60c0d41497ba05ca1873e917ee0"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Using pad_token, but it is not set yet.\n"]},{"output_type":"stream","name":"stdout","text":["Loaded pretrained model gpt2-small into HookedTransformer\n"]}],"source":["model = HookedTransformer.from_pretrained(\n","    \"gpt2-small\",\n","    center_unembed=True,\n","    center_writing_weights=True,\n","    fold_ln=True,\n","    refactor_factored_attn_matrices=True,\n",")"]},{"cell_type":"markdown","source":["# Extracted Model"],"metadata":{"id":"HfTOVF9eS_0-"}},{"cell_type":"markdown","source":["## use only L9, then only L0 and L9"],"metadata":{"id":"Iq44imzAp87u"}},{"cell_type":"code","source":["import torch.nn as nn\n","\n","class ExtractedModel(nn.Module):\n","    def __init__(self, original_model):\n","        super(ExtractedModel, self).__init__()\n","        self.embed = original_model.embed\n","        self.pos_embed = original_model.pos_embed\n","\n","        # MLP 0 seems impt as another embedding layer\n","        self.ln1_0 = original_model.blocks[0].ln1\n","        self.attn_0 = original_model.blocks[0].attn\n","        self.ln2_0 = original_model.blocks[0].ln2\n","        self.mlp_0 = original_model.blocks[0].mlp  # the MLP layer in the 9th transformer block\n","\n","        self.ln1 = original_model.blocks[9].ln1\n","        self.attn = original_model.blocks[9].attn\n","        self.ln2 = original_model.blocks[9].ln2\n","        self.mlp = original_model.blocks[9].mlp  # the MLP layer in the 9th transformer block\n","        self.ln_final = original_model.ln_final\n","        self.unembed = original_model.unembed\n","\n","    def forward(self, tokens):\n","        # tokens [batch, position]\n","        embed = self.embed(tokens)\n","        pos_embed = self.pos_embed(tokens)\n","        resid_pre = embed + pos_embed\n","\n","        normalized_resid_pre = self.ln1_0(resid_pre)\n","        attn_out = self.attn_0(normalized_resid_pre, normalized_resid_pre, normalized_resid_pre)\n","        resid_mid = resid_pre + attn_out\n","\n","        normalized_resid_mid = self.ln2_0(resid_mid)\n","        # normalized_resid_mid = self.ln2(resid_pre)\n","        mlp_out = self.mlp_0(normalized_resid_mid)\n","        resid_post = resid_mid + mlp_out\n","        # resid_post = resid_pre + mlp_out\n","\n","        normalized_resid_pre = self.ln1(resid_post)\n","        # normalized_resid_pre = self.ln1(resid_pre)\n","        attn_out = self.attn(normalized_resid_pre, normalized_resid_pre, normalized_resid_pre)\n","        resid_mid = resid_pre + attn_out\n","\n","        normalized_resid_mid = self.ln2(resid_mid)\n","        # normalized_resid_mid = self.ln2(resid_pre)\n","        mlp_out = self.mlp(normalized_resid_mid)\n","        resid_post = resid_mid + mlp_out\n","        # resid_post = resid_pre + mlp_out\n","\n","        normalized_resid_final = self.ln_final(resid_post)\n","        logits = self.unembed(normalized_resid_final)\n","        # logits have shape [batch, position, logits]\n","        return logits\n","\n","extracted_model = ExtractedModel(model)"],"metadata":{"id":"mOB9OqOdUVUY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokens = model.to_tokens([\"1 2 3 4\"], prepend_bos=True)\n","tokens = tokens.cuda() # Move the tokens to the GPU\n","original_logits = extracted_model(tokens) # Run the model and cache all activations"],"metadata":{"id":"B5Lr4BnNVc_X"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["original_logits.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jkUMmpJVaFw8","executionInfo":{"status":"ok","timestamp":1689423595769,"user_tz":240,"elapsed":380,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"f41c9fa1-9607-4926-8dba-158db05494ce"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1, 5, 50257])"]},"metadata":{},"execution_count":19}]},{"cell_type":"code","source":["def remove_batch_dim(\n","    tensor: Float[torch.Tensor, \"1 ...\"]\n",") -> Float[torch.Tensor, \"...\"]:\n","    \"\"\"\n","    Removes the first dimension of a tensor if it is size 1, otherwise returns the tensor unchanged\n","    \"\"\"\n","    if tensor.shape[0] == 1:\n","        return tensor.squeeze(0)\n","    else:\n","        return tensor\n","\n","def test_prompt(\n","    prompt: str,\n","    answer: str,\n","    model,\n","    orig_model,\n","    prepend_space_to_answer: bool = True,\n","    print_details: bool = True,\n","    prepend_bos: bool = True,\n","    top_k: int = 10,\n","):\n","    \"\"\"\n","    Function to test whether a model can give the correct answer to a prompt. Intended for exploratory analysis, so it prints things out rather than returning things.\n","\n","    Works for multi-token answers and multi-token prompts.\n","\n","    Will always print the ranks of the answer tokens, and if print_details will print the logit and prob for the answer tokens and the top k tokens returned for each answer position.\n","    \"\"\"\n","    if prepend_space_to_answer and not answer.startswith(\" \"):\n","        answer = \" \" + answer\n","    # GPT-2 often treats the first token weirdly, so lets give it a resting position\n","    tokens = orig_model.to_tokens(prompt + answer, prepend_bos=prepend_bos)\n","\n","\n","    prompt_str_tokens = orig_model.to_str_tokens(prompt, prepend_bos=prepend_bos)\n","    answer_str_tokens = orig_model.to_str_tokens(answer, prepend_bos=False)\n","    prompt_length = len(prompt_str_tokens)\n","    answer_length = len(answer_str_tokens)\n","    if print_details:\n","        print(\"Tokenized prompt:\", prompt_str_tokens)\n","        print(\"Tokenized answer:\", answer_str_tokens)\n","\n","    logits = remove_batch_dim(model(tokens))\n","\n","    probs = logits.softmax(dim=-1)\n","    answer_ranks = []\n","    for index in range(prompt_length, prompt_length + answer_length):\n","        answer_token = tokens[0, index]\n","        answer_str_token = answer_str_tokens[index - prompt_length]\n","        # Offset by 1 because models predict the NEXT token\n","        token_probs = probs[index - 1]\n","        sorted_token_probs, sorted_token_values = token_probs.sort(descending=True) # sorted_token_values are the indices; the indices correspond to token num of vocab\n","        # Janky way to get the index of the token in the sorted list - I couldn't find a better way?\n","        correct_rank = torch.arange(len(sorted_token_values))[\n","            (sorted_token_values == answer_token).cpu()\n","        ].item()\n","        answer_ranks.append((answer_str_token, correct_rank))\n","        if print_details:\n","            # String formatting syntax - the first number gives the number of characters to pad to, the second number gives the number of decimal places.\n","            # rprint gives rich text printing\n","            print(\n","                f\"Performance on answer token:\\n[b]Rank: {correct_rank: <8} Logit: {logits[index-1, answer_token].item():5.2f} Prob: {token_probs[answer_token].item():6.2%} Token: |{answer_str_token}|[/b]\"\n","            )\n","            for i in range(top_k):\n","                print(\n","                    f\"Top {i}th token. Logit: {logits[index-1, sorted_token_values[i]].item():5.2f} Prob: {sorted_token_probs[i].item():6.2%} Token: |{orig_model.to_string(sorted_token_values[i])}|\"\n","                )\n","    print(f\"[b]Ranks of the answer tokens:[/b] {answer_ranks}\")"],"metadata":{"id":"mjEwnqdFejDt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_prompt(\"1 2 3 4 5 6 7 8 9\", \" 10\", extracted_model, model, prepend_bos=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c3sDR4soetCV","executionInfo":{"status":"ok","timestamp":1689424493521,"user_tz":240,"elapsed":7,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"1368e391-83e9-4bff-c39e-6985dc9133f2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Tokenized prompt: ['<|endoftext|>', '1', ' 2', ' 3', ' 4', ' 5', ' 6', ' 7', ' 8', ' 9']\n","Tokenized answer: [' 10']\n","Performance on answer token:\n","[b]Rank: 234      Logit: 13.02 Prob:  0.02% Token: | 10|[/b]\n","Top 0th token. Logit: 19.92 Prob: 22.72% Token: | South|\n","Top 1th token. Logit: 19.65 Prob: 17.50% Token: | North|\n","Top 2th token. Logit: 18.69 Prob:  6.70% Token: | live|\n","Top 3th token. Logit: 18.58 Prob:  5.98% Token: | West|\n","Top 4th token. Logit: 18.57 Prob:  5.94% Token: |,|\n","Top 5th token. Logit: 17.93 Prob:  3.14% Token: | a|\n","Top 6th token. Logit: 17.27 Prob:  1.62% Token: | Force|\n","Top 7th token. Logit: 17.09 Prob:  1.34% Token: | Long|\n","Top 8th token. Logit: 16.88 Prob:  1.10% Token: | U|\n","Top 9th token. Logit: 16.84 Prob:  1.05% Token: | in|\n","[b]Ranks of the answer tokens:[/b] [(' 10', 234)]\n"]}]},{"cell_type":"code","source":["test_prompt(\"January February March April\", \" May\", extracted_model, model, prepend_bos=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DNuvtnpciH28","executionInfo":{"status":"ok","timestamp":1689424501481,"user_tz":240,"elapsed":345,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"90f5258f-2f19-4ba6-fa96-17a008b27430"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Tokenized prompt: ['<|endoftext|>', 'January', ' February', ' March', ' April']\n","Tokenized answer: [' May']\n","Performance on answer token:\n","[b]Rank: 733      Logit: 11.10 Prob:  0.00% Token: | May|[/b]\n","Top 0th token. Logit: 19.69 Prob: 17.42% Token: | North|\n","Top 1th token. Logit: 19.49 Prob: 14.33% Token: | South|\n","Top 2th token. Logit: 19.03 Prob:  9.01% Token: | West|\n","Top 3th token. Logit: 19.01 Prob:  8.87% Token: | live|\n","Top 4th token. Logit: 18.11 Prob:  3.61% Token: | a|\n","Top 5th token. Logit: 18.05 Prob:  3.40% Token: |,|\n","Top 6th token. Logit: 17.62 Prob:  2.20% Token: | Force|\n","Top 7th token. Logit: 17.57 Prob:  2.10% Token: | Islands|\n","Top 8th token. Logit: 17.09 Prob:  1.30% Token: | Long|\n","Top 9th token. Logit: 16.92 Prob:  1.10% Token: | in|\n","[b]Ranks of the answer tokens:[/b] [(' May', 733)]\n"]}]},{"cell_type":"markdown","source":["## Test validity of extraction class\n","\n","Test if can recover original model using this extraction on all layers\n","\n","If not, extraction is wrong"],"metadata":{"id":"KVnaWFVxkBN-"}},{"cell_type":"code","source":["import torch.nn as nn\n","\n","class ExtractedModel_full(nn.Module):\n","    def __init__(self, original_model):\n","        super(ExtractedModel_full, self).__init__()\n","        self.embed = original_model.embed\n","        self.pos_embed = original_model.pos_embed\n","\n","        self.blocks = original_model.blocks\n","\n","        self.ln_final = original_model.ln_final\n","        self.unembed = original_model.unembed\n","\n","    def forward(self, tokens):\n","        # tokens [batch, position]\n","        embed = self.embed(tokens)\n","        pos_embed = self.pos_embed(tokens)\n","        residual = embed + pos_embed\n","\n","        for block in self.blocks:\n","            residual = block(residual)\n","\n","        normalized_resid_final = self.ln_final(residual)\n","        logits = self.unembed(normalized_resid_final)\n","        # logits have shape [batch, position, logits]\n","        return logits\n","\n","extracted_model_full = ExtractedModel_full(model)"],"metadata":{"id":"9UguWLW6kL8D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_prompt(\"1 2 3 4\", \" 5\", extracted_model_full, model, prepend_bos=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PzumoSIblasZ","executionInfo":{"status":"ok","timestamp":1689425134722,"user_tz":240,"elapsed":175,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"3f9ee50a-b971-4ae9-99d4-16a081e138c3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Tokenized prompt: ['<|endoftext|>', '1', ' 2', ' 3', ' 4']\n","Tokenized answer: [' 5']\n","Performance on answer token:\n","[b]Rank: 0        Logit: 18.76 Prob: 96.17% Token: | 5|[/b]\n","Top 0th token. Logit: 18.76 Prob: 96.17% Token: | 5|\n","Top 1th token. Logit: 13.27 Prob:  0.40% Token: | Next|\n","Top 2th token. Logit: 13.01 Prob:  0.30% Token: |\n","|\n","Top 3th token. Logit: 12.87 Prob:  0.27% Token: | >|\n","Top 4th token. Logit: 12.04 Prob:  0.12% Token: | 4|\n","Top 5th token. Logit: 11.88 Prob:  0.10% Token: | 50|\n","Top 6th token. Logit: 11.83 Prob:  0.09% Token: | 6|\n","Top 7th token. Logit: 11.71 Prob:  0.08% Token: | <|\n","Top 8th token. Logit: 11.64 Prob:  0.08% Token: | $|\n","Top 9th token. Logit: 11.63 Prob:  0.08% Token: | 1|\n","[b]Ranks of the answer tokens:[/b] [(' 5', 0)]\n"]}]},{"cell_type":"markdown","source":["This is an exact match to:\n","\n","https://colab.research.google.com/drive/1uSuPtHrh9venKNlIt2O-1piknwfQpJmK#scrollTo=m90WRkxYIAiL&line=1&uniqifier=1"],"metadata":{"id":"0FPXQveJll9T"}},{"cell_type":"code","source":["test_prompt(\"January February March April\", \" May\", extracted_model_full, model, prepend_bos=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OWe216EhCif8","executionInfo":{"status":"ok","timestamp":1689432779925,"user_tz":240,"elapsed":361,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"d36dfb4e-7e77-4c41-b6cb-9bde0ed09a1a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Tokenized prompt: ['<|endoftext|>', 'January', ' February', ' March', ' April']\n","Tokenized answer: [' May']\n","Performance on answer token:\n","[b]Rank: 0        Logit: 30.05 Prob: 99.87% Token: | May|[/b]\n","Top 0th token. Logit: 30.05 Prob: 99.87% Token: | May|\n","Top 1th token. Logit: 22.47 Prob:  0.05% Token: | 5|\n","Top 2th token. Logit: 21.69 Prob:  0.02% Token: | July|\n","Top 3th token. Logit: 21.12 Prob:  0.01% Token: |\n","|\n","Top 4th token. Logit: 21.04 Prob:  0.01% Token: | June|\n","Top 5th token. Logit: 19.32 Prob:  0.00% Token: | Wh|\n","Top 6th token. Logit: 19.18 Prob:  0.00% Token: | If|\n","Top 7th token. Logit: 18.90 Prob:  0.00% Token: | March|\n","Top 8th token. Logit: 18.79 Prob:  0.00% Token: | April|\n","Top 9th token. Logit: 18.69 Prob:  0.00% Token: | Please|\n","[b]Ranks of the answer tokens:[/b] [(' May', 0)]\n"]}]},{"cell_type":"markdown","source":["Yes, it's able to do this. Thus, our extracted model is correct. Can we see what happens as we gradually add layers? Perhaps from 0 to 9? (This is like logit lens, except we can take away layers)\n","\n","Given that vector addition logit lens showed this is possible, it should be."],"metadata":{"id":"TEyx5_3BlMm1"}},{"cell_type":"markdown","source":[" Try different layers combos"],"metadata":{"id":"2LlyTDVTqbLg"}},{"cell_type":"markdown","source":["## keep only L0 to L9"],"metadata":{"id":"v4YENbmYlU7_"}},{"cell_type":"code","source":["import torch.nn as nn\n","\n","class ExtractedModel_toL9(nn.Module):\n","    def __init__(self, original_model):\n","        super(ExtractedModel_toL9, self).__init__()\n","        self.embed = original_model.embed\n","        self.pos_embed = original_model.pos_embed\n","        self.blocks = original_model.blocks\n","        self.ln_final = original_model.ln_final\n","        self.unembed = original_model.unembed\n","\n","    def forward(self, tokens):\n","        # tokens [batch, position]\n","        embed = self.embed(tokens)\n","        pos_embed = self.pos_embed(tokens)\n","        residual = embed + pos_embed\n","\n","        for block in self.blocks[0:10]:\n","            residual = block(residual)\n","\n","        normalized_resid_final = self.ln_final(residual)\n","        logits = self.unembed(normalized_resid_final)\n","        # logits have shape [batch, position, logits]\n","        return logits\n","\n","extracted_model_toL9 = ExtractedModel_toL9(model)"],"metadata":{"id":"TTqOEiELlZdZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_prompt(\"1 2 3 4\", \" 5\", extracted_model_toL9, model, prepend_bos=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NJvIwOrBl-u_","executionInfo":{"status":"ok","timestamp":1689425280495,"user_tz":240,"elapsed":256,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"52cb5832-30b4-4adc-d6ec-0f4429d3153d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Tokenized prompt: ['<|endoftext|>', '1', ' 2', ' 3', ' 4']\n","Tokenized answer: [' 5']\n","Performance on answer token:\n","[b]Rank: 0        Logit: 34.66 Prob: 99.98% Token: | 5|[/b]\n","Top 0th token. Logit: 34.66 Prob: 99.98% Token: | 5|\n","Top 1th token. Logit: 25.98 Prob:  0.02% Token: | 4|\n","Top 2th token. Logit: 24.19 Prob:  0.00% Token: | 6|\n","Top 3th token. Logit: 23.46 Prob:  0.00% Token: | 7|\n","Top 4th token. Logit: 23.30 Prob:  0.00% Token: | Next|\n","Top 5th token. Logit: 22.81 Prob:  0.00% Token: | 9|\n","Top 6th token. Logit: 21.98 Prob:  0.00% Token: |5|\n","Top 7th token. Logit: 21.46 Prob:  0.00% Token: |Next|\n","Top 8th token. Logit: 20.87 Prob:  0.00% Token: |♥|\n","Top 9th token. Logit: 20.43 Prob:  0.00% Token: | 3|\n","[b]Ranks of the answer tokens:[/b] [(' 5', 0)]\n"]}]},{"cell_type":"markdown","source":["It's actually doing BETTER than with L10 and L11. This seems inconsistent with this plot; even though there are dips at 10 and 11, they seem to still go up in the end:\n","\n","https://colab.research.google.com/drive/1eavp74fMMDHIBeVeE0UupRmlXHPAenoa#scrollTo=gYOOrypHIAiR&line=2&uniqifier=1"],"metadata":{"id":"XeOV8E1kmBc5"}},{"cell_type":"code","source":["test_prompt(\"one two three four\", \" five\", extracted_model_toL9, model, prepend_bos=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1689433657208,"user_tz":240,"elapsed":9,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"19fc770f-9115-4b18-fbb2-8c4654cf0819","id":"hw0F1KSEF6PK"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Tokenized prompt: ['<|endoftext|>', 'one', ' two', ' three', ' four']\n","Tokenized answer: [' five']\n","Performance on answer token:\n","[b]Rank: 0        Logit: 26.86 Prob: 83.10% Token: | five|[/b]\n","Top 0th token. Logit: 26.86 Prob: 83.10% Token: | five|\n","Top 1th token. Logit: 23.97 Prob:  4.63% Token: | four|\n","Top 2th token. Logit: 23.30 Prob:  2.37% Token: | Four|\n","Top 3th token. Logit: 23.28 Prob:  2.32% Token: | Five|\n","Top 4th token. Logit: 23.18 Prob:  2.11% Token: | seven|\n","Top 5th token. Logit: 22.42 Prob:  0.98% Token: |teen|\n","Top 6th token. Logit: 22.29 Prob:  0.86% Token: | six|\n","Top 7th token. Logit: 22.16 Prob:  0.76% Token: |five|\n","Top 8th token. Logit: 22.09 Prob:  0.71% Token: |Five|\n","Top 9th token. Logit: 22.06 Prob:  0.69% Token: | 5|\n","[b]Ranks of the answer tokens:[/b] [(' five', 0)]\n"]}]},{"cell_type":"markdown","source":["## Try skipping layers\n"],"metadata":{"id":"hknzGObEmFC5"}},{"cell_type":"code","source":["[0, 1, 2, 3, 4, 5][0:3]  # basic pythoon sanity check"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YB0v2vGRnQre","executionInfo":{"status":"ok","timestamp":1689425635845,"user_tz":240,"elapsed":140,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"5f6e8c6a-35d2-47a9-a08a-987b5cb29fb7"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[0, 1, 2]"]},"metadata":{},"execution_count":30}]},{"cell_type":"markdown","source":["### skip L5 only"],"metadata":{"id":"v3M07pnRnFIi"}},{"cell_type":"markdown","source":["Try skipping L5, because it's red:\n","\n","https://colab.research.google.com/drive/1eavp74fMMDHIBeVeE0UupRmlXHPAenoa#scrollTo=TNhyDx1XIAia&line=1&uniqifier=1"],"metadata":{"id":"dquqZxKPmxwY"}},{"cell_type":"code","source":["import torch.nn as nn\n","\n","class ExtractedModel(nn.Module):\n","    def __init__(self, original_model):\n","        super(ExtractedModel, self).__init__()\n","        self.embed = original_model.embed\n","        self.pos_embed = original_model.pos_embed\n","        self.blocks = original_model.blocks\n","        self.ln_final = original_model.ln_final\n","        self.unembed = original_model.unembed\n","\n","    def forward(self, tokens):\n","        # tokens [batch, position]\n","        embed = self.embed(tokens)\n","        pos_embed = self.pos_embed(tokens)\n","        residual = embed + pos_embed\n","\n","        for block in self.blocks[0:5] + self.blocks[6:10]:\n","            residual = block(residual)\n","\n","        normalized_resid_final = self.ln_final(residual)\n","        logits = self.unembed(normalized_resid_final)\n","        # logits have shape [batch, position, logits]\n","        return logits\n","\n","extracted_model = ExtractedModel(model)"],"metadata":{"id":"dafwaDdemJwg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_prompt(\"1 2 3 4\", \" 5\", extracted_model, model, prepend_bos=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z5T_6_HAmuLV","executionInfo":{"status":"ok","timestamp":1689425773474,"user_tz":240,"elapsed":8,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"49c892d3-4da2-496c-c71b-cad3c5e2bfb8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Tokenized prompt: ['<|endoftext|>', '1', ' 2', ' 3', ' 4']\n","Tokenized answer: [' 5']\n","Performance on answer token:\n","[b]Rank: 56       Logit: 12.34 Prob:  0.29% Token: | 5|[/b]\n","Top 0th token. Logit: 15.91 Prob: 10.41% Token: |th|\n","Top 1th token. Logit: 15.70 Prob:  8.43% Token: |34|\n","Top 2th token. Logit: 15.64 Prob:  7.91% Token: | -|\n","Top 3th token. Logit: 15.10 Prob:  4.63% Token: |54|\n","Top 4th token. Logit: 15.01 Prob:  4.20% Token: |ts|\n","Top 5th token. Logit: 14.98 Prob:  4.08% Token: |:|\n","Top 6th token. Logit: 14.66 Prob:  2.98% Token: |67|\n","Top 7th token. Logit: 14.45 Prob:  2.42% Token: |74|\n","Top 8th token. Logit: 14.35 Prob:  2.18% Token: |.|\n","Top 9th token. Logit: 14.26 Prob:  1.99% Token: |39|\n","[b]Ranks of the answer tokens:[/b] [(' 5', 56)]\n"]}]},{"cell_type":"code","source":["test_prompt(\"January February March April\", \" May\", extracted_model, model, prepend_bos=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1TteNo7Xm6qv","executionInfo":{"status":"ok","timestamp":1689425779410,"user_tz":240,"elapsed":144,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"709e991a-4857-4ab0-dbd3-a314b3dc7d87"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Tokenized prompt: ['<|endoftext|>', 'January', ' February', ' March', ' April']\n","Tokenized answer: [' May']\n","Performance on answer token:\n","[b]Rank: 62       Logit: 13.65 Prob:  0.02% Token: | May|[/b]\n","Top 0th token. Logit: 20.77 Prob: 25.32% Token: | 29|\n","Top 1th token. Logit: 20.18 Prob: 14.05% Token: | 24|\n","Top 2th token. Logit: 19.67 Prob:  8.47% Token: | 26|\n","Top 3th token. Logit: 19.50 Prob:  7.09% Token: | 28|\n","Top 4th token. Logit: 19.39 Prob:  6.37% Token: | 2018|\n","Top 5th token. Logit: 19.34 Prob:  6.06% Token: | 27|\n","Top 6th token. Logit: 18.96 Prob:  4.17% Token: | 4|\n","Top 7th token. Logit: 18.73 Prob:  3.31% Token: | 30|\n","Top 8th token. Logit: 18.34 Prob:  2.22% Token: | 6|\n","Top 9th token. Logit: 18.19 Prob:  1.92% Token: | 3|\n","[b]Ranks of the answer tokens:[/b] [(' May', 62)]\n"]}]},{"cell_type":"markdown","source":["It's much worse. We need L5. Yet, it's still recognizing numbers."],"metadata":{"id":"CJQuuop0m-0l"}},{"cell_type":"markdown","source":["### keep only L0 to L8"],"metadata":{"id":"-mWss55ynBcp"}},{"cell_type":"code","source":["import torch.nn as nn\n","\n","class ExtractedModel(nn.Module):\n","    def __init__(self, original_model):\n","        super(ExtractedModel, self).__init__()\n","        self.embed = original_model.embed\n","        self.pos_embed = original_model.pos_embed\n","        self.blocks = original_model.blocks\n","        self.ln_final = original_model.ln_final\n","        self.unembed = original_model.unembed\n","\n","    def forward(self, tokens):\n","        # tokens [batch, position]\n","        embed = self.embed(tokens)\n","        pos_embed = self.pos_embed(tokens)\n","        residual = embed + pos_embed\n","\n","        for block in self.blocks[0:9]: # + self.blocks[9:10]:\n","            residual = block(residual)\n","\n","        normalized_resid_final = self.ln_final(residual)\n","        logits = self.unembed(normalized_resid_final)\n","        # logits have shape [batch, position, logits]\n","        return logits\n","\n","extracted_model = ExtractedModel(model)"],"metadata":{"id":"D0ITeONtny_S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_prompt(\"1 2 3 4\", \" 5\", extracted_model, model, prepend_bos=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1689425836177,"user_tz":240,"elapsed":154,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"9fff8d7a-a36d-49c1-b424-fa75d49b22fe","id":"CDWkP-yHny_c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Tokenized prompt: ['<|endoftext|>', '1', ' 2', ' 3', ' 4']\n","Tokenized answer: [' 5']\n","Performance on answer token:\n","[b]Rank: 1        Logit: 26.25 Prob:  2.53% Token: | 5|[/b]\n","Top 0th token. Logit: 29.89 Prob: 96.58% Token: | 4|\n","Top 1th token. Logit: 26.25 Prob:  2.53% Token: | 5|\n","Top 2th token. Logit: 24.31 Prob:  0.36% Token: | 3|\n","Top 3th token. Logit: 23.98 Prob:  0.26% Token: | 6|\n","Top 4th token. Logit: 22.35 Prob:  0.05% Token: | 8|\n","Top 5th token. Logit: 22.18 Prob:  0.04% Token: | 9|\n","Top 6th token. Logit: 22.10 Prob:  0.04% Token: | 7|\n","Top 7th token. Logit: 21.94 Prob:  0.03% Token: | ..........|\n","Top 8th token. Logit: 21.46 Prob:  0.02% Token: | 1|\n","Top 9th token. Logit: 20.93 Prob:  0.01% Token: | <!--|\n","[b]Ranks of the answer tokens:[/b] [(' 5', 1)]\n"]}]},{"cell_type":"markdown","source":["As we see in logit lens, L9 is needed to convert the +1. But L0 to L8 are enough to \"get the number\". This means we can't rely on a single layer."],"metadata":{"id":"buKY2gozoJBO"}},{"cell_type":"code","source":["test_prompt(\"January February March April\", \" May\", extracted_model, model, prepend_bos=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oS4aqJkfA_tN","executionInfo":{"status":"ok","timestamp":1689432688851,"user_tz":240,"elapsed":8,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"45663fb3-71fd-4075-84ef-33bdc76cdc98"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Tokenized prompt: ['<|endoftext|>', 'January', ' February', ' March', ' April']\n","Tokenized answer: [' May']\n","Performance on answer token:\n","[b]Rank: 4        Logit: 22.44 Prob:  1.95% Token: | May|[/b]\n","Top 0th token. Logit: 26.08 Prob: 74.10% Token: | April|\n","Top 1th token. Logit: 23.84 Prob:  7.93% Token: | 2014|\n","Top 2th token. Logit: 23.35 Prob:  4.84% Token: | 2015|\n","Top 3th token. Logit: 22.79 Prob:  2.78% Token: | March|\n","Top 4th token. Logit: 22.44 Prob:  1.95% Token: | May|\n","Top 5th token. Logit: 22.26 Prob:  1.62% Token: | 2017|\n","Top 6th token. Logit: 21.93 Prob:  1.17% Token: | June|\n","Top 7th token. Logit: 21.70 Prob:  0.93% Token: | September|\n","Top 8th token. Logit: 21.03 Prob:  0.48% Token: | 2018|\n","Top 9th token. Logit: 20.97 Prob:  0.45% Token: | 1989|\n","[b]Ranks of the answer tokens:[/b] [(' May', 4)]\n"]}]},{"cell_type":"markdown","source":["Same thing for months (as seen in logit lens)"],"metadata":{"id":"HzFTX0PICRah"}},{"cell_type":"markdown","source":["### keep only L0 to 7"],"metadata":{"id":"Pc7fd9XvohPm"}},{"cell_type":"code","source":["import torch.nn as nn\n","\n","class ExtractedModel(nn.Module):\n","    def __init__(self, original_model):\n","        super(ExtractedModel, self).__init__()\n","        self.embed = original_model.embed\n","        self.pos_embed = original_model.pos_embed\n","        self.blocks = original_model.blocks\n","        self.ln_final = original_model.ln_final\n","        self.unembed = original_model.unembed\n","\n","    def forward(self, tokens):\n","        # tokens [batch, position]\n","        embed = self.embed(tokens)\n","        pos_embed = self.pos_embed(tokens)\n","        residual = embed + pos_embed\n","\n","        for block in self.blocks[0:8]: # + self.blocks[9:10]:\n","            residual = block(residual)\n","\n","        normalized_resid_final = self.ln_final(residual)\n","        logits = self.unembed(normalized_resid_final)\n","        # logits have shape [batch, position, logits]\n","        return logits\n","\n","extracted_model = ExtractedModel(model)"],"metadata":{"id":"f0zNK7Kvo3Xo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_prompt(\"1 2 3 4\", \" 5\", extracted_model, model, prepend_bos=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1689426062339,"user_tz":240,"elapsed":9,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"a3e45d76-0844-4fb8-d729-6c12b1c30196","id":"qIrmCfnPo3Xx"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Tokenized prompt: ['<|endoftext|>', '1', ' 2', ' 3', ' 4']\n","Tokenized answer: [' 5']\n","Performance on answer token:\n","[b]Rank: 4        Logit: 20.50 Prob:  1.07% Token: | 5|[/b]\n","Top 0th token. Logit: 24.64 Prob: 67.40% Token: | 4|\n","Top 1th token. Logit: 23.67 Prob: 25.44% Token: | ..........|\n","Top 2th token. Logit: 20.82 Prob:  1.48% Token: | <!--|\n","Top 3th token. Logit: 20.73 Prob:  1.35% Token: | ★|\n","Top 4th token. Logit: 20.50 Prob:  1.07% Token: | 5|\n","Top 5th token. Logit: 20.14 Prob:  0.75% Token: | Tycoon|\n","Top 6th token. Logit: 19.83 Prob:  0.55% Token: | 6|\n","Top 7th token. Logit: 18.99 Prob:  0.24% Token: | 3|\n","Top 8th token. Logit: 18.52 Prob:  0.15% Token: | 8|\n","Top 9th token. Logit: 18.44 Prob:  0.14% Token: | Next|\n","[b]Ranks of the answer tokens:[/b] [(' 5', 4)]\n"]}]},{"cell_type":"markdown","source":["As we saw in logit lens, L0 to L7 is required to recognize the next should be a number"],"metadata":{"id":"RkS7wLyQo_ct"}},{"cell_type":"markdown","source":["### L0 to L7, skip 8, go only to 9"],"metadata":{"id":"jYFX2fEepfdF"}},{"cell_type":"code","source":["import torch.nn as nn\n","\n","class ExtractedModel(nn.Module):\n","    def __init__(self, original_model):\n","        super(ExtractedModel, self).__init__()\n","        self.embed = original_model.embed\n","        self.pos_embed = original_model.pos_embed\n","        self.blocks = original_model.blocks\n","        self.ln_final = original_model.ln_final\n","        self.unembed = original_model.unembed\n","\n","    def forward(self, tokens):\n","        # tokens [batch, position]\n","        embed = self.embed(tokens)\n","        pos_embed = self.pos_embed(tokens)\n","        residual = embed + pos_embed\n","\n","        for block in self.blocks[0:8] + self.blocks[9:10]:\n","            residual = block(residual)\n","\n","        normalized_resid_final = self.ln_final(residual)\n","        logits = self.unembed(normalized_resid_final)\n","        # logits have shape [batch, position, logits]\n","        return logits\n","\n","extracted_model = ExtractedModel(model)"],"metadata":{"id":"fxLyrxu3pqUQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_prompt(\"1 2 3 4\", \" 5\", extracted_model, model, prepend_bos=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1689426254217,"user_tz":240,"elapsed":182,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"6b7d3ff7-26c5-4267-c164-7e80a0e5fe2b","id":"V0xlHztSpqUZ"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Tokenized prompt: ['<|endoftext|>', '1', ' 2', ' 3', ' 4']\n","Tokenized answer: [' 5']\n","Performance on answer token:\n","[b]Rank: 0        Logit: 31.84 Prob: 99.94% Token: | 5|[/b]\n","Top 0th token. Logit: 31.84 Prob: 99.94% Token: | 5|\n","Top 1th token. Logit: 23.68 Prob:  0.03% Token: | Next|\n","Top 2th token. Logit: 23.00 Prob:  0.01% Token: | 4|\n","Top 3th token. Logit: 22.21 Prob:  0.01% Token: | 6|\n","Top 4th token. Logit: 21.70 Prob:  0.00% Token: |Next|\n","Top 5th token. Logit: 21.10 Prob:  0.00% Token: | 7|\n","Top 6th token. Logit: 20.47 Prob:  0.00% Token: | <!--|\n","Top 7th token. Logit: 20.00 Prob:  0.00% Token: | Five|\n","Top 8th token. Logit: 19.96 Prob:  0.00% Token: | Player|\n","Top 9th token. Logit: 19.03 Prob:  0.00% Token: | 9|\n","[b]Ranks of the answer tokens:[/b] [(' 5', 0)]\n"]}]},{"cell_type":"markdown","source":["We see L8 is not important for this. Excellent performance by skipping it! It is not as good as the 99.98% if we kept L8, but its contribution is trivial. This is odd, seeing that 8.11 does \"soemthing\"; perhaps it acts as a backup."],"metadata":{"id":"UcmBdDvqpuiY"}},{"cell_type":"code","source":["test_prompt(\"January February March April\", \" May\", extracted_model, model, prepend_bos=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u3caVMUhCeNk","executionInfo":{"status":"ok","timestamp":1689432746694,"user_tz":240,"elapsed":315,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"ab9a82db-7001-4dcf-95ff-a3b216300c08"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Tokenized prompt: ['<|endoftext|>', 'January', ' February', ' March', ' April']\n","Tokenized answer: [' May']\n","Performance on answer token:\n","[b]Rank: 0        Logit: 26.42 Prob: 77.81% Token: | May|[/b]\n","Top 0th token. Logit: 26.42 Prob: 77.81% Token: | May|\n","Top 1th token. Logit: 24.63 Prob: 13.05% Token: | 5|\n","Top 2th token. Logit: 23.03 Prob:  2.63% Token: | 2015|\n","Top 3th token. Logit: 22.87 Prob:  2.23% Token: | 2005|\n","Top 4th token. Logit: 21.28 Prob:  0.46% Token: | April|\n","Top 5th token. Logit: 21.26 Prob:  0.45% Token: | 05|\n","Top 6th token. Logit: 21.20 Prob:  0.42% Token: | 25|\n","Top 7th token. Logit: 20.89 Prob:  0.31% Token: | June|\n","Top 8th token. Logit: 20.13 Prob:  0.14% Token: | 1995|\n","Top 9th token. Logit: 20.05 Prob:  0.13% Token: |\n","|\n","[b]Ranks of the answer tokens:[/b] [(' May', 0)]\n"]}]},{"cell_type":"code","source":["test_prompt(\"Sunday Monday Tuesday Wednesday\", \" Thursday\", extracted_model, model, prepend_bos=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1689433137676,"user_tz":240,"elapsed":485,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"752ba31a-c824-4906-da5c-b1280c0b519c","id":"n5-filDMD7Tx"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Tokenized prompt: ['<|endoftext|>', 'Sunday', ' Monday', ' Tuesday', ' Wednesday']\n","Tokenized answer: [' Thursday']\n","Performance on answer token:\n","[b]Rank: 0        Logit: 31.53 Prob: 87.14% Token: | Thursday|[/b]\n","Top 0th token. Logit: 31.53 Prob: 87.14% Token: | Thursday|\n","Top 1th token. Logit: 29.13 Prob:  7.88% Token: | Wednesday|\n","Top 2th token. Logit: 27.48 Prob:  1.52% Token: | September|\n","Top 3th token. Logit: 27.10 Prob:  1.04% Token: | January|\n","Top 4th token. Logit: 26.31 Prob:  0.47% Token: | Friday|\n","Top 5th token. Logit: 26.14 Prob:  0.40% Token: | March|\n","Top 6th token. Logit: 25.95 Prob:  0.33% Token: | December|\n","Top 7th token. Logit: 25.89 Prob:  0.31% Token: | Thurs|\n","Top 8th token. Logit: 25.68 Prob:  0.25% Token: | Saturday|\n","Top 9th token. Logit: 25.21 Prob:  0.16% Token: | July|\n","[b]Ranks of the answer tokens:[/b] [(' Thursday', 0)]\n"]}]},{"cell_type":"code","source":["test_prompt(\"A B C D\", \" E\", extracted_model, model, prepend_bos=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1689433142604,"user_tz":240,"elapsed":336,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"5eac2ffd-ab9e-4927-b983-a93107988c51","id":"HgRryjq3D7Ty"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Tokenized prompt: ['<|endoftext|>', 'A', ' B', ' C', ' D']\n","Tokenized answer: [' E']\n","Performance on answer token:\n","[b]Rank: 0        Logit: 22.92 Prob: 54.31% Token: | E|[/b]\n","Top 0th token. Logit: 22.92 Prob: 54.31% Token: | E|\n","Top 1th token. Logit: 22.65 Prob: 41.40% Token: | D|\n","Top 2th token. Logit: 18.40 Prob:  0.59% Token: | F|\n","Top 3th token. Logit: 18.25 Prob:  0.51% Token: | d|\n","Top 4th token. Logit: 17.57 Prob:  0.26% Token: | T|\n","Top 5th token. Logit: 17.52 Prob:  0.24% Token: |\n","|\n","Top 6th token. Logit: 17.47 Prob:  0.23% Token: | G|\n","Top 7th token. Logit: 17.10 Prob:  0.16% Token: | e|\n","Top 8th token. Logit: 16.75 Prob:  0.11% Token: | 3|\n","Top 9th token. Logit: 16.64 Prob:  0.10% Token: | Y|\n","[b]Ranks of the answer tokens:[/b] [(' E', 0)]\n"]}]},{"cell_type":"markdown","source":["But the alphabet gets worse without 8"],"metadata":{"id":"IzoxvcENEFKM"}},{"cell_type":"code","source":["test_prompt(\"A B C D\", \" E\", extracted_model_full, model, prepend_bos=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qj4eIt85ECTR","executionInfo":{"status":"ok","timestamp":1689433161838,"user_tz":240,"elapsed":466,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"ea5a96c2-6674-4064-bf7f-4f5e93af1007"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Tokenized prompt: ['<|endoftext|>', 'A', ' B', ' C', ' D']\n","Tokenized answer: [' E']\n","Performance on answer token:\n","[b]Rank: 0        Logit: 24.16 Prob: 99.24% Token: | E|[/b]\n","Top 0th token. Logit: 24.16 Prob: 99.24% Token: | E|\n","Top 1th token. Logit: 18.62 Prob:  0.39% Token: | e|\n","Top 2th token. Logit: 17.16 Prob:  0.09% Token: | F|\n","Top 3th token. Logit: 16.45 Prob:  0.04% Token: | G|\n","Top 4th token. Logit: 15.88 Prob:  0.03% Token: | É|\n","Top 5th token. Logit: 15.80 Prob:  0.02% Token: | 1|\n","Top 6th token. Logit: 15.03 Prob:  0.01% Token: | D|\n","Top 7th token. Logit: 15.01 Prob:  0.01% Token: | ER|\n","Top 8th token. Logit: 14.57 Prob:  0.01% Token: | 2|\n","Top 9th token. Logit: 14.55 Prob:  0.01% Token: |E|\n","[b]Ranks of the answer tokens:[/b] [(' E', 0)]\n"]}]},{"cell_type":"markdown","source":["### skip attention in L9, only use MLP9"],"metadata":{"id":"-cGX7o8ZpXIH"}},{"cell_type":"code","source":["import torch.nn as nn\n","\n","class ExtractedModel(nn.Module):\n","    def __init__(self, original_model):\n","        super(ExtractedModel, self).__init__()\n","        self.embed = original_model.embed\n","        self.pos_embed = original_model.pos_embed\n","\n","        self.blocks = original_model.blocks\n","\n","        # self.ln1 = original_model.blocks[9].ln1\n","        # self.attn = original_model.blocks[9].attn\n","        # self.ln2 = original_model.blocks[9].ln2\n","        # self.mlp = original_model.blocks[9].mlp  # the MLP layer in the 9th transformer block\n","\n","        self.ln_final = original_model.ln_final\n","        self.unembed = original_model.unembed\n","\n","    def forward(self, tokens):\n","        # tokens [batch, position]\n","        embed = self.embed(tokens)\n","        pos_embed = self.pos_embed(tokens)\n","        residual = embed + pos_embed\n","\n","        for block in self.blocks[0:8]:\n","            residual = block(residual)\n","\n","        # normalized_resid_pre = self.ln1(residual)\n","        # attn_out = self.attn(normalized_resid_pre, normalized_resid_pre, normalized_resid_pre)\n","        # residual = residual + attn_out\n","\n","        normalized_resid_mid = self.blocks[9].ln2(residual)\n","        mlp_out = self.blocks[9].mlp(normalized_resid_mid)\n","        residual = residual + mlp_out\n","\n","        normalized_resid_final = self.ln_final(residual)\n","        logits = self.unembed(normalized_resid_final)\n","        # logits have shape [batch, position, logits]\n","        return logits\n","\n","extracted_model = ExtractedModel(model)"],"metadata":{"id":"VTge8t9oq2hL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_prompt(\"1 2 3 4\", \" 5\", extracted_model, model, prepend_bos=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1689427207033,"user_tz":240,"elapsed":184,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"d5ff6498-dc2b-41ec-ac18-ed58b1cee136","id":"RcQNReGOq2hV"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Tokenized prompt: ['<|endoftext|>', '1', ' 2', ' 3', ' 4']\n","Tokenized answer: [' 5']\n","Performance on answer token:\n","[b]Rank: 0        Logit: 28.38 Prob: 98.87% Token: | 5|[/b]\n","Top 0th token. Logit: 28.38 Prob: 98.87% Token: | 5|\n","Top 1th token. Logit: 22.89 Prob:  0.41% Token: | 4|\n","Top 2th token. Logit: 22.12 Prob:  0.19% Token: | <!--|\n","Top 3th token. Logit: 21.41 Prob:  0.09% Token: | Next|\n","Top 4th token. Logit: 21.16 Prob:  0.07% Token: | 3|\n","Top 5th token. Logit: 20.93 Prob:  0.06% Token: | 7|\n","Top 6th token. Logit: 20.76 Prob:  0.05% Token: | ★|\n","Top 7th token. Logit: 20.61 Prob:  0.04% Token: |★|\n","Top 8th token. Logit: 20.17 Prob:  0.03% Token: | ..........|\n","Top 9th token. Logit: 19.97 Prob:  0.02% Token: |\n","|\n","[b]Ranks of the answer tokens:[/b] [(' 5', 0)]\n"]}]},{"cell_type":"markdown","source":["It doesn't even need attention L9, though it helps it give it a full 1%. it just needs the MLP. This is consistent with vector addition notebook."],"metadata":{"id":"hcE3I_uitX0a"}},{"cell_type":"markdown","source":["Vector addition showed only adding output of MLP to residual recovered\n","\n","it seems L0 to L7 do preprocessing to be fit for MLP adding. So having \"just\" MLP9 isn't enough, it needs to get the digit. Then MLP convert 4 to 5, or April to May, etc."],"metadata":{"id":"vJE1QjHosSIM"}},{"cell_type":"code","source":["test_prompt(\"January February March April\", \" May\", extracted_model, model, prepend_bos=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rgDd66vBDIds","executionInfo":{"status":"ok","timestamp":1689433039524,"user_tz":240,"elapsed":313,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"afffa33b-d95f-4536-e92e-d0ace2959baa"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Tokenized prompt: ['<|endoftext|>', 'January', ' February', ' March', ' April']\n","Tokenized answer: [' May']\n","Performance on answer token:\n","[b]Rank: 2        Logit: 22.27 Prob:  9.29% Token: | May|[/b]\n","Top 0th token. Logit: 23.13 Prob: 21.98% Token: | 5|\n","Top 1th token. Logit: 22.79 Prob: 15.61% Token: | 2015|\n","Top 2th token. Logit: 22.27 Prob:  9.29% Token: | May|\n","Top 3th token. Logit: 21.64 Prob:  4.97% Token: | 25|\n","Top 4th token. Logit: 21.57 Prob:  4.59% Token: | 2005|\n","Top 5th token. Logit: 21.25 Prob:  3.35% Token: | Top|\n","Top 6th token. Logit: 21.22 Prob:  3.25% Token: | 27|\n","Top 7th token. Logit: 21.03 Prob:  2.70% Token: | Category|\n","Top 8th token. Logit: 20.83 Prob:  2.19% Token: | 2017|\n","Top 9th token. Logit: 20.57 Prob:  1.70% Token: | 29|\n","[b]Ranks of the answer tokens:[/b] [(' May', 2)]\n"]}]},{"cell_type":"markdown","source":["Wait, without attention in L9, it becomes 5? Did we just uncover the link between the primordial and modern covering?"],"metadata":{"id":"ScFU1uXRDQU5"}},{"cell_type":"code","source":["test_prompt(\"Sunday Monday Tuesday Wednesday\", \" Thursday\", extracted_model, model, prepend_bos=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9TqY980sDWjt","executionInfo":{"status":"ok","timestamp":1689432994710,"user_tz":240,"elapsed":320,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"9bcea4ab-ad82-4bf8-bc24-afd9ae710db2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Tokenized prompt: ['<|endoftext|>', 'Sunday', ' Monday', ' Tuesday', ' Wednesday']\n","Tokenized answer: [' Thursday']\n","Performance on answer token:\n","[b]Rank: 0        Logit: 26.03 Prob: 26.03% Token: | Thursday|[/b]\n","Top 0th token. Logit: 26.03 Prob: 26.03% Token: | Thursday|\n","Top 1th token. Logit: 25.61 Prob: 17.13% Token: | Wednesday|\n","Top 2th token. Logit: 25.35 Prob: 13.13% Token: | September|\n","Top 3th token. Logit: 25.01 Prob:  9.39% Token: | July|\n","Top 4th token. Logit: 24.89 Prob:  8.33% Token: | January|\n","Top 5th token. Logit: 24.23 Prob:  4.29% Token: | December|\n","Top 6th token. Logit: 24.19 Prob:  4.14% Token: | nights|\n","Top 7th token. Logit: 23.72 Prob:  2.59% Token: | March|\n","Top 8th token. Logit: 23.46 Prob:  1.98% Token: | November|\n","Top 9th token. Logit: 23.09 Prob:  1.38% Token: | night|\n","[b]Ranks of the answer tokens:[/b] [(' Thursday', 0)]\n"]}]},{"cell_type":"markdown","source":["Nope, this still retains it as Thursday"],"metadata":{"id":"U-JtRh1-DcBJ"}},{"cell_type":"code","source":["test_prompt(\"A B C D\", \" E\", extracted_model, model, prepend_bos=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n0Jcr7I5DfDq","executionInfo":{"status":"ok","timestamp":1689433018913,"user_tz":240,"elapsed":8,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"a2dc9dd7-dcca-4668-d765-f8938ba2501d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Tokenized prompt: ['<|endoftext|>', 'A', ' B', ' C', ' D']\n","Tokenized answer: [' E']\n","Performance on answer token:\n","[b]Rank: 1        Logit: 21.58 Prob: 20.49% Token: | E|[/b]\n","Top 0th token. Logit: 22.75 Prob: 65.99% Token: | D|\n","Top 1th token. Logit: 21.58 Prob: 20.49% Token: | E|\n","Top 2th token. Logit: 19.23 Prob:  1.96% Token: | G|\n","Top 3th token. Logit: 19.04 Prob:  1.62% Token: | T|\n","Top 4th token. Logit: 18.83 Prob:  1.32% Token: | C|\n","Top 5th token. Logit: 18.76 Prob:  1.22% Token: |\n","|\n","Top 6th token. Logit: 18.35 Prob:  0.81% Token: | F|\n","Top 7th token. Logit: 17.81 Prob:  0.47% Token: | +|\n","Top 8th token. Logit: 17.57 Prob:  0.37% Token: | O|\n","Top 9th token. Logit: 17.56 Prob:  0.37% Token: | L|\n","[b]Ranks of the answer tokens:[/b] [(' E', 1)]\n"]}]},{"cell_type":"markdown","source":["This also isn't saying \"5\"."],"metadata":{"id":"q9IYw2anDh4L"}},{"cell_type":"markdown","source":["### skip MLP9, only use attn9"],"metadata":{"id":"VU-dlRYLuzbi"}},{"cell_type":"code","source":["import torch.nn as nn\n","\n","class ExtractedModel(nn.Module):\n","    def __init__(self, original_model):\n","        super(ExtractedModel, self).__init__()\n","        self.embed = original_model.embed\n","        self.pos_embed = original_model.pos_embed\n","\n","        self.blocks = original_model.blocks\n","\n","        # self.ln1 = original_model.blocks[9].ln1\n","        # self.attn = original_model.blocks[9].attn\n","        # self.ln2 = original_model.blocks[9].ln2\n","        # self.mlp = original_model.blocks[9].mlp  # the MLP layer in the 9th transformer block\n","\n","        self.ln_final = original_model.ln_final\n","        self.unembed = original_model.unembed\n","\n","    def forward(self, tokens):\n","        # tokens [batch, position]\n","        embed = self.embed(tokens)\n","        pos_embed = self.pos_embed(tokens)\n","        residual = embed + pos_embed\n","\n","        for block in self.blocks[0:8]:\n","            residual = block(residual)\n","\n","        normalized_resid_pre = self.blocks[9].ln1(residual)\n","        attn_out = self.blocks[9].attn(normalized_resid_pre, normalized_resid_pre, normalized_resid_pre)\n","        residual = residual + attn_out\n","\n","        # normalized_resid_mid = self.blocks[9].ln2(residual)\n","        # mlp_out = self.blocks[9].mlp(normalized_resid_mid)\n","        # residual = residual + mlp_out\n","\n","        normalized_resid_final = self.ln_final(residual)\n","        logits = self.unembed(normalized_resid_final)\n","        # logits have shape [batch, position, logits]\n","        return logits\n","\n","extracted_model = ExtractedModel(model)"],"metadata":{"id":"AfLerN3Su47Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_prompt(\"1 2 3 4\", \" 5\", extracted_model, model, prepend_bos=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1689427637847,"user_tz":240,"elapsed":175,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"a85e3a7f-c949-4a3f-b239-2ec9b99277ed","id":"LrpKCb8uu47i"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Tokenized prompt: ['<|endoftext|>', '1', ' 2', ' 3', ' 4']\n","Tokenized answer: [' 5']\n","Performance on answer token:\n","[b]Rank: 1        Logit: 25.61 Prob: 10.24% Token: | 5|[/b]\n","Top 0th token. Logit: 27.76 Prob: 87.32% Token: | 4|\n","Top 1th token. Logit: 25.61 Prob: 10.24% Token: | 5|\n","Top 2th token. Logit: 23.55 Prob:  1.30% Token: | 6|\n","Top 3th token. Logit: 22.56 Prob:  0.48% Token: |Next|\n","Top 4th token. Logit: 21.60 Prob:  0.19% Token: | Next|\n","Top 5th token. Logit: 21.51 Prob:  0.17% Token: | ..........|\n","Top 6th token. Logit: 20.95 Prob:  0.10% Token: | 8|\n","Top 7th token. Logit: 20.63 Prob:  0.07% Token: | <!--|\n","Top 8th token. Logit: 20.35 Prob:  0.05% Token: | 7|\n","Top 9th token. Logit: 19.23 Prob:  0.02% Token: | 9|\n","[b]Ranks of the answer tokens:[/b] [(' 5', 1)]\n"]}]},{"cell_type":"markdown","source":["As expected, attn only 9 doesn't allow it to change to 5. The MLP, as seen in vector addition, is what's impt"],"metadata":{"id":"Rf2nO3ucvAnB"}},{"cell_type":"markdown","source":["What's strange is that we can't directly pass digit tokens through MLP 9; it needs to be in the form after L7, and needs the layers before"],"metadata":{"id":"OSIwG9LbyF8g"}},{"cell_type":"markdown","source":["### skip L9 only"],"metadata":{"id":"fSgxBcWcyMa-"}},{"cell_type":"markdown","source":["Are there backups, say in L10 and L11?"],"metadata":{"id":"ufVN-DykyaAR"}},{"cell_type":"code","source":["import torch.nn as nn\n","\n","class ExtractedModel(nn.Module):\n","    def __init__(self, original_model):\n","        super(ExtractedModel, self).__init__()\n","        self.embed = original_model.embed\n","        self.pos_embed = original_model.pos_embed\n","        self.blocks = original_model.blocks\n","        self.ln_final = original_model.ln_final\n","        self.unembed = original_model.unembed\n","\n","    def forward(self, tokens):\n","        # tokens [batch, position]\n","        embed = self.embed(tokens)\n","        pos_embed = self.pos_embed(tokens)\n","        residual = embed + pos_embed\n","\n","        for block in self.blocks[0:9] + self.blocks[10:12]:\n","            residual = block(residual)\n","\n","        normalized_resid_final = self.ln_final(residual)\n","        logits = self.unembed(normalized_resid_final)\n","        # logits have shape [batch, position, logits]\n","        return logits\n","\n","extracted_model = ExtractedModel(model)"],"metadata":{"id":"ta0gGGOSycM2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_prompt(\"1 2 3 4\", \" 5\", extracted_model, model, prepend_bos=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1689428556439,"user_tz":240,"elapsed":200,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"675e451f-156c-45c6-f120-62eea8a7d410","id":"kdf6JIeXycM_"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Tokenized prompt: ['<|endoftext|>', '1', ' 2', ' 3', ' 4']\n","Tokenized answer: [' 5']\n","Performance on answer token:\n","[b]Rank: 1        Logit: 14.61 Prob:  8.00% Token: | 5|[/b]\n","Top 0th token. Logit: 16.54 Prob: 55.15% Token: | 4|\n","Top 1th token. Logit: 14.61 Prob:  8.00% Token: | 5|\n","Top 2th token. Logit: 14.43 Prob:  6.73% Token: | 1|\n","Top 3th token. Logit: 14.33 Prob:  6.05% Token: | 3|\n","Top 4th token. Logit: 13.74 Prob:  3.36% Token: | 6|\n","Top 5th token. Logit: 13.72 Prob:  3.29% Token: |\n","|\n","Top 6th token. Logit: 13.37 Prob:  2.32% Token: | 2|\n","Top 7th token. Logit: 13.08 Prob:  1.74% Token: | >|\n","Top 8th token. Logit: 12.19 Prob:  0.71% Token: | [|\n","Top 9th token. Logit: 12.00 Prob:  0.59% Token: | 8|\n","[b]Ranks of the answer tokens:[/b] [(' 5', 1)]\n"]}]},{"cell_type":"markdown","source":["No; L9 seems required to boost up 5 a lot. It's not \"converting\" 4 to 5, but making 5 be vastly more predicted."],"metadata":{"id":"D-Kv3nHXygVa"}},{"cell_type":"markdown","source":["### use attn only for L0 to L7, then add MLP9"],"metadata":{"id":"piNYV2JayrER"}},{"cell_type":"code","source":["import torch.nn as nn\n","\n","class ExtractedModel(nn.Module):\n","    def __init__(self, original_model):\n","        super(ExtractedModel, self).__init__()\n","        self.embed = original_model.embed\n","        self.pos_embed = original_model.pos_embed\n","        self.blocks = original_model.blocks\n","        self.ln_final = original_model.ln_final\n","        self.unembed = original_model.unembed\n","\n","    def forward(self, tokens):\n","        # tokens [batch, position]\n","        embed = self.embed(tokens)\n","        pos_embed = self.pos_embed(tokens)\n","        residual = embed + pos_embed\n","\n","        for block in self.blocks[0:8]:\n","            normalized_resid_pre = block.ln1(residual)\n","            attn_out = block.attn(normalized_resid_pre, normalized_resid_pre, normalized_resid_pre)\n","            residual = residual + attn_out\n","\n","        normalized_resid_mid = self.blocks[9].ln2(residual)\n","        mlp_out = self.blocks[9].mlp(normalized_resid_mid)\n","        residual = residual + mlp_out\n","\n","        normalized_resid_final = self.ln_final(residual)\n","        logits = self.unembed(normalized_resid_final)\n","        # logits have shape [batch, position, logits]\n","        return logits\n","\n","extracted_model = ExtractedModel(model)"],"metadata":{"id":"UjUXwy-Tyxmx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_prompt(\"1 2 3 4\", \" 5\", extracted_model, model, prepend_bos=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1689428717626,"user_tz":240,"elapsed":143,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"be156827-7b13-4ca2-fa61-246ef9cae9a8","id":"ancx0uDZyxm7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Tokenized prompt: ['<|endoftext|>', '1', ' 2', ' 3', ' 4']\n","Tokenized answer: [' 5']\n","Performance on answer token:\n","[b]Rank: 3602     Logit:  6.59 Prob:  0.00% Token: | 5|[/b]\n","Top 0th token. Logit: 19.24 Prob: 29.06% Token: | ›|\n","Top 1th token. Logit: 19.06 Prob: 24.33% Token: | Posted|\n","Top 2th token. Logit: 17.82 Prob:  6.99% Token: | 2018|\n","Top 3th token. Logit: 17.76 Prob:  6.60% Token: |!|\n","Top 4th token. Logit: 17.46 Prob:  4.90% Token: | !|\n","Top 5th token. Logit: 16.97 Prob:  2.99% Token: | 2017|\n","Top 6th token. Logit: 16.40 Prob:  1.70% Token: | 2015|\n","Top 7th token. Logit: 16.25 Prob:  1.46% Token: | dash|\n","Top 8th token. Logit: 16.22 Prob:  1.43% Token: | Xiaomi|\n","Top 9th token. Logit: 15.71 Prob:  0.85% Token: | Profile|\n","[b]Ranks of the answer tokens:[/b] [(' 5', 3602)]\n"]}]},{"cell_type":"markdown","source":["So they're not just moving info; MLPs are required"],"metadata":{"id":"1mcplH0IzHvY"}},{"cell_type":"markdown","source":["### skip L4 only (for L0 to 9)"],"metadata":{"id":"vB3vClaYyzFA"}},{"cell_type":"code","source":["import torch.nn as nn\n","\n","class ExtractedModel(nn.Module):\n","    def __init__(self, original_model):\n","        super(ExtractedModel, self).__init__()\n","        self.embed = original_model.embed\n","        self.pos_embed = original_model.pos_embed\n","        self.blocks = original_model.blocks\n","        self.ln_final = original_model.ln_final\n","        self.unembed = original_model.unembed\n","\n","    def forward(self, tokens):\n","        # tokens [batch, position]\n","        embed = self.embed(tokens)\n","        pos_embed = self.pos_embed(tokens)\n","        residual = embed + pos_embed\n","\n","        for block in self.blocks[0:4] + self.blocks[5:10]:\n","            residual = block(residual)\n","\n","        normalized_resid_final = self.ln_final(residual)\n","        logits = self.unembed(normalized_resid_final)\n","        # logits have shape [batch, position, logits]\n","        return logits\n","\n","extracted_model = ExtractedModel(model)"],"metadata":{"id":"Gb9H6lrZzaE4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_prompt(\"1 2 3 4\", \" 5\", extracted_model, model, prepend_bos=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1689428811020,"user_tz":240,"elapsed":179,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"6ba6834d-eef5-499e-c3f0-72e7380734d6","id":"zOvrhE0XzaFC"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Tokenized prompt: ['<|endoftext|>', '1', ' 2', ' 3', ' 4']\n","Tokenized answer: [' 5']\n","Performance on answer token:\n","[b]Rank: 0        Logit: 34.78 Prob: 99.36% Token: | 5|[/b]\n","Top 0th token. Logit: 34.78 Prob: 99.36% Token: | 5|\n","Top 1th token. Logit: 29.57 Prob:  0.54% Token: | 4|\n","Top 2th token. Logit: 27.22 Prob:  0.05% Token: | 3|\n","Top 3th token. Logit: 26.65 Prob:  0.03% Token: | 6|\n","Top 4th token. Logit: 25.88 Prob:  0.01% Token: | 7|\n","Top 5th token. Logit: 24.49 Prob:  0.00% Token: | 9|\n","Top 6th token. Logit: 23.41 Prob:  0.00% Token: | 1|\n","Top 7th token. Logit: 22.98 Prob:  0.00% Token: | 0|\n","Top 8th token. Logit: 22.79 Prob:  0.00% Token: | 8|\n","Top 9th token. Logit: 22.22 Prob:  0.00% Token: | 10|\n","[b]Ranks of the answer tokens:[/b] [(' 5', 0)]\n"]}]},{"cell_type":"markdown","source":["Wait, L4 isn't required either"],"metadata":{"id":"cyPd1dGwzf5n"}},{"cell_type":"code","source":["test_prompt(\"one two three four\", \" five\", extracted_model, model, prepend_bos=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1689433839524,"user_tz":240,"elapsed":324,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"71e5ea2d-2587-4a65-fb6c-b4d0a13be60a","id":"aqCF8lePGnjS"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Tokenized prompt: ['<|endoftext|>', 'one', ' two', ' three', ' four']\n","Tokenized answer: [' five']\n","Performance on answer token:\n","[b]Rank: 1        Logit: 24.22 Prob: 27.46% Token: | five|[/b]\n","Top 0th token. Logit: 24.82 Prob: 49.85% Token: | four|\n","Top 1th token. Logit: 24.22 Prob: 27.46% Token: | five|\n","Top 2th token. Logit: 22.50 Prob:  4.90% Token: | six|\n","Top 3th token. Logit: 22.33 Prob:  4.14% Token: | seven|\n","Top 4th token. Logit: 22.23 Prob:  3.72% Token: | Four|\n","Top 5th token. Logit: 21.99 Prob:  2.93% Token: |four|\n","Top 6th token. Logit: 21.92 Prob:  2.75% Token: | three|\n","Top 7th token. Logit: 21.40 Prob:  1.63% Token: | eight|\n","Top 8th token. Logit: 20.01 Prob:  0.41% Token: |teen|\n","Top 9th token. Logit: 19.90 Prob:  0.36% Token: | nine|\n","[b]Ranks of the answer tokens:[/b] [(' five', 1)]\n"]}]},{"cell_type":"markdown","source":["But it is for number words, months, etc except alphabets (which is aided by MLP8)\n","\n","In all cases, however, it is MLP9 which adds to it"],"metadata":{"id":"Jvb2KzoMGr2B"}},{"cell_type":"code","source":["test_prompt(\"January February March April\", \" May\", extracted_model, model, prepend_bos=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2CIcBwCcH9Ps","executionInfo":{"status":"ok","timestamp":1689434185416,"user_tz":240,"elapsed":327,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"1277175d-18c4-44a9-e608-97063393f326"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Tokenized prompt: ['<|endoftext|>', 'January', ' February', ' March', ' April']\n","Tokenized answer: [' May']\n","Performance on answer token:\n","[b]Rank: 1        Logit: 34.11 Prob: 17.75% Token: | May|[/b]\n","Top 0th token. Logit: 35.59 Prob: 77.57% Token: | April|\n","Top 1th token. Logit: 34.11 Prob: 17.75% Token: | May|\n","Top 2th token. Logit: 32.01 Prob:  2.17% Token: | June|\n","Top 3th token. Logit: 31.39 Prob:  1.16% Token: | March|\n","Top 4th token. Logit: 30.53 Prob:  0.49% Token: | September|\n","Top 5th token. Logit: 29.95 Prob:  0.27% Token: | July|\n","Top 6th token. Logit: 29.15 Prob:  0.12% Token: | October|\n","Top 7th token. Logit: 29.08 Prob:  0.12% Token: | February|\n","Top 8th token. Logit: 28.96 Prob:  0.10% Token: | August|\n","Top 9th token. Logit: 28.77 Prob:  0.08% Token: | November|\n","[b]Ranks of the answer tokens:[/b] [(' May', 1)]\n"]}]},{"cell_type":"code","source":["test_prompt(\"Sunday Monday Tuesday Wednesday\", \" Thursday\", extracted_model, model, prepend_bos=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1689434050743,"user_tz":240,"elapsed":333,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"cf1bacfa-60b4-4ced-c9c0-278403404997","id":"zF9SGH48Hahp"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Tokenized prompt: ['<|endoftext|>', 'Sunday', ' Monday', ' Tuesday', ' Wednesday']\n","Tokenized answer: [' Thursday']\n","Performance on answer token:\n","[b]Rank: 18       Logit: 27.14 Prob:  0.77% Token: | Thursday|[/b]\n","Top 0th token. Logit: 30.78 Prob: 29.15% Token: | morning|\n","Top 1th token. Logit: 29.64 Prob:  9.36% Token: | July|\n","Top 2th token. Logit: 29.49 Prob:  8.05% Token: | February|\n","Top 3th token. Logit: 29.27 Prob:  6.44% Token: | afternoon|\n","Top 4th token. Logit: 29.25 Prob:  6.32% Token: | September|\n","Top 5th token. Logit: 29.22 Prob:  6.18% Token: | June|\n","Top 6th token. Logit: 28.79 Prob:  4.00% Token: | January|\n","Top 7th token. Logit: 28.79 Prob:  3.99% Token: | evening|\n","Top 8th token. Logit: 28.73 Prob:  3.76% Token: | November|\n","Top 9th token. Logit: 28.68 Prob:  3.60% Token: | October|\n","[b]Ranks of the answer tokens:[/b] [(' Thursday', 18)]\n"]}]},{"cell_type":"code","source":["test_prompt(\"A B C D\", \" E\", extracted_model, model, prepend_bos=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1689434056022,"user_tz":240,"elapsed":346,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"6e42675e-dbc7-4418-e905-4fd1916c52f2","id":"sQGq1o37Hahy"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Tokenized prompt: ['<|endoftext|>', 'A', ' B', ' C', ' D']\n","Tokenized answer: [' E']\n","Performance on answer token:\n","[b]Rank: 0        Logit: 26.32 Prob: 97.62% Token: | E|[/b]\n","Top 0th token. Logit: 26.32 Prob: 97.62% Token: | E|\n","Top 1th token. Logit: 21.87 Prob:  1.14% Token: | D|\n","Top 2th token. Logit: 21.18 Prob:  0.57% Token: | F|\n","Top 3th token. Logit: 19.71 Prob:  0.13% Token: | G|\n","Top 4th token. Logit: 19.45 Prob:  0.10% Token: | e|\n","Top 5th token. Logit: 19.21 Prob:  0.08% Token: | +|\n","Top 6th token. Logit: 18.56 Prob:  0.04% Token: |\n","|\n","Top 7th token. Logit: 18.16 Prob:  0.03% Token: | H|\n","Top 8th token. Logit: 17.73 Prob:  0.02% Token: |+|\n","Top 9th token. Logit: 17.58 Prob:  0.02% Token: |ensity|\n","[b]Ranks of the answer tokens:[/b] [(' E', 0)]\n"]}]},{"cell_type":"markdown","source":["### skip L4, L8, L10 and L11"],"metadata":{"id":"ZI7y-UrVziIg"}},{"cell_type":"code","source":["import torch.nn as nn\n","\n","class ExtractedModel(nn.Module):\n","    def __init__(self, original_model):\n","        super(ExtractedModel, self).__init__()\n","        self.embed = original_model.embed\n","        self.pos_embed = original_model.pos_embed\n","        self.blocks = original_model.blocks\n","        self.ln_final = original_model.ln_final\n","        self.unembed = original_model.unembed\n","\n","    def forward(self, tokens):\n","        # tokens [batch, position]\n","        embed = self.embed(tokens)\n","        pos_embed = self.pos_embed(tokens)\n","        residual = embed + pos_embed\n","\n","        for block in self.blocks[0:4] + self.blocks[5:8] + self.blocks[9:10]:\n","            residual = block(residual)\n","\n","        normalized_resid_final = self.ln_final(residual)\n","        logits = self.unembed(normalized_resid_final)\n","        # logits have shape [batch, position, logits]\n","        return logits\n","\n","extracted_model = ExtractedModel(model)"],"metadata":{"id":"CC61Nq2rzrGw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_prompt(\"1 2 3 4\", \" 5\", extracted_model, model, prepend_bos=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1689428893691,"user_tz":240,"elapsed":8,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"a250e1ad-a61b-46c3-a0fc-a7bb5a57d420","id":"dxxhMSJ_zrG6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Tokenized prompt: ['<|endoftext|>', '1', ' 2', ' 3', ' 4']\n","Tokenized answer: [' 5']\n","Performance on answer token:\n","[b]Rank: 0        Logit: 36.43 Prob: 99.91% Token: | 5|[/b]\n","Top 0th token. Logit: 36.43 Prob: 99.91% Token: | 5|\n","Top 1th token. Logit: 28.80 Prob:  0.05% Token: | 4|\n","Top 2th token. Logit: 28.28 Prob:  0.03% Token: | 6|\n","Top 3th token. Logit: 26.55 Prob:  0.01% Token: | 7|\n","Top 4th token. Logit: 26.43 Prob:  0.00% Token: | 3|\n","Top 5th token. Logit: 24.44 Prob:  0.00% Token: | 1|\n","Top 6th token. Logit: 23.65 Prob:  0.00% Token: | 9|\n","Top 7th token. Logit: 23.44 Prob:  0.00% Token: | 0|\n","Top 8th token. Logit: 23.28 Prob:  0.00% Token: | 10|\n","Top 9th token. Logit: 22.61 Prob:  0.00% Token: | 8|\n","[b]Ranks of the answer tokens:[/b] [(' 5', 0)]\n"]}]},{"cell_type":"code","source":["my_list = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n","my_list = [str(i) for i in my_list]\n","indices_to_remove = [4, 8, 10]\n","indices_to_remove.sort(reverse=True)  # Start removing from the end to avoid index shifting\n","\n","for index in indices_to_remove:\n","    my_list.pop(index)\n","my_list"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fOZZFiGNz94g","executionInfo":{"status":"ok","timestamp":1689429180393,"user_tz":240,"elapsed":166,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"d8fd126b-0d3a-47d6-81b8-6aa22dee0baf"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['0', '1', '2', '3', '5', '6', '7', '9', '11']"]},"metadata":{},"execution_count":74}]},{"cell_type":"code","source":["import copy\n","model_copy = copy.deepcopy(model)\n","indices_to_remove = [4, 8, 10, 11]\n","indices_to_remove.sort(reverse=True)  # Start removing from the end to avoid index shifting\n","for index in indices_to_remove:\n","    model_copy.blocks.pop(index)\n","model_copy.blocks"],"metadata":{"id":"Pjpt74mL1CxL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.blocks  # check that copy didnt destroy original by ref"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"To4UtYlm1Rvj","executionInfo":{"status":"ok","timestamp":1689432549735,"user_tz":240,"elapsed":351,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"f713ba9a-754a-432e-d350-d0ac6ed19cc4"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["ModuleList(\n","  (0-11): 12 x TransformerBlock(\n","    (ln1): LayerNormPre(\n","      (hook_scale): HookPoint()\n","      (hook_normalized): HookPoint()\n","    )\n","    (ln2): LayerNormPre(\n","      (hook_scale): HookPoint()\n","      (hook_normalized): HookPoint()\n","    )\n","    (attn): Attention(\n","      (hook_k): HookPoint()\n","      (hook_q): HookPoint()\n","      (hook_v): HookPoint()\n","      (hook_z): HookPoint()\n","      (hook_attn_scores): HookPoint()\n","      (hook_pattern): HookPoint()\n","      (hook_result): HookPoint()\n","    )\n","    (mlp): MLP(\n","      (hook_pre): HookPoint()\n","      (hook_post): HookPoint()\n","    )\n","    (hook_q_input): HookPoint()\n","    (hook_k_input): HookPoint()\n","    (hook_v_input): HookPoint()\n","    (hook_attn_out): HookPoint()\n","    (hook_mlp_in): HookPoint()\n","    (hook_mlp_out): HookPoint()\n","    (hook_resid_pre): HookPoint()\n","    (hook_resid_mid): HookPoint()\n","    (hook_resid_post): HookPoint()\n","  )\n",")"]},"metadata":{},"execution_count":21}]},{"cell_type":"code","source":["import torch.nn as nn\n","\n","class ExtractedModel(nn.Module):\n","    def __init__(self, original_model_before_copy):\n","        super(ExtractedModel, self).__init__()\n","\n","        # don't do this every time run forward()\n","        original_model = copy.deepcopy(original_model_before_copy)\n","        indices_to_remove = [4, 8, 10, 11]\n","        indices_to_remove.sort(reverse=True)  # Start removing from the end to avoid index shifting\n","        for index in indices_to_remove:\n","            original_model.blocks.pop(index)\n","        self.blocks = original_model.blocks\n","\n","        self.embed = original_model.embed\n","        self.pos_embed = original_model.pos_embed\n","        self.blocks = original_model.blocks\n","        self.ln_final = original_model.ln_final\n","        self.unembed = original_model.unembed\n","\n","    def forward(self, tokens):\n","        # tokens [batch, position]\n","        embed = self.embed(tokens)\n","        pos_embed = self.pos_embed(tokens)\n","        residual = embed + pos_embed\n","\n","        for block in self.blocks:\n","            residual = block(residual)\n","\n","        normalized_resid_final = self.ln_final(residual)\n","        logits = self.unembed(normalized_resid_final)\n","        # logits have shape [batch, position, logits]\n","        return logits\n","\n","extracted_model = ExtractedModel(model)"],"metadata":{"id":"dLiy6duez4Ep"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_prompt(\"1 2 3 4\", \" 5\", extracted_model, model, prepend_bos=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OjnnFTYB0ojz","executionInfo":{"status":"ok","timestamp":1689432651915,"user_tz":240,"elapsed":342,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"fd4fb8af-f104-4ce1-9535-6a2e84ee091e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Tokenized prompt: ['<|endoftext|>', '1', ' 2', ' 3', ' 4']\n","Tokenized answer: [' 5']\n","Performance on answer token:\n","[b]Rank: 0        Logit: 36.43 Prob: 99.91% Token: | 5|[/b]\n","Top 0th token. Logit: 36.43 Prob: 99.91% Token: | 5|\n","Top 1th token. Logit: 28.80 Prob:  0.05% Token: | 4|\n","Top 2th token. Logit: 28.28 Prob:  0.03% Token: | 6|\n","Top 3th token. Logit: 26.55 Prob:  0.01% Token: | 7|\n","Top 4th token. Logit: 26.43 Prob:  0.00% Token: | 3|\n","Top 5th token. Logit: 24.44 Prob:  0.00% Token: | 1|\n","Top 6th token. Logit: 23.65 Prob:  0.00% Token: | 9|\n","Top 7th token. Logit: 23.44 Prob:  0.00% Token: | 0|\n","Top 8th token. Logit: 23.28 Prob:  0.00% Token: | 10|\n","Top 9th token. Logit: 22.61 Prob:  0.00% Token: | 8|\n","[b]Ranks of the answer tokens:[/b] [(' 5', 0)]\n"]}]},{"cell_type":"markdown","source":["This is one of the best ones yet"],"metadata":{"id":"z0gv7eT_z1Ej"}},{"cell_type":"code","source":["test_prompt(\"January February March April\", \" May\", extracted_model, model, prepend_bos=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IiGiiS5fBlwr","executionInfo":{"status":"ok","timestamp":1689432654310,"user_tz":240,"elapsed":371,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"75d62da5-5011-4dc6-df0a-b78055b6f067"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Tokenized prompt: ['<|endoftext|>', 'January', ' February', ' March', ' April']\n","Tokenized answer: [' May']\n","Performance on answer token:\n","[b]Rank: 3        Logit: 33.98 Prob:  0.70% Token: | May|[/b]\n","Top 0th token. Logit: 38.90 Prob: 95.94% Token: | April|\n","Top 1th token. Logit: 34.94 Prob:  1.82% Token: | March|\n","Top 2th token. Logit: 34.36 Prob:  1.02% Token: | February|\n","Top 3th token. Logit: 33.98 Prob:  0.70% Token: | May|\n","Top 4th token. Logit: 33.12 Prob:  0.30% Token: | June|\n","Top 5th token. Logit: 31.34 Prob:  0.05% Token: | July|\n","Top 6th token. Logit: 31.29 Prob:  0.05% Token: | September|\n","Top 7th token. Logit: 30.83 Prob:  0.03% Token: | January|\n","Top 8th token. Logit: 30.72 Prob:  0.03% Token: | October|\n","Top 9th token. Logit: 30.54 Prob:  0.02% Token: | August|\n","[b]Ranks of the answer tokens:[/b] [(' May', 3)]\n"]}]},{"cell_type":"markdown","source":["But months suffers!"],"metadata":{"id":"5WG3wMCoCJjQ"}},{"cell_type":"markdown","source":["### skip layers 3, 4, 8, 10, 11"],"metadata":{"id":"6DMDCRImz3CB"}},{"cell_type":"code","source":["import torch.nn as nn\n","\n","class ExtractedModel(nn.Module):\n","    def __init__(self, original_model_before_copy):\n","        super(ExtractedModel, self).__init__()\n","\n","        original_model = copy.deepcopy(original_model_before_copy)\n","\n","        self.embed = original_model.embed\n","        self.pos_embed = original_model.pos_embed\n","        self.blocks = original_model.blocks\n","        self.ln_final = original_model.ln_final\n","        self.unembed = original_model.unembed\n","\n","    def forward(self, tokens):\n","        # tokens [batch, position]\n","        embed = self.embed(tokens)\n","        pos_embed = self.pos_embed(tokens)\n","        residual = embed + pos_embed\n","\n","        indices_to_remove = [3, 4, 8, 10, 11]\n","        indices_to_remove.sort(reverse=True)  # Start removing from the end to avoid index shifting\n","        for index in indices_to_remove:\n","            self.blocks.pop(index)\n","\n","        for block in self.blocks:\n","            residual = block(residual)\n","\n","        normalized_resid_final = self.ln_final(residual)\n","        logits = self.unembed(normalized_resid_final)\n","        # logits have shape [batch, position, logits]\n","        return logits\n","\n","extracted_model = ExtractedModel(model)"],"metadata":{"id":"I7Vnk8f510bK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_prompt(\"1 2 3 4\", \" 5\", extracted_model, model, prepend_bos=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1689429435891,"user_tz":240,"elapsed":208,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"3ca0b170-cd18-4dc1-a09d-41a78aaa3065","id":"cttVxGFwz4Ey"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Tokenized prompt: ['<|endoftext|>', '1', ' 2', ' 3', ' 4']\n","Tokenized answer: [' 5']\n","Performance on answer token:\n","[b]Rank: 0        Logit: 31.95 Prob: 92.24% Token: | 5|[/b]\n","Top 0th token. Logit: 31.95 Prob: 92.24% Token: | 5|\n","Top 1th token. Logit: 29.11 Prob:  5.40% Token: | 4|\n","Top 2th token. Logit: 27.91 Prob:  1.62% Token: | 3|\n","Top 3th token. Logit: 26.60 Prob:  0.44% Token: | 6|\n","Top 4th token. Logit: 25.03 Prob:  0.09% Token: | 1|\n","Top 5th token. Logit: 24.83 Prob:  0.08% Token: | 7|\n","Top 6th token. Logit: 24.79 Prob:  0.07% Token: | 0|\n","Top 7th token. Logit: 24.17 Prob:  0.04% Token: | 2|\n","Top 8th token. Logit: 22.23 Prob:  0.01% Token: | 9|\n","Top 9th token. Logit: 21.58 Prob:  0.00% Token: | 8|\n","[b]Ranks of the answer tokens:[/b] [(' 5', 0)]\n"]}]},{"cell_type":"markdown","source":["L3 helps, but not required"],"metadata":{"id":"iPfTIWk813Sz"}},{"cell_type":"code","source":["test_prompt(\"January February March April\", \" May\", extracted_model, model, prepend_bos=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rqNmsC3FA3Q-","executionInfo":{"status":"ok","timestamp":1689432331140,"user_tz":240,"elapsed":346,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"9661c5bd-7e6d-48e4-df42-7f5867f6e6f6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Tokenized prompt: ['<|endoftext|>', 'January', ' February', ' March', ' April']\n","Tokenized answer: [' May']\n","Performance on answer token:\n","[b]Rank: 16       Logit: 23.40 Prob:  0.42% Token: | May|[/b]\n","Top 0th token. Logit: 27.93 Prob: 38.70% Token: | 2018|\n","Top 1th token. Logit: 27.28 Prob: 20.14% Token: | 2017|\n","Top 2th token. Logit: 26.56 Prob:  9.86% Token: | 2015|\n","Top 3th token. Logit: 26.15 Prob:  6.54% Token: | 2016|\n","Top 4th token. Logit: 26.00 Prob:  5.61% Token: | 2014|\n","Top 5th token. Logit: 25.67 Prob:  4.04% Token: | April|\n","Top 6th token. Logit: 25.25 Prob:  2.64% Token: | 29|\n","Top 7th token. Logit: 25.07 Prob:  2.22% Token: | 2013|\n","Top 8th token. Logit: 24.59 Prob:  1.37% Token: | 28|\n","Top 9th token. Logit: 23.90 Prob:  0.69% Token: | 25|\n","[b]Ranks of the answer tokens:[/b] [(' May', 16)]\n"]}]},{"cell_type":"markdown","source":["Wait, L3 is required for months!"],"metadata":{"id":"bj_eTEPnA64V"}},{"cell_type":"markdown","source":["### skip 2, 3, 4, 8, 10 11\n","\n","Keeps only: 0, 1, 5, 6, 7, 9"],"metadata":{"id":"RYUROmm3142F"}},{"cell_type":"code","source":["import torch.nn as nn\n","\n","class ExtractedModel(nn.Module):\n","    def __init__(self, original_model_before_copy):\n","        super(ExtractedModel, self).__init__()\n","\n","        original_model = copy.deepcopy(original_model_before_copy)\n","\n","        self.embed = original_model.embed\n","        self.pos_embed = original_model.pos_embed\n","        self.blocks = original_model.blocks\n","        self.ln_final = original_model.ln_final\n","        self.unembed = original_model.unembed\n","\n","    def forward(self, tokens):\n","        # tokens [batch, position]\n","        embed = self.embed(tokens)\n","        pos_embed = self.pos_embed(tokens)\n","        residual = embed + pos_embed\n","\n","        indices_to_remove = [2, 3, 4, 8, 10, 11]\n","        indices_to_remove.sort(reverse=True)  # Start removing from the end to avoid index shifting\n","        for index in indices_to_remove:\n","            self.blocks.pop(index)\n","\n","        for block in self.blocks:\n","            residual = block(residual)\n","\n","        normalized_resid_final = self.ln_final(residual)\n","        logits = self.unembed(normalized_resid_final)\n","        # logits have shape [batch, position, logits]\n","        return logits\n","\n","extracted_model = ExtractedModel(model)"],"metadata":{"id":"o5L22qHh2Cik"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_prompt(\"1 2 3 4\", \" 5\", extracted_model, model, prepend_bos=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1689429495634,"user_tz":240,"elapsed":167,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"30b9b655-f9f0-431b-df3b-ea5c32a175c3","id":"-Io1bTzv2Ciw"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Tokenized prompt: ['<|endoftext|>', '1', ' 2', ' 3', ' 4']\n","Tokenized answer: [' 5']\n","Performance on answer token:\n","[b]Rank: 1        Logit: 19.54 Prob: 22.06% Token: | 5|[/b]\n","Top 0th token. Logit: 19.56 Prob: 22.46% Token: | 3|\n","Top 1th token. Logit: 19.54 Prob: 22.06% Token: | 5|\n","Top 2th token. Logit: 19.03 Prob: 13.21% Token: | 4|\n","Top 3th token. Logit: 19.01 Prob: 12.96% Token: | 1|\n","Top 4th token. Logit: 18.52 Prob:  7.95% Token: | 2|\n","Top 5th token. Logit: 18.13 Prob:  5.39% Token: | 0|\n","Top 6th token. Logit: 17.16 Prob:  2.04% Token: | 6|\n","Top 7th token. Logit: 17.07 Prob:  1.87% Token: | 7|\n","Top 8th token. Logit: 16.61 Prob:  1.18% Token: | 9|\n","Top 9th token. Logit: 16.41 Prob:  0.97% Token: |\n","|\n","[b]Ranks of the answer tokens:[/b] [(' 5', 1)]\n"]}]},{"cell_type":"markdown","source":["2 is required"],"metadata":{"id":"SKZHKSrY2G5T"}},{"cell_type":"markdown","source":["### skip 3, 4, 6, 8, 10 11\n","\n","Keeps only: 0, 1, 2, 5, 7, 9"],"metadata":{"id":"c1d-MxCc2JfJ"}},{"cell_type":"code","source":["import torch.nn as nn\n","\n","class ExtractedModel(nn.Module):\n","    def __init__(self, original_model_before_copy):\n","        super(ExtractedModel, self).__init__()\n","\n","        original_model = copy.deepcopy(original_model_before_copy)\n","\n","        self.embed = original_model.embed\n","        self.pos_embed = original_model.pos_embed\n","        self.blocks = original_model.blocks\n","        self.ln_final = original_model.ln_final\n","        self.unembed = original_model.unembed\n","\n","    def forward(self, tokens):\n","        # tokens [batch, position]\n","        embed = self.embed(tokens)\n","        pos_embed = self.pos_embed(tokens)\n","        residual = embed + pos_embed\n","\n","        indices_to_remove = [6, 3, 4, 8, 10, 11]\n","        indices_to_remove.sort(reverse=True)  # Start removing from the end to avoid index shifting\n","        for index in indices_to_remove:\n","            self.blocks.pop(index)\n","\n","        for block in self.blocks:\n","            residual = block(residual)\n","\n","        normalized_resid_final = self.ln_final(residual)\n","        logits = self.unembed(normalized_resid_final)\n","        # logits have shape [batch, position, logits]\n","        return logits\n","\n","extracted_model = ExtractedModel(model)"],"metadata":{"id":"KEhB_ZkS2JfS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_prompt(\"1 2 3 4\", \" 5\", extracted_model, model, prepend_bos=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1689429495634,"user_tz":240,"elapsed":167,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"30b9b655-f9f0-431b-df3b-ea5c32a175c3","id":"TUYPr1Wp2JfS"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Tokenized prompt: ['<|endoftext|>', '1', ' 2', ' 3', ' 4']\n","Tokenized answer: [' 5']\n","Performance on answer token:\n","[b]Rank: 1        Logit: 19.54 Prob: 22.06% Token: | 5|[/b]\n","Top 0th token. Logit: 19.56 Prob: 22.46% Token: | 3|\n","Top 1th token. Logit: 19.54 Prob: 22.06% Token: | 5|\n","Top 2th token. Logit: 19.03 Prob: 13.21% Token: | 4|\n","Top 3th token. Logit: 19.01 Prob: 12.96% Token: | 1|\n","Top 4th token. Logit: 18.52 Prob:  7.95% Token: | 2|\n","Top 5th token. Logit: 18.13 Prob:  5.39% Token: | 0|\n","Top 6th token. Logit: 17.16 Prob:  2.04% Token: | 6|\n","Top 7th token. Logit: 17.07 Prob:  1.87% Token: | 7|\n","Top 8th token. Logit: 16.61 Prob:  1.18% Token: | 9|\n","Top 9th token. Logit: 16.41 Prob:  0.97% Token: |\n","|\n","[b]Ranks of the answer tokens:[/b] [(' 5', 1)]\n"]}]},{"cell_type":"markdown","source":["6 is required"],"metadata":{"id":"mOfpSaHV2JfT"}},{"cell_type":"code","source":["test_prompt(\"January February March April\", \" May\", extracted_model, model, prepend_bos=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zfqynCIvAlZg","executionInfo":{"status":"ok","timestamp":1689432306088,"user_tz":240,"elapsed":326,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"926ced90-a35e-40ac-859c-2f7b9562581c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Tokenized prompt: ['<|endoftext|>', 'January', ' February', ' March', ' April']\n","Tokenized answer: [' May']\n","Performance on answer token:\n","[b]Rank: 22       Logit: 20.30 Prob:  0.31% Token: | May|[/b]\n","Top 0th token. Logit: 24.86 Prob: 29.39% Token: | 2017|\n","Top 1th token. Logit: 24.40 Prob: 18.61% Token: | 2018|\n","Top 2th token. Logit: 23.50 Prob:  7.56% Token: | 29|\n","Top 3th token. Logit: 23.35 Prob:  6.45% Token: | 2014|\n","Top 4th token. Logit: 23.30 Prob:  6.14% Token: | 2015|\n","Top 5th token. Logit: 23.24 Prob:  5.82% Token: | 2016|\n","Top 6th token. Logit: 22.98 Prob:  4.49% Token: | 28|\n","Top 7th token. Logit: 22.90 Prob:  4.15% Token: | 2013|\n","Top 8th token. Logit: 22.17 Prob:  1.99% Token: | 27|\n","Top 9th token. Logit: 22.02 Prob:  1.71% Token: | 23|\n","[b]Ranks of the answer tokens:[/b] [(' May', 22)]\n"]}]},{"cell_type":"markdown","source":["### Automate this"],"metadata":{"id":"IjQc8FXW2Yrb"}},{"cell_type":"markdown","source":["Clearly we can just check what it does with logits"],"metadata":{"id":"ba6T6egJ2bMl"}},{"cell_type":"markdown","source":["### skip layer norms"],"metadata":{"id":"1uaqOKyny9Tv"}},{"cell_type":"code","source":["import torch.nn as nn\n","\n","class ExtractedModel(nn.Module):\n","    def __init__(self, original_model):\n","        super(ExtractedModel, self).__init__()\n","        self.embed = original_model.embed\n","        self.pos_embed = original_model.pos_embed\n","\n","        self.blocks = original_model.blocks\n","\n","        self.ln_final = original_model.ln_final\n","        self.unembed = original_model.unembed\n","\n","    def forward(self, tokens):\n","        # tokens [batch, position]\n","        embed = self.embed(tokens)\n","        pos_embed = self.pos_embed(tokens)\n","        residual = embed + pos_embed\n","\n","        for block in self.blocks[0:8]:\n","            residual = block(residual)\n","\n","        # normalized_resid_pre = self.ln1(residual)\n","        # attn_out = self.attn(normalized_resid_pre, normalized_resid_pre, normalized_resid_pre)\n","        # residual = residual + attn_out\n","\n","        # normalized_resid_mid = self.blocks[9].ln2(residual)\n","        mlp_out = self.blocks[9].mlp(residual)\n","        residual = residual + mlp_out\n","\n","        normalized_resid_final = self.ln_final(residual)\n","        logits = self.unembed(normalized_resid_final)\n","        # logits have shape [batch, position, logits]\n","        return logits\n","\n","extracted_model = ExtractedModel(model)"],"metadata":{"id":"el43N8EI3GHS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_prompt(\"1 2 3 4\", \" 5\", extracted_model, model, prepend_bos=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1689429789396,"user_tz":240,"elapsed":151,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"45653b17-8f19-4181-8c89-ee51a83d8b73","id":"O37cYIBE3GHb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Tokenized prompt: ['<|endoftext|>', '1', ' 2', ' 3', ' 4']\n","Tokenized answer: [' 5']\n","Performance on answer token:\n","[b]Rank: 0        Logit: 28.18 Prob: 97.27% Token: | 5|[/b]\n","Top 0th token. Logit: 28.18 Prob: 97.27% Token: | 5|\n","Top 1th token. Logit: 24.15 Prob:  1.72% Token: | Five|\n","Top 2th token. Logit: 21.29 Prob:  0.10% Token: | five|\n","Top 3th token. Logit: 20.99 Prob:  0.07% Token: | 50|\n","Top 4th token. Logit: 20.90 Prob:  0.07% Token: | +|\n","Top 5th token. Logit: 20.82 Prob:  0.06% Token: | Player|\n","Top 6th token. Logit: 20.67 Prob:  0.05% Token: | >|\n","Top 7th token. Logit: 20.65 Prob:  0.05% Token: | Copyright|\n","Top 8th token. Logit: 20.43 Prob:  0.04% Token: |.|\n","Top 9th token. Logit: 20.14 Prob:  0.03% Token: | 7|\n","[b]Ranks of the answer tokens:[/b] [(' 5', 0)]\n"]}]},{"cell_type":"markdown","source":["MLP9's ln not req either, but it helps. likely for more complex inputs. A strange thing to note is that \"Five\" and \"five\" are outputted in second place, instead of digits like 4. Still, not by much. This may be an anomaly from this input, so check again.\n","\n","This is the first time we see this result. We saw 'five' arise when deleting L8, but not to this ranking.\n","\n","Another thing is that 'next' and '>' and '<' are common predictions. In fact, in the full model, Next is predicted as 2nd place. INdications are that this has something to do with it"],"metadata":{"id":"7Ct7knPs3Oi4"}},{"cell_type":"code","source":["test_prompt(\"5 6 7 8\", \" 9\", extracted_model, model, prepend_bos=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xLGbuplq4IO7","executionInfo":{"status":"ok","timestamp":1689430042986,"user_tz":240,"elapsed":155,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"3b9bb765-1161-4e52-ce3c-371f397ac956"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Tokenized prompt: ['<|endoftext|>', '5', ' 6', ' 7', ' 8']\n","Tokenized answer: [' 9']\n","Performance on answer token:\n","[b]Rank: 0        Logit: 24.66 Prob: 56.31% Token: | 9|[/b]\n","Top 0th token. Logit: 24.66 Prob: 56.31% Token: | 9|\n","Top 1th token. Logit: 23.71 Prob: 21.69% Token: | Copyright|\n","Top 2th token. Logit: 22.11 Prob:  4.40% Token: | >|\n","Top 3th token. Logit: 21.72 Prob:  2.97% Token: | 5|\n","Top 4th token. Logit: 21.63 Prob:  2.73% Token: | 1|\n","Top 5th token. Logit: 21.21 Prob:  1.78% Token: | <|\n","Top 6th token. Logit: 20.74 Prob:  1.12% Token: | 3|\n","Top 7th token. Logit: 20.28 Prob:  0.70% Token: | 13|\n","Top 8th token. Logit: 20.07 Prob:  0.57% Token: |.|\n","Top 9th token. Logit: 19.82 Prob:  0.44% Token: | 7|\n","[b]Ranks of the answer tokens:[/b] [(' 9', 0)]\n"]}]},{"cell_type":"markdown","source":["Indeed, 'nine' is not predicted as a ranking, so we can say the 'five' is a fluke and not consistent.. It's also not as high as a logit. So the full model with the missing layers is doing something for other inputs."],"metadata":{"id":"xR1R01JD4LbU"}},{"cell_type":"markdown","source":["### use MLPs only for L0 to L9"],"metadata":{"id":"9Y1TU4Pt8zJQ"}},{"cell_type":"code","source":["import torch.nn as nn\n","\n","class ExtractedModel(nn.Module):\n","    def __init__(self, original_model):\n","        super(ExtractedModel, self).__init__()\n","        self.embed = original_model.embed\n","        self.pos_embed = original_model.pos_embed\n","        self.blocks = original_model.blocks\n","        self.ln_final = original_model.ln_final\n","        self.unembed = original_model.unembed\n","\n","    def forward(self, tokens):\n","        # tokens [batch, position]\n","        embed = self.embed(tokens)\n","        pos_embed = self.pos_embed(tokens)\n","        residual = embed + pos_embed\n","\n","        for block in self.blocks[0:8]:\n","            # normalized_resid_pre = block.ln1(residual)\n","            # attn_out = block.attn(normalized_resid_pre, normalized_resid_pre, normalized_resid_pre)\n","            # residual = residual + attn_out\n","\n","            normalized_resid_mid = block.ln2(residual)\n","            mlp_out = block.mlp(normalized_resid_mid)\n","            residual = residual + mlp_out\n","\n","        normalized_resid_mid = self.blocks[9].ln2(residual)\n","        mlp_out = self.blocks[9].mlp(normalized_resid_mid)\n","        residual = residual + mlp_out\n","\n","        normalized_resid_final = self.ln_final(residual)\n","        logits = self.unembed(normalized_resid_final)\n","        # logits have shape [batch, position, logits]\n","        return logits\n","\n","extracted_model = ExtractedModel(model)"],"metadata":{"id":"ISAuzKWp83uN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_prompt(\"1 2 3 4\", \" 5\", extracted_model, model, prepend_bos=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1689431476754,"user_tz":240,"elapsed":6833,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"7c0d3118-8912-41e7-d1cc-acf8061d867c","id":"hNoz538f83uZ"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Tokenized prompt: ['<|endoftext|>', '1', ' 2', ' 3', ' 4']\n","Tokenized answer: [' 5']\n","Performance on answer token:\n","[b]Rank: 89       Logit:  5.75 Prob:  0.10% Token: | 5|[/b]\n","Top 0th token. Logit: 10.26 Prob:  9.10% Token: |th|\n","Top 1th token. Logit:  9.61 Prob:  4.72% Token: |-|\n","Top 2th token. Logit:  9.39 Prob:  3.79% Token: |.|\n","Top 3th token. Logit:  8.90 Prob:  2.34% Token: |,|\n","Top 4th token. Logit:  7.92 Prob:  0.87% Token: | and|\n","Top 5th token. Logit:  7.74 Prob:  0.73% Token: | of|\n","Top 6th token. Logit:  7.65 Prob:  0.67% Token: |\n","|\n","Top 7th token. Logit:  7.64 Prob:  0.66% Token: |:|\n","Top 8th token. Logit:  7.64 Prob:  0.66% Token: | the|\n","Top 9th token. Logit:  7.60 Prob:  0.63% Token: |x|\n","[b]Ranks of the answer tokens:[/b] [(' 5', 89)]\n"]}]},{"cell_type":"markdown","source":["Ok so you need attention. From which layers?"],"metadata":{"id":"UQ7pco8r9pzo"}},{"cell_type":"markdown","source":["### skip MLP in L8, only use attn8. good for alphabet?"],"metadata":{"id":"pTXEWrpcEZmR"}},{"cell_type":"code","source":["import torch.nn as nn\n","\n","class ExtractedModel(nn.Module):\n","    def __init__(self, original_model):\n","        super(ExtractedModel, self).__init__()\n","        self.embed = original_model.embed\n","        self.pos_embed = original_model.pos_embed\n","\n","        self.blocks = original_model.blocks\n","\n","        self.ln_final = original_model.ln_final\n","        self.unembed = original_model.unembed\n","\n","    def forward(self, tokens):\n","        # tokens [batch, position]\n","        embed = self.embed(tokens)\n","        pos_embed = self.pos_embed(tokens)\n","        residual = embed + pos_embed\n","\n","        for block in self.blocks[0:8]:\n","            residual = block(residual)\n","\n","        normalized_resid_pre = self.blocks[8].ln1(residual)\n","        attn_out = self.blocks[8].attn(normalized_resid_pre, normalized_resid_pre, normalized_resid_pre)\n","        residual = residual + attn_out\n","\n","        normalized_resid_mid = self.blocks[9].ln2(residual)\n","        mlp_out = self.blocks[9].mlp(normalized_resid_mid)\n","        residual = residual + mlp_out\n","\n","        normalized_resid_final = self.ln_final(residual)\n","        logits = self.unembed(normalized_resid_final)\n","        # logits have shape [batch, position, logits]\n","        return logits\n","\n","extracted_model = ExtractedModel(model)"],"metadata":{"id":"erCN3p03EZma"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_prompt(\"1 2 3 4\", \" 5\", extracted_model, model, prepend_bos=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1689433366820,"user_tz":240,"elapsed":328,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"1199b7a0-d353-4a51-bb29-5fd529359924","id":"QjuF-P8KEZmb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Tokenized prompt: ['<|endoftext|>', '1', ' 2', ' 3', ' 4']\n","Tokenized answer: [' 5']\n","Performance on answer token:\n","[b]Rank: 0        Logit: 30.49 Prob: 99.69% Token: | 5|[/b]\n","Top 0th token. Logit: 30.49 Prob: 99.69% Token: | 5|\n","Top 1th token. Logit: 24.27 Prob:  0.20% Token: | 4|\n","Top 2th token. Logit: 22.57 Prob:  0.04% Token: | <!--|\n","Top 3th token. Logit: 21.64 Prob:  0.01% Token: | Next|\n","Top 4th token. Logit: 21.01 Prob:  0.01% Token: | 7|\n","Top 5th token. Logit: 20.97 Prob:  0.01% Token: | 3|\n","Top 6th token. Logit: 20.68 Prob:  0.01% Token: |★|\n","Top 7th token. Logit: 20.47 Prob:  0.00% Token: | 6|\n","Top 8th token. Logit: 20.34 Prob:  0.00% Token: |\n","|\n","Top 9th token. Logit: 20.22 Prob:  0.00% Token: | ★|\n","[b]Ranks of the answer tokens:[/b] [(' 5', 0)]\n"]}]},{"cell_type":"code","source":["test_prompt(\"January February March April\", \" May\", extracted_model, model, prepend_bos=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1689433370538,"user_tz":240,"elapsed":429,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"259f4a6b-2ed7-4c93-8258-a8d8d210efde","id":"ediOnWk3EZmc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Tokenized prompt: ['<|endoftext|>', 'January', ' February', ' March', ' April']\n","Tokenized answer: [' May']\n","Performance on answer token:\n","[b]Rank: 0        Logit: 24.90 Prob: 56.25% Token: | May|[/b]\n","Top 0th token. Logit: 24.90 Prob: 56.25% Token: | May|\n","Top 1th token. Logit: 23.23 Prob: 10.53% Token: | 2015|\n","Top 2th token. Logit: 22.90 Prob:  7.60% Token: | 5|\n","Top 3th token. Logit: 22.09 Prob:  3.36% Token: | 2005|\n","Top 4th token. Logit: 21.98 Prob:  3.02% Token: | April|\n","Top 5th token. Logit: 21.40 Prob:  1.69% Token: | 2017|\n","Top 6th token. Logit: 21.23 Prob:  1.43% Token: | Top|\n","Top 7th token. Logit: 21.01 Prob:  1.14% Token: | 25|\n","Top 8th token. Logit: 21.00 Prob:  1.13% Token: | 2018|\n","Top 9th token. Logit: 20.83 Prob:  0.96% Token: |\n","|\n","[b]Ranks of the answer tokens:[/b] [(' May', 0)]\n"]}]},{"cell_type":"code","source":["test_prompt(\"Sunday Monday Tuesday Wednesday\", \" Thursday\", extracted_model, model, prepend_bos=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1689433406666,"user_tz":240,"elapsed":343,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"82a144f5-fc68-4941-9993-0d05795fe85c","id":"FrAygLn-EZmd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Tokenized prompt: ['<|endoftext|>', 'Sunday', ' Monday', ' Tuesday', ' Wednesday']\n","Tokenized answer: [' Thursday']\n","Performance on answer token:\n","[b]Rank: 0        Logit: 28.79 Prob: 52.66% Token: | Thursday|[/b]\n","Top 0th token. Logit: 28.79 Prob: 52.66% Token: | Thursday|\n","Top 1th token. Logit: 28.60 Prob: 43.65% Token: | Wednesday|\n","Top 2th token. Logit: 24.50 Prob:  0.73% Token: | September|\n","Top 3th token. Logit: 23.76 Prob:  0.34% Token: | Thurs|\n","Top 4th token. Logit: 23.47 Prob:  0.26% Token: | Saturday|\n","Top 5th token. Logit: 23.46 Prob:  0.26% Token: | March|\n","Top 6th token. Logit: 23.35 Prob:  0.23% Token: | Friday|\n","Top 7th token. Logit: 23.14 Prob:  0.19% Token: | 29|\n","Top 8th token. Logit: 23.05 Prob:  0.17% Token: | January|\n","Top 9th token. Logit: 22.86 Prob:  0.14% Token: | December|\n","[b]Ranks of the answer tokens:[/b] [(' Thursday', 0)]\n"]}]},{"cell_type":"code","source":["test_prompt(\"A B C D\", \" E\", extracted_model, model, prepend_bos=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1689433410845,"user_tz":240,"elapsed":435,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"33edce4f-9a97-46f8-bb8a-968cba6b1364","id":"CQIkYP7lEZme"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Tokenized prompt: ['<|endoftext|>', 'A', ' B', ' C', ' D']\n","Tokenized answer: [' E']\n","Performance on answer token:\n","[b]Rank: 1        Logit: 26.56 Prob:  8.56% Token: | E|[/b]\n","Top 0th token. Logit: 28.92 Prob: 90.96% Token: | D|\n","Top 1th token. Logit: 26.56 Prob:  8.56% Token: | E|\n","Top 2th token. Logit: 23.11 Prob:  0.27% Token: | C|\n","Top 3th token. Logit: 21.61 Prob:  0.06% Token: | d|\n","Top 4th token. Logit: 21.30 Prob:  0.04% Token: | G|\n","Top 5th token. Logit: 21.00 Prob:  0.03% Token: | F|\n","Top 6th token. Logit: 19.69 Prob:  0.01% Token: | Close|\n","Top 7th token. Logit: 19.35 Prob:  0.01% Token: | T|\n","Top 8th token. Logit: 19.16 Prob:  0.01% Token: | >>|\n","Top 9th token. Logit: 18.92 Prob:  0.00% Token: |\n","|\n","[b]Ranks of the answer tokens:[/b] [(' E', 1)]\n"]}]},{"cell_type":"markdown","source":["No, attn 8 is not good for alphabet."],"metadata":{"id":"18CSvBZsEZme"}},{"cell_type":"code","source":["test_prompt(\"one two three four\", \" five\", extracted_model, model, prepend_bos=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1689433712609,"user_tz":240,"elapsed":343,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"877ba3e9-3a02-43b8-d84f-eefc0c10eadc","id":"qwdJ0Bo3GHo4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Tokenized prompt: ['<|endoftext|>', 'one', ' two', ' three', ' four']\n","Tokenized answer: [' five']\n","Performance on answer token:\n","[b]Rank: 2        Logit: 21.01 Prob:  8.45% Token: | five|[/b]\n","Top 0th token. Logit: 23.02 Prob: 63.23% Token: |teen|\n","Top 1th token. Logit: 21.21 Prob: 10.33% Token: |ths|\n","Top 2th token. Logit: 21.01 Prob:  8.45% Token: | five|\n","Top 3th token. Logit: 20.22 Prob:  3.85% Token: | 5|\n","Top 4th token. Logit: 19.89 Prob:  2.75% Token: | Five|\n","Top 5th token. Logit: 19.49 Prob:  1.86% Token: | seven|\n","Top 6th token. Logit: 19.06 Prob:  1.20% Token: | Four|\n","Top 7th token. Logit: 18.95 Prob:  1.07% Token: | four|\n","Top 8th token. Logit: 18.72 Prob:  0.85% Token: |teenth|\n","Top 9th token. Logit: 18.64 Prob:  0.79% Token: | fifth|\n","[b]Ranks of the answer tokens:[/b] [(' five', 2)]\n"]}]},{"cell_type":"markdown","source":["attn 8 is not enough for number words"],"metadata":{"id":"3b6G4jz-GLgo"}},{"cell_type":"markdown","source":["### skip attn in L8, only use MLP8. good for alphabet?"],"metadata":{"id":"wHV6r-JTFPRh"}},{"cell_type":"code","source":["import torch.nn as nn\n","\n","class ExtractedModel(nn.Module):\n","    def __init__(self, original_model):\n","        super(ExtractedModel, self).__init__()\n","        self.embed = original_model.embed\n","        self.pos_embed = original_model.pos_embed\n","\n","        self.blocks = original_model.blocks\n","\n","        self.ln_final = original_model.ln_final\n","        self.unembed = original_model.unembed\n","\n","    def forward(self, tokens):\n","        # tokens [batch, position]\n","        embed = self.embed(tokens)\n","        pos_embed = self.pos_embed(tokens)\n","        residual = embed + pos_embed\n","\n","        for block in self.blocks[0:8]:\n","            residual = block(residual)\n","\n","        # normalized_resid_pre = self.blocks[8].ln1(residual)\n","        # attn_out = self.blocks[8].attn(normalized_resid_pre, normalized_resid_pre, normalized_resid_pre)\n","        # residual = residual + attn_out\n","\n","        normalized_resid_mid = self.blocks[8].ln2(residual)\n","        mlp_out = self.blocks[8].mlp(normalized_resid_mid)\n","        residual = residual + mlp_out\n","\n","        normalized_resid_mid = self.blocks[9].ln2(residual)\n","        mlp_out = self.blocks[9].mlp(normalized_resid_mid)\n","        residual = residual + mlp_out\n","\n","        normalized_resid_final = self.ln_final(residual)\n","        logits = self.unembed(normalized_resid_final)\n","        # logits have shape [batch, position, logits]\n","        return logits\n","\n","extracted_model = ExtractedModel(model)"],"metadata":{"id":"TXPU-MulFPRq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_prompt(\"1 2 3 4\", \" 5\", extracted_model, model, prepend_bos=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1689433533212,"user_tz":240,"elapsed":337,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"9eb52b56-ffcf-401d-d688-6f8fd199dd61","id":"JIvdBLyeFPRq"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Tokenized prompt: ['<|endoftext|>', '1', ' 2', ' 3', ' 4']\n","Tokenized answer: [' 5']\n","Performance on answer token:\n","[b]Rank: 0        Logit: 31.24 Prob: 99.39% Token: | 5|[/b]\n","Top 0th token. Logit: 31.24 Prob: 99.39% Token: | 5|\n","Top 1th token. Logit: 25.76 Prob:  0.41% Token: | 4|\n","Top 2th token. Logit: 23.75 Prob:  0.06% Token: | 7|\n","Top 3th token. Logit: 23.62 Prob:  0.05% Token: | 3|\n","Top 4th token. Logit: 23.47 Prob:  0.04% Token: | 9|\n","Top 5th token. Logit: 22.09 Prob:  0.01% Token: | 6|\n","Top 6th token. Logit: 22.07 Prob:  0.01% Token: | Next|\n","Top 7th token. Logit: 21.91 Prob:  0.01% Token: |★|\n","Top 8th token. Logit: 20.88 Prob:  0.00% Token: | 49|\n","Top 9th token. Logit: 20.69 Prob:  0.00% Token: | 15|\n","[b]Ranks of the answer tokens:[/b] [(' 5', 0)]\n"]}]},{"cell_type":"code","source":["test_prompt(\"January February March April\", \" May\", extracted_model, model, prepend_bos=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1689433536324,"user_tz":240,"elapsed":339,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"1ed25f9c-978f-4641-8e3f-d45dd09519d8","id":"wBrYS-p8FPRr"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Tokenized prompt: ['<|endoftext|>', 'January', ' February', ' March', ' April']\n","Tokenized answer: [' May']\n","Performance on answer token:\n","[b]Rank: 1        Logit: 23.83 Prob: 20.82% Token: | May|[/b]\n","Top 0th token. Logit: 24.37 Prob: 35.68% Token: | 2015|\n","Top 1th token. Logit: 23.83 Prob: 20.82% Token: | May|\n","Top 2th token. Logit: 22.39 Prob:  4.90% Token: | 2017|\n","Top 3th token. Logit: 22.13 Prob:  3.80% Token: | 2005|\n","Top 4th token. Logit: 21.92 Prob:  3.07% Token: | 5|\n","Top 5th token. Logit: 21.88 Prob:  2.96% Token: | Rate|\n","Top 6th token. Logit: 21.73 Prob:  2.56% Token: | Category|\n","Top 7th token. Logit: 21.48 Prob:  1.99% Token: | 1997|\n","Top 8th token. Logit: 21.33 Prob:  1.71% Token: | 57|\n","Top 9th token. Logit: 21.04 Prob:  1.28% Token: |\n","|\n","[b]Ranks of the answer tokens:[/b] [(' May', 1)]\n"]}]},{"cell_type":"code","source":["test_prompt(\"Sunday Monday Tuesday Wednesday\", \" Thursday\", extracted_model, model, prepend_bos=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1689433545670,"user_tz":240,"elapsed":333,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"47519a94-7866-48bd-8282-a443729a931b","id":"vgQFUbVMFPRr"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Tokenized prompt: ['<|endoftext|>', 'Sunday', ' Monday', ' Tuesday', ' Wednesday']\n","Tokenized answer: [' Thursday']\n","Performance on answer token:\n","[b]Rank: 0        Logit: 25.45 Prob: 22.95% Token: | Thursday|[/b]\n","Top 0th token. Logit: 25.45 Prob: 22.95% Token: | Thursday|\n","Top 1th token. Logit: 24.81 Prob: 12.10% Token: | Wednesday|\n","Top 2th token. Logit: 24.71 Prob: 11.01% Token: | Evening|\n","Top 3th token. Logit: 24.61 Prob:  9.93% Token: | September|\n","Top 4th token. Logit: 24.31 Prob:  7.36% Token: | night|\n","Top 5th token. Logit: 24.13 Prob:  6.13% Token: | July|\n","Top 6th token. Logit: 23.59 Prob:  3.58% Token: | nights|\n","Top 7th token. Logit: 23.59 Prob:  3.57% Token: | January|\n","Top 8th token. Logit: 23.55 Prob:  3.43% Token: | December|\n","Top 9th token. Logit: 23.43 Prob:  3.06% Token: | Year|\n","[b]Ranks of the answer tokens:[/b] [(' Thursday', 0)]\n"]}]},{"cell_type":"code","source":["test_prompt(\"A B C D\", \" E\", extracted_model, model, prepend_bos=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1689433548821,"user_tz":240,"elapsed":332,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"9439d21f-2b7a-44a1-989c-c23941fcb4df","id":"zuJ8ZAi9FPRs"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Tokenized prompt: ['<|endoftext|>', 'A', ' B', ' C', ' D']\n","Tokenized answer: [' E']\n","Performance on answer token:\n","[b]Rank: 0        Logit: 25.53 Prob: 67.87% Token: | E|[/b]\n","Top 0th token. Logit: 25.53 Prob: 67.87% Token: | E|\n","Top 1th token. Logit: 24.63 Prob: 27.67% Token: | D|\n","Top 2th token. Logit: 21.98 Prob:  1.95% Token: | G|\n","Top 3th token. Logit: 20.77 Prob:  0.58% Token: | F|\n","Top 4th token. Logit: 20.01 Prob:  0.27% Token: | C|\n","Top 5th token. Logit: 19.44 Prob:  0.15% Token: | Q|\n","Top 6th token. Logit: 19.24 Prob:  0.13% Token: | H|\n","Top 7th token. Logit: 19.21 Prob:  0.12% Token: |\n","|\n","Top 8th token. Logit: 19.11 Prob:  0.11% Token: | d|\n","Top 9th token. Logit: 19.08 Prob:  0.11% Token: |+|\n","[b]Ranks of the answer tokens:[/b] [(' E', 0)]\n"]}]},{"cell_type":"markdown","source":["Yes, MLP 8 is CRUCIAL for alphabet. Not so much attn 8. But does this mean alphabet is a diff pattern than digits?"],"metadata":{"id":"s4Q1K4frFPRs"}},{"cell_type":"code","source":["test_prompt(\"one two three four\", \" five\", extracted_model, model, prepend_bos=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gzZ-_jycFsKY","executionInfo":{"status":"ok","timestamp":1689433604600,"user_tz":240,"elapsed":519,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"6d9270c4-d9c8-41c6-c031-f1cb7e6ff428"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Tokenized prompt: ['<|endoftext|>', 'one', ' two', ' three', ' four']\n","Tokenized answer: [' five']\n","Performance on answer token:\n","[b]Rank: 10       Logit: 18.42 Prob:  0.02% Token: | five|[/b]\n","Top 0th token. Logit: 26.76 Prob: 98.94% Token: |teen|\n","Top 1th token. Logit: 20.51 Prob:  0.19% Token: |ths|\n","Top 2th token. Logit: 20.32 Prob:  0.16% Token: | 5|\n","Top 3th token. Logit: 19.95 Prob:  0.11% Token: |\n","|\n","Top 4th token. Logit: 19.60 Prob:  0.08% Token: | 3|\n","Top 5th token. Logit: 19.49 Prob:  0.07% Token: |teenth|\n","Top 6th token. Logit: 19.46 Prob:  0.07% Token: | ...|\n","Top 7th token. Logit: 19.01 Prob:  0.04% Token: | 13|\n","Top 8th token. Logit: 18.82 Prob:  0.04% Token: | 7|\n","Top 9th token. Logit: 18.57 Prob:  0.03% Token: | 4|\n","[b]Ranks of the answer tokens:[/b] [(' five', 10)]\n"]}]},{"cell_type":"markdown","source":["Skipping attn 8 is horrible for number words"],"metadata":{"id":"iVO_P03LFz3i"}},{"cell_type":"markdown","source":["### skip all attn until attn 9"],"metadata":{"id":"mkHsL5ho-ZLn"}},{"cell_type":"code","source":["import torch.nn as nn\n","\n","class ExtractedModel(nn.Module):\n","    def __init__(self, original_model):\n","        super(ExtractedModel, self).__init__()\n","        self.embed = original_model.embed\n","        self.pos_embed = original_model.pos_embed\n","\n","        self.blocks = original_model.blocks\n","\n","        self.ln_final = original_model.ln_final\n","        self.unembed = original_model.unembed\n","\n","    def forward(self, tokens):\n","        # tokens [batch, position]\n","        embed = self.embed(tokens)\n","        pos_embed = self.pos_embed(tokens)\n","        residual = embed + pos_embed\n","\n","        for block in self.blocks[0:9]:\n","            # residual = block(residual)\n","\n","            # normalized_resid_pre = self.blocks[8].ln1(residual)\n","            # attn_out = self.blocks[8].attn(normalized_resid_pre, normalized_resid_pre, normalized_resid_pre)\n","            # residual = residual + attn_out\n","\n","            normalized_resid_mid = block.ln2(residual)\n","            mlp_out = block.mlp(normalized_resid_mid)\n","            residual = residual + mlp_out\n","\n","        residual = self.blocks[9](residual)\n","\n","        normalized_resid_final = self.ln_final(residual)\n","        logits = self.unembed(normalized_resid_final)\n","        # logits have shape [batch, position, logits]\n","        return logits\n","\n","extracted_model = ExtractedModel(model)"],"metadata":{"id":"edkVUALP-ZLo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_prompt(\"1 2 3 4\", \" 5\", extracted_model, model, prepend_bos=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1689466144824,"user_tz":240,"elapsed":162,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"7dff0dca-b934-413c-939f-3dd330f842c5","id":"l8LvxeK9-ZLo"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Tokenized prompt: ['<|endoftext|>', '1', ' 2', ' 3', ' 4']\n","Tokenized answer: [' 5']\n","Performance on answer token:\n","[b]Rank: 73       Logit:  5.94 Prob:  0.11% Token: | 5|[/b]\n","Top 0th token. Logit: 10.46 Prob: 10.39% Token: |th|\n","Top 1th token. Logit:  9.60 Prob:  4.40% Token: |-|\n","Top 2th token. Logit:  9.60 Prob:  4.40% Token: |.|\n","Top 3th token. Logit:  9.20 Prob:  2.96% Token: |,|\n","Top 4th token. Logit:  8.13 Prob:  1.01% Token: | and|\n","Top 5th token. Logit:  7.81 Prob:  0.74% Token: | of|\n","Top 6th token. Logit:  7.76 Prob:  0.70% Token: |:|\n","Top 7th token. Logit:  7.76 Prob:  0.70% Token: | the|\n","Top 8th token. Logit:  7.72 Prob:  0.67% Token: |x|\n","Top 9th token. Logit:  7.69 Prob:  0.65% Token: |\n","|\n","[b]Ranks of the answer tokens:[/b] [(' 5', 73)]\n"]}]},{"cell_type":"code","source":["test_prompt(\"January February March April\", \" May\", extracted_model, model, prepend_bos=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1689466148175,"user_tz":240,"elapsed":178,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"d6393560-bf02-46e6-8bde-84b9409e0b20","id":"2z6JvA_e-ZLp"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Tokenized prompt: ['<|endoftext|>', 'January', ' February', ' March', ' April']\n","Tokenized answer: [' May']\n","Performance on answer token:\n","[b]Rank: 32       Logit:  6.92 Prob:  0.34% Token: | May|[/b]\n","Top 0th token. Logit:  9.10 Prob:  3.07% Token: |,|\n","Top 1th token. Logit:  8.91 Prob:  2.52% Token: | the|\n","Top 2th token. Logit:  8.19 Prob:  1.23% Token: |.|\n","Top 3th token. Logit:  8.15 Prob:  1.19% Token: | and|\n","Top 4th token. Logit:  8.05 Prob:  1.07% Token: |-|\n","Top 5th token. Logit:  8.01 Prob:  1.03% Token: | of|\n","Top 6th token. Logit:  7.85 Prob:  0.88% Token: | a|\n","Top 7th token. Logit:  7.63 Prob:  0.70% Token: |\n","|\n","Top 8th token. Logit:  7.58 Prob:  0.67% Token: | in|\n","Top 9th token. Logit:  7.56 Prob:  0.65% Token: | \"|\n","[b]Ranks of the answer tokens:[/b] [(' May', 32)]\n"]}]},{"cell_type":"code","source":["test_prompt(\"Sunday Monday Tuesday Wednesday\", \" Thursday\", extracted_model, model, prepend_bos=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1689466150495,"user_tz":240,"elapsed":258,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"e37fdcff-b4a5-41c7-8bd2-89f7c1edbba0","id":"N5KxCF5j-ZLp"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Tokenized prompt: ['<|endoftext|>', 'Sunday', ' Monday', ' Tuesday', ' Wednesday']\n","Tokenized answer: [' Thursday']\n","Performance on answer token:\n","[b]Rank: 13       Logit: 15.71 Prob:  0.00% Token: | Thursday|[/b]\n","Top 0th token. Logit: 27.37 Prob: 56.10% Token: | morning|\n","Top 1th token. Logit: 26.53 Prob: 24.26% Token: | night|\n","Top 2th token. Logit: 25.95 Prob: 13.59% Token: | afternoon|\n","Top 3th token. Logit: 25.12 Prob:  5.90% Token: | evening|\n","Top 4th token. Logit: 20.82 Prob:  0.08% Token: | nights|\n","Top 5th token. Logit: 19.79 Prob:  0.03% Token: | mornings|\n","Top 6th token. Logit: 19.00 Prob:  0.01% Token: | Night|\n","Top 7th token. Logit: 18.35 Prob:  0.01% Token: | Tuesday|\n","Top 8th token. Logit: 17.83 Prob:  0.00% Token: | evenings|\n","Top 9th token. Logit: 17.65 Prob:  0.00% Token: | Wednesday|\n","[b]Ranks of the answer tokens:[/b] [(' Thursday', 13)]\n"]}]},{"cell_type":"code","source":["test_prompt(\"A B C D\", \" E\", extracted_model, model, prepend_bos=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1689466154186,"user_tz":240,"elapsed":158,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"f5970a99-dea3-488f-a40c-53387bd8ed30","id":"ErWyQ8t3-ZLp"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Tokenized prompt: ['<|endoftext|>', 'A', ' B', ' C', ' D']\n","Tokenized answer: [' E']\n","Performance on answer token:\n","[b]Rank: 192      Logit:  6.32 Prob:  0.09% Token: | E|[/b]\n","Top 0th token. Logit:  9.22 Prob:  1.61% Token: |-|\n","Top 1th token. Logit:  9.02 Prob:  1.32% Token: |.|\n","Top 2th token. Logit:  8.89 Prob:  1.16% Token: |orm|\n","Top 3th token. Logit:  8.87 Prob:  1.14% Token: |ere|\n","Top 4th token. Logit:  8.64 Prob:  0.91% Token: |AG|\n","Top 5th token. Logit:  8.45 Prob:  0.75% Token: |Y|\n","Top 6th token. Logit:  8.31 Prob:  0.65% Token: |ried|\n","Top 7th token. Logit:  8.24 Prob:  0.61% Token: |etermined|\n","Top 8th token. Logit:  8.13 Prob:  0.54% Token: |aim|\n","Top 9th token. Logit:  8.03 Prob:  0.49% Token: |ella|\n","[b]Ranks of the answer tokens:[/b] [(' E', 192)]\n"]}]},{"cell_type":"code","source":["test_prompt(\"one two three four\", \" five\", extracted_model, model, prepend_bos=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1689466157668,"user_tz":240,"elapsed":180,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"af08c016-aad9-4ea9-b71f-8001d944e809","id":"ofImL3YB-ZLq"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Tokenized prompt: ['<|endoftext|>', 'one', ' two', ' three', ' four']\n","Tokenized answer: [' five']\n","Performance on answer token:\n","[b]Rank: 147      Logit:  4.56 Prob:  0.06% Token: | five|[/b]\n","Top 0th token. Logit:  8.16 Prob:  2.09% Token: |-|\n","Top 1th token. Logit:  8.04 Prob:  1.85% Token: |,|\n","Top 2th token. Logit:  8.03 Prob:  1.83% Token: | of|\n","Top 3th token. Logit:  7.71 Prob:  1.33% Token: | and|\n","Top 4th token. Logit:  7.67 Prob:  1.27% Token: | the|\n","Top 5th token. Logit:  7.58 Prob:  1.17% Token: |.|\n","Top 6th token. Logit:  7.25 Prob:  0.84% Token: | in|\n","Top 7th token. Logit:  7.17 Prob:  0.78% Token: | to|\n","Top 8th token. Logit:  7.14 Prob:  0.76% Token: | a|\n","Top 9th token. Logit:  6.84 Prob:  0.56% Token: |\n","|\n","[b]Ranks of the answer tokens:[/b] [(' five', 147)]\n"]}]},{"cell_type":"markdown","source":["Unfortunately, this doesn't work. Clearly we can't just slice and dice models this simply; we must ablate heads. Though, sometimes we can skip some things for very specific tasks."],"metadata":{"id":"gfExkQiy-ZLq"}},{"cell_type":"markdown","source":["### skip all attn until attn 9"],"metadata":{"id":"07foBJFKS_TM"}},{"cell_type":"code","source":["import torch.nn as nn\n","\n","class ExtractedModel(nn.Module):\n","    def __init__(self, original_model):\n","        super(ExtractedModel, self).__init__()\n","        self.embed = original_model.embed\n","        self.pos_embed = original_model.pos_embed\n","\n","        self.blocks = original_model.blocks\n","\n","        self.ln_final = original_model.ln_final\n","        self.unembed = original_model.unembed\n","\n","    def forward(self, tokens):\n","        # tokens [batch, position]\n","        embed = self.embed(tokens)\n","        pos_embed = self.pos_embed(tokens)\n","        residual = embed + pos_embed\n","\n","        for block in self.blocks[0:9]:\n","            # residual = block(residual)\n","\n","            # normalized_resid_pre = self.blocks[8].ln1(residual)\n","            # attn_out = self.blocks[8].attn(normalized_resid_pre, normalized_resid_pre, normalized_resid_pre)\n","            # residual = residual + attn_out\n","\n","            normalized_resid_mid = block.ln2(residual)\n","            mlp_out = block.mlp(normalized_resid_mid)\n","            residual = residual + mlp_out\n","\n","        residual = self.blocks[9](residual)\n","\n","        normalized_resid_final = self.ln_final(residual)\n","        logits = self.unembed(normalized_resid_final)\n","        # logits have shape [batch, position, logits]\n","        return logits\n","\n","extracted_model = ExtractedModel(model)"],"metadata":{"id":"E9icis13S_TM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_prompt(\"1 2 3 4\", \" 5\", extracted_model, model, prepend_bos=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1689466144824,"user_tz":240,"elapsed":162,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"7dff0dca-b934-413c-939f-3dd330f842c5","id":"WDUSKu7IS_TN"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Tokenized prompt: ['<|endoftext|>', '1', ' 2', ' 3', ' 4']\n","Tokenized answer: [' 5']\n","Performance on answer token:\n","[b]Rank: 73       Logit:  5.94 Prob:  0.11% Token: | 5|[/b]\n","Top 0th token. Logit: 10.46 Prob: 10.39% Token: |th|\n","Top 1th token. Logit:  9.60 Prob:  4.40% Token: |-|\n","Top 2th token. Logit:  9.60 Prob:  4.40% Token: |.|\n","Top 3th token. Logit:  9.20 Prob:  2.96% Token: |,|\n","Top 4th token. Logit:  8.13 Prob:  1.01% Token: | and|\n","Top 5th token. Logit:  7.81 Prob:  0.74% Token: | of|\n","Top 6th token. Logit:  7.76 Prob:  0.70% Token: |:|\n","Top 7th token. Logit:  7.76 Prob:  0.70% Token: | the|\n","Top 8th token. Logit:  7.72 Prob:  0.67% Token: |x|\n","Top 9th token. Logit:  7.69 Prob:  0.65% Token: |\n","|\n","[b]Ranks of the answer tokens:[/b] [(' 5', 73)]\n"]}]},{"cell_type":"code","source":["test_prompt(\"January February March April\", \" May\", extracted_model, model, prepend_bos=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1689466148175,"user_tz":240,"elapsed":178,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"d6393560-bf02-46e6-8bde-84b9409e0b20","id":"dOeyOF0DS_TN"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Tokenized prompt: ['<|endoftext|>', 'January', ' February', ' March', ' April']\n","Tokenized answer: [' May']\n","Performance on answer token:\n","[b]Rank: 32       Logit:  6.92 Prob:  0.34% Token: | May|[/b]\n","Top 0th token. Logit:  9.10 Prob:  3.07% Token: |,|\n","Top 1th token. Logit:  8.91 Prob:  2.52% Token: | the|\n","Top 2th token. Logit:  8.19 Prob:  1.23% Token: |.|\n","Top 3th token. Logit:  8.15 Prob:  1.19% Token: | and|\n","Top 4th token. Logit:  8.05 Prob:  1.07% Token: |-|\n","Top 5th token. Logit:  8.01 Prob:  1.03% Token: | of|\n","Top 6th token. Logit:  7.85 Prob:  0.88% Token: | a|\n","Top 7th token. Logit:  7.63 Prob:  0.70% Token: |\n","|\n","Top 8th token. Logit:  7.58 Prob:  0.67% Token: | in|\n","Top 9th token. Logit:  7.56 Prob:  0.65% Token: | \"|\n","[b]Ranks of the answer tokens:[/b] [(' May', 32)]\n"]}]},{"cell_type":"code","source":["test_prompt(\"Sunday Monday Tuesday Wednesday\", \" Thursday\", extracted_model, model, prepend_bos=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1689466150495,"user_tz":240,"elapsed":258,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"e37fdcff-b4a5-41c7-8bd2-89f7c1edbba0","id":"c7X2nVeoS_TN"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Tokenized prompt: ['<|endoftext|>', 'Sunday', ' Monday', ' Tuesday', ' Wednesday']\n","Tokenized answer: [' Thursday']\n","Performance on answer token:\n","[b]Rank: 13       Logit: 15.71 Prob:  0.00% Token: | Thursday|[/b]\n","Top 0th token. Logit: 27.37 Prob: 56.10% Token: | morning|\n","Top 1th token. Logit: 26.53 Prob: 24.26% Token: | night|\n","Top 2th token. Logit: 25.95 Prob: 13.59% Token: | afternoon|\n","Top 3th token. Logit: 25.12 Prob:  5.90% Token: | evening|\n","Top 4th token. Logit: 20.82 Prob:  0.08% Token: | nights|\n","Top 5th token. Logit: 19.79 Prob:  0.03% Token: | mornings|\n","Top 6th token. Logit: 19.00 Prob:  0.01% Token: | Night|\n","Top 7th token. Logit: 18.35 Prob:  0.01% Token: | Tuesday|\n","Top 8th token. Logit: 17.83 Prob:  0.00% Token: | evenings|\n","Top 9th token. Logit: 17.65 Prob:  0.00% Token: | Wednesday|\n","[b]Ranks of the answer tokens:[/b] [(' Thursday', 13)]\n"]}]},{"cell_type":"code","source":["test_prompt(\"A B C D\", \" E\", extracted_model, model, prepend_bos=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1689466154186,"user_tz":240,"elapsed":158,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"f5970a99-dea3-488f-a40c-53387bd8ed30","id":"LYUC6x_pS_TN"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Tokenized prompt: ['<|endoftext|>', 'A', ' B', ' C', ' D']\n","Tokenized answer: [' E']\n","Performance on answer token:\n","[b]Rank: 192      Logit:  6.32 Prob:  0.09% Token: | E|[/b]\n","Top 0th token. Logit:  9.22 Prob:  1.61% Token: |-|\n","Top 1th token. Logit:  9.02 Prob:  1.32% Token: |.|\n","Top 2th token. Logit:  8.89 Prob:  1.16% Token: |orm|\n","Top 3th token. Logit:  8.87 Prob:  1.14% Token: |ere|\n","Top 4th token. Logit:  8.64 Prob:  0.91% Token: |AG|\n","Top 5th token. Logit:  8.45 Prob:  0.75% Token: |Y|\n","Top 6th token. Logit:  8.31 Prob:  0.65% Token: |ried|\n","Top 7th token. Logit:  8.24 Prob:  0.61% Token: |etermined|\n","Top 8th token. Logit:  8.13 Prob:  0.54% Token: |aim|\n","Top 9th token. Logit:  8.03 Prob:  0.49% Token: |ella|\n","[b]Ranks of the answer tokens:[/b] [(' E', 192)]\n"]}]},{"cell_type":"code","source":["test_prompt(\"one two three four\", \" five\", extracted_model, model, prepend_bos=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1689466157668,"user_tz":240,"elapsed":180,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"af08c016-aad9-4ea9-b71f-8001d944e809","id":"2lUBJhilS_TO"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Tokenized prompt: ['<|endoftext|>', 'one', ' two', ' three', ' four']\n","Tokenized answer: [' five']\n","Performance on answer token:\n","[b]Rank: 147      Logit:  4.56 Prob:  0.06% Token: | five|[/b]\n","Top 0th token. Logit:  8.16 Prob:  2.09% Token: |-|\n","Top 1th token. Logit:  8.04 Prob:  1.85% Token: |,|\n","Top 2th token. Logit:  8.03 Prob:  1.83% Token: | of|\n","Top 3th token. Logit:  7.71 Prob:  1.33% Token: | and|\n","Top 4th token. Logit:  7.67 Prob:  1.27% Token: | the|\n","Top 5th token. Logit:  7.58 Prob:  1.17% Token: |.|\n","Top 6th token. Logit:  7.25 Prob:  0.84% Token: | in|\n","Top 7th token. Logit:  7.17 Prob:  0.78% Token: | to|\n","Top 8th token. Logit:  7.14 Prob:  0.76% Token: | a|\n","Top 9th token. Logit:  6.84 Prob:  0.56% Token: |\n","|\n","[b]Ranks of the answer tokens:[/b] [(' five', 147)]\n"]}]},{"cell_type":"markdown","source":["Unfortunately, this doesn't work. Clearly we can't just slice and dice models this simply; we must ablate heads. Though, sometimes we can skip some things for very specific tasks."],"metadata":{"id":"PimRdZk0S_TO"}},{"cell_type":"markdown","source":["# What happens if we skip L10 in \"One is 1\"?"],"metadata":{"id":"L3CgRtVYosQy"}},{"cell_type":"code","source":["import torch.nn as nn\n","\n","class ExtractedModel_toL9(nn.Module):\n","    def __init__(self, original_model):\n","        super(ExtractedModel_toL9, self).__init__()\n","        self.embed = original_model.embed\n","        self.pos_embed = original_model.pos_embed\n","        self.blocks = original_model.blocks\n","        self.ln_final = original_model.ln_final\n","        self.unembed = original_model.unembed\n","\n","    def forward(self, tokens):\n","        # tokens [batch, position]\n","        embed = self.embed(tokens)\n","        pos_embed = self.pos_embed(tokens)\n","        residual = embed + pos_embed\n","\n","        for block in self.blocks[0:10]:\n","            residual = block(residual)\n","\n","        normalized_resid_final = self.ln_final(residual)\n","        logits = self.unembed(normalized_resid_final)\n","        # logits have shape [batch, position, logits]\n","        return logits\n","\n","extracted_model_toL9 = ExtractedModel_toL9(model)"],"metadata":{"id":"wQdzHvalrAno"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_prompt(\"One is 1. Two is 2. Three is 3. Four is 4. Five is\", \" 5\", extracted_model_toL9, model, prepend_bos=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1689429545108,"user_tz":240,"elapsed":188,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"1e2954f7-1f29-4dfa-aa3f-448fd43f05ec","id":"gUWUaFI4rAnp"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Tokenized prompt: ['<|endoftext|>', 'One', ' is', ' 1', '.', ' Two', ' is', ' 2', '.', ' Three', ' is', ' 3', '.', ' Four', ' is', ' 4', '.', ' Five', ' is']\n","Tokenized answer: [' 5']\n","Performance on answer token:\n","[b]Rank: 548      Logit:  7.85 Prob:  0.00% Token: | 5|[/b]\n","Top 0th token. Logit: 16.69 Prob: 32.19% Token: | not|\n","Top 1th token. Logit: 15.12 Prob:  6.68% Token: | shown|\n","Top 2th token. Logit: 15.11 Prob:  6.63% Token: | also|\n","Top 3th token. Logit: 14.89 Prob:  5.30% Token: | a|\n","Top 4th token. Logit: 14.83 Prob:  4.99% Token: | definitely|\n","Top 5th token. Logit: 14.22 Prob:  2.72% Token: | still|\n","Top 6th token. Logit: 14.18 Prob:  2.61% Token: | probably|\n","Top 7th token. Logit: 13.85 Prob:  1.88% Token: | considered|\n","Top 8th token. Logit: 13.74 Prob:  1.68% Token: |ometric|\n","Top 9th token. Logit: 13.51 Prob:  1.33% Token: |nt|\n","[b]Ranks of the answer tokens:[/b] [(' 5', 548)]\n"]}]},{"cell_type":"markdown","source":["Even THIS works. This means 10.7 was useless. Perhaps it was backup."],"metadata":{"id":"VwFDkVEZrbrI"}},{"cell_type":"markdown","source":["Sanity check to see if bugs in extracted model for this input:"],"metadata":{"id":"cl_6mmBqrlxS"}},{"cell_type":"code","source":["import torch.nn as nn\n","\n","class ExtractedModel_toL9(nn.Module):\n","    def __init__(self, original_model):\n","        super(ExtractedModel_toL9, self).__init__()\n","        self.embed = original_model.embed\n","        self.pos_embed = original_model.pos_embed\n","        self.blocks = original_model.blocks\n","        self.ln_final = original_model.ln_final\n","        self.unembed = original_model.unembed\n","\n","    def forward(self, tokens):\n","        # tokens [batch, position]\n","        embed = self.embed(tokens)\n","        pos_embed = self.pos_embed(tokens)\n","        residual = embed + pos_embed\n","\n","        for block in self.blocks[0:5]:\n","            residual = block(residual)\n","\n","        normalized_resid_final = self.ln_final(residual)\n","        logits = self.unembed(normalized_resid_final)\n","        # logits have shape [batch, position, logits]\n","        return logits\n","\n","extracted_model_toL9 = ExtractedModel_toL9(model)"],"metadata":{"id":"l3wKhS_SrqXj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_prompt(\"One is 1. Two is 2. Three is 3. Four is 4. Five is\", \" 5\", extracted_model_toL9, model, prepend_bos=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1689426772773,"user_tz":240,"elapsed":149,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"ce55c416-c118-4d14-e7bc-6424bbc3087b","id":"Oz6rv5E3rqXj"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Tokenized prompt: ['<|endoftext|>', 'One', ' is', ' 1', '.', ' Two', ' is', ' 2', '.', ' Three', ' is', ' 3', '.', ' Four', ' is', ' 4', '.', ' Five', ' is']\n","Tokenized answer: [' 5']\n","Performance on answer token:\n","[b]Rank: 681      Logit:  7.50 Prob:  0.01% Token: | 5|[/b]\n","Top 0th token. Logit: 16.12 Prob: 31.64% Token: | not|\n","Top 1th token. Logit: 14.65 Prob:  7.21% Token: | a|\n","Top 2th token. Logit: 14.51 Prob:  6.30% Token: | shown|\n","Top 3th token. Logit: 14.42 Prob:  5.75% Token: | also|\n","Top 4th token. Logit: 14.04 Prob:  3.95% Token: | still|\n","Top 5th token. Logit: 13.46 Prob:  2.20% Token: | an|\n","Top 6th token. Logit: 13.04 Prob:  1.45% Token: | definitely|\n","Top 7th token. Logit: 13.02 Prob:  1.42% Token: | probably|\n","Top 8th token. Logit: 12.68 Prob:  1.01% Token: | the|\n","Top 9th token. Logit: 12.42 Prob:  0.78% Token: | considered|\n","[b]Ranks of the answer tokens:[/b] [(' 5', 681)]\n"]}]},{"cell_type":"markdown","source":["Ok, so it's working well as when we change just one thing (:9 to :5) it breaks. So only L0 to L7, then L9, are important."],"metadata":{"id":"aae8cqclrsyh"}}]}