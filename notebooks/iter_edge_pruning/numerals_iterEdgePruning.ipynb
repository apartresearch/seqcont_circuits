{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "IBxzrUmCgsfw",
        "GeskwYZbOY_z",
        "c54cqFf2lT2F",
        "ccROLT75vT7y",
        "qStvFtFrBHTE",
        "y1DQqZXrEi4o",
        "u1VIlinotlLf"
      ],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b13177b7"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wlg100/numseqcont_circuit_expms/blob/main/nb_templates/circuit_expms_template.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" align=\"left\"/></a>&nbsp;or in a local notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This will save files to your local machine if `save_files` is set to True."
      ],
      "metadata": {
        "id": "6iIlUWijq7eJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Change Inputs Here"
      ],
      "metadata": {
        "id": "vKYgaZ9JjihZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "task = \"numerals\"  # choose: numerals, numwords, months\n",
        "prompt_types = ['done', 'lost', 'names']\n",
        "num_samps_per_ptype = 512 #768 512\n",
        "\n",
        "model_name = \"gpt2-small\"\n",
        "\n",
        "save_files = True\n",
        "run_on_other_tasks = True"
      ],
      "metadata": {
        "id": "KSKP_OsTDki6"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DcZG9rm2IAiA"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install git+https://github.com/neelnanda-io/TransformerLens.git"
      ],
      "metadata": {
        "id": "F1wsEy0MqHU0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c397c9bf-d6a4-43c0-faca-ffc9c7c4b352"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/neelnanda-io/TransformerLens.git\n",
            "  Cloning https://github.com/neelnanda-io/TransformerLens.git to /tmp/pip-req-build-3je_em5u\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/neelnanda-io/TransformerLens.git /tmp/pip-req-build-3je_em5u\n",
            "  Resolved https://github.com/neelnanda-io/TransformerLens.git to commit 829084a53836c5b8b388aa37a5ffce73b6371712\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: accelerate>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformer-lens==0.0.0) (0.26.1)\n",
            "Requirement already satisfied: beartype<0.15.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformer-lens==0.0.0) (0.14.1)\n",
            "Requirement already satisfied: better-abc<0.0.4,>=0.0.3 in /usr/local/lib/python3.10/dist-packages (from transformer-lens==0.0.0) (0.0.3)\n",
            "Requirement already satisfied: datasets>=2.7.1 in /usr/local/lib/python3.10/dist-packages (from transformer-lens==0.0.0) (2.16.1)\n",
            "Requirement already satisfied: einops>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from transformer-lens==0.0.0) (0.7.0)\n",
            "Requirement already satisfied: fancy-einsum>=0.0.3 in /usr/local/lib/python3.10/dist-packages (from transformer-lens==0.0.0) (0.0.3)\n",
            "Requirement already satisfied: jaxtyping>=0.2.11 in /usr/local/lib/python3.10/dist-packages (from transformer-lens==0.0.0) (0.2.25)\n",
            "Requirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.10/dist-packages (from transformer-lens==0.0.0) (1.26.4)\n",
            "Requirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.10/dist-packages (from transformer-lens==0.0.0) (1.5.3)\n",
            "Requirement already satisfied: rich>=12.6.0 in /usr/local/lib/python3.10/dist-packages (from transformer-lens==0.0.0) (13.7.0)\n",
            "Requirement already satisfied: torch!=2.0,!=2.1.0,>=1.10 in /usr/local/lib/python3.10/dist-packages (from transformer-lens==0.0.0) (2.2.0)\n",
            "Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.10/dist-packages (from transformer-lens==0.0.0) (4.66.1)\n",
            "Requirement already satisfied: transformers>=4.34 in /usr/local/lib/python3.10/dist-packages (from transformer-lens==0.0.0) (4.35.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from transformer-lens==0.0.0) (4.9.0)\n",
            "Requirement already satisfied: wandb>=0.13.5 in /usr/local/lib/python3.10/dist-packages (from transformer-lens==0.0.0) (0.16.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.23.0->transformer-lens==0.0.0) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.23.0->transformer-lens==0.0.0) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.23.0->transformer-lens==0.0.0) (6.0.1)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.23.0->transformer-lens==0.0.0) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.23.0->transformer-lens==0.0.0) (0.4.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer-lens==0.0.0) (3.13.1)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer-lens==0.0.0) (10.0.1)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer-lens==0.0.0) (0.6)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer-lens==0.0.0) (0.3.7)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer-lens==0.0.0) (2.31.0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer-lens==0.0.0) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer-lens==0.0.0) (0.70.15)\n",
            "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer-lens==0.0.0) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer-lens==0.0.0) (3.9.3)\n",
            "Requirement already satisfied: typeguard<3,>=2.13.3 in /usr/local/lib/python3.10/dist-packages (from jaxtyping>=0.2.11->transformer-lens==0.0.0) (2.13.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->transformer-lens==0.0.0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->transformer-lens==0.0.0) (2023.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=12.6.0->transformer-lens==0.0.0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=12.6.0->transformer-lens==0.0.0) (2.16.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch!=2.0,!=2.1.0,>=1.10->transformer-lens==0.0.0) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch!=2.0,!=2.1.0,>=1.10->transformer-lens==0.0.0) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch!=2.0,!=2.1.0,>=1.10->transformer-lens==0.0.0) (3.1.3)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch!=2.0,!=2.1.0,>=1.10->transformer-lens==0.0.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch!=2.0,!=2.1.0,>=1.10->transformer-lens==0.0.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch!=2.0,!=2.1.0,>=1.10->transformer-lens==0.0.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch!=2.0,!=2.1.0,>=1.10->transformer-lens==0.0.0) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch!=2.0,!=2.1.0,>=1.10->transformer-lens==0.0.0) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch!=2.0,!=2.1.0,>=1.10->transformer-lens==0.0.0) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch!=2.0,!=2.1.0,>=1.10->transformer-lens==0.0.0) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch!=2.0,!=2.1.0,>=1.10->transformer-lens==0.0.0) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch!=2.0,!=2.1.0,>=1.10->transformer-lens==0.0.0) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch!=2.0,!=2.1.0,>=1.10->transformer-lens==0.0.0) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch!=2.0,!=2.1.0,>=1.10->transformer-lens==0.0.0) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch!=2.0,!=2.1.0,>=1.10->transformer-lens==0.0.0) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch!=2.0,!=2.1.0,>=1.10->transformer-lens==0.0.0) (12.3.101)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.34->transformer-lens==0.0.0) (2023.12.25)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.34->transformer-lens==0.0.0) (0.15.1)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer-lens==0.0.0) (8.1.7)\n",
            "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer-lens==0.0.0) (3.1.41)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer-lens==0.0.0) (1.40.2)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer-lens==0.0.0) (0.4.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer-lens==0.0.0) (1.3.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer-lens==0.0.0) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer-lens==0.0.0) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer-lens==0.0.0) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb>=0.13.5->transformer-lens==0.0.0) (1.16.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer-lens==0.0.0) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer-lens==0.0.0) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer-lens==0.0.0) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer-lens==0.0.0) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer-lens==0.0.0) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer-lens==0.0.0) (4.0.3)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from GitPython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer-lens==0.0.0) (4.0.11)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=12.6.0->transformer-lens==0.0.0) (0.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=2.7.1->transformer-lens==0.0.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=2.7.1->transformer-lens==0.0.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=2.7.1->transformer-lens==0.0.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=2.7.1->transformer-lens==0.0.0) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch!=2.0,!=2.1.0,>=1.10->transformer-lens==0.0.0) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch!=2.0,!=2.1.0,>=1.10->transformer-lens==0.0.0) (1.3.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer-lens==0.0.0) (5.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "Z6b1n2tvIAiD"
      },
      "outputs": [],
      "source": [
        "import torch as t\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import einops\n",
        "from fancy_einsum import einsum\n",
        "import tqdm.notebook as tqdm\n",
        "import random\n",
        "from pathlib import Path\n",
        "from torch.utils.data import DataLoader\n",
        "from torch import Tensor\n",
        "\n",
        "from jaxtyping import Float, Int\n",
        "from typing import List, Union, Optional, Callable, Tuple, Dict, Literal, Set\n",
        "from functools import partial\n",
        "import copy\n",
        "\n",
        "import itertools\n",
        "from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer\n",
        "import dataclasses\n",
        "import datasets\n",
        "from IPython.display import HTML\n",
        "\n",
        "import pickle\n",
        "from google.colab import files\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import statistics\n",
        "\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "zuhzYxbsIAiE"
      },
      "outputs": [],
      "source": [
        "import transformer_lens\n",
        "import transformer_lens.utils as utils\n",
        "from transformer_lens.hook_points import (\n",
        "    HookedRootModule,\n",
        "    HookPoint,\n",
        ")  # Hooking utilities\n",
        "from transformer_lens import HookedTransformer, HookedTransformerConfig, FactoredMatrix, ActivationCache\n",
        "\n",
        "t.set_grad_enabled(False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Model"
      ],
      "metadata": {
        "id": "OLkInsdjyHMx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "xLwDyosvIAiJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b43e69d-b09d-4463-b4e7-f04a964764c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded pretrained model gpt2-small into HookedTransformer\n"
          ]
        }
      ],
      "source": [
        "model = HookedTransformer.from_pretrained(\n",
        "    model_name,\n",
        "    center_unembed=True,\n",
        "    center_writing_weights=True,\n",
        "    fold_ln=True,\n",
        "    refactor_factored_attn_matrices=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import functions from repo"
      ],
      "metadata": {
        "id": "Z4iJEGh6b56v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !git clone https://github.com/wlg1/seqcont_circ_expms.git\n",
        "!git clone https://github.com/apartresearch/seqcont_circuits.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc3d5783-148a-4b9a-9014-b15d97cdb89c",
        "id": "F8TXMRL3CoPd"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'seqcont_circuits' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %cd /content/seqcont_circ_expms\n",
        "#  node_ablation\n",
        "%cd /content/seqcont_circuits/src/iter_node_pruning"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L41onbdlDQWg",
        "outputId": "508001ac-12c3-4512-e65e-a58157f75545"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/seqcont_circuits/src/node_ablation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from dataset import Dataset\n",
        "from metrics import *\n",
        "from head_ablation_fns import *\n",
        "from mlp_ablation_fns import *\n",
        "from node_ablation_fns import *\n",
        "from loop_node_ablation_fns import *"
      ],
      "metadata": {
        "id": "22TI4zjMDMfQ"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/seqcont_circuits/src/iter_edge_pruning\n",
        "\n",
        "from edge_pruning_fns import *"
      ],
      "metadata": {
        "id": "ujEhuZBNhhFn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load datasets"
      ],
      "metadata": {
        "id": "6Fuq8XW770vX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompts_list = []\n",
        "\n",
        "for i in prompt_types:\n",
        "    # file_name = f'/content/seqcont_circ_expms/data/{task}/{task}_prompts_{i}.pkl'\n",
        "    file_name = f'/content/seqcont_circuits/data/{task}/{task}_prompts_{i}.pkl'\n",
        "    with open(file_name, 'rb') as file:\n",
        "        filelist = pickle.load(file)\n",
        "\n",
        "    print(filelist[0]['text'])\n",
        "    prompts_list += filelist [:num_samps_per_ptype]\n",
        "\n",
        "len(prompts_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CIe5yXuDhgEK",
        "outputId": "88ce03e0-c6f5-4034-db14-0d089b9c8803"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Van done in 1. Hat done in 2. Ring done in 3. Desk done in 4. Sun done in\n",
            "Oil lost in 1. Apple lost in 2. Tree lost in 3. Snow lost in 4. Apple lost in\n",
            "Marcus born in 1. Victoria born in 2. George born in 3. Brandon born in 4. Jamie born in\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1536"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pos_dict = {}\n",
        "for i in range(len(model.tokenizer.tokenize(prompts_list[0]['text']))):\n",
        "    pos_dict['S'+str(i)] = i"
      ],
      "metadata": {
        "id": "kS_Tlrb_70vg"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_1 = Dataset(prompts_list, pos_dict, model.tokenizer)"
      ],
      "metadata": {
        "id": "u0NPSKcZ1iDe"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# file_name = f'/content/seqcont_circ_expms/data/{task}/randDS_{task}.pkl'\n",
        "file_name = f'/content/seqcont_circuits/data/{task}/randDS_{task}.pkl'\n",
        "with open(file_name, 'rb') as file:\n",
        "    prompts_list_2 = pickle.load(file)"
      ],
      "metadata": {
        "id": "-GJ_ZC48FB1i"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_2 = Dataset(prompts_list_2, pos_dict, model.tokenizer)"
      ],
      "metadata": {
        "id": "msu6D4p_feW5"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Path patching fns"
      ],
      "metadata": {
        "id": "OL0vNCqgD3m7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Performance Metrics"
      ],
      "metadata": {
        "id": "ePVYTBKY5QXh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "\n",
        "t.cuda.empty_cache()\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iSjSLuVxIRnE",
        "outputId": "7007d4fb-de25-48af-e7b7-b38e6f057adb"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "VPKTb0ril6mT"
      },
      "outputs": [],
      "source": [
        "def logits_to_ave_logit_diff_2(logits: Float[Tensor, \"batch seq d_vocab\"], dataset_1: Dataset = dataset_1, per_prompt=False):\n",
        "    '''\n",
        "    Returns logit difference between the correct and incorrect answer.\n",
        "\n",
        "    If per_prompt=True, return the array of differences rather than the average.\n",
        "    '''\n",
        "\n",
        "    # Only the final logits are relevant for the answer\n",
        "    # Get the logits corresponding to the indirect object / subject tokens respectively\n",
        "    corr_logits: Float[Tensor, \"batch\"] = logits[range(logits.size(0)), dataset_1.word_idx[\"end\"], dataset_1.corr_tokenIDs]\n",
        "    incorr_logits: Float[Tensor, \"batch\"] = logits[range(logits.size(0)), dataset_1.word_idx[\"end\"], dataset_1.incorr_tokenIDs]\n",
        "    # Find logit difference\n",
        "    answer_logit_diff = corr_logits - incorr_logits\n",
        "    return answer_logit_diff if per_prompt else answer_logit_diff.mean()\n",
        "\n",
        "model.reset_hooks(including_permanent=True)\n",
        "\n",
        "ioi_logits_original = model(dataset_1.toks)\n",
        "abc_logits_original = model(dataset_2.toks)\n",
        "\n",
        "ioi_average_logit_diff = logits_to_ave_logit_diff_2(ioi_logits_original).item()\n",
        "abc_average_logit_diff = logits_to_ave_logit_diff_2(abc_logits_original).item()\n",
        "orig_score = ioi_average_logit_diff"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def ioi_metric_3(\n",
        "    logits: Float[Tensor, \"batch seq d_vocab\"],\n",
        "    clean_logit_diff: float = ioi_average_logit_diff,\n",
        "    corrupted_logit_diff: float = abc_average_logit_diff,\n",
        "    dataset_1: Dataset = dataset_1,\n",
        ") -> float:\n",
        "    patched_logit_diff = logits_to_ave_logit_diff_2(logits, dataset_1)\n",
        "    return (patched_logit_diff / clean_logit_diff)\n",
        "\n",
        "print(f\"IOI metric (IOI dataset): {ioi_metric_3(ioi_logits_original):.4f}\")\n",
        "print(f\"IOI metric (ABC dataset): {ioi_metric_3(abc_logits_original):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5GGyaa_u7jGC",
        "outputId": "1d5818e7-761a-4aa0-bc3b-e7e732069a41"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "IOI metric (IOI dataset): 1.0000\n",
            "IOI metric (ABC dataset): -0.0116\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "del(ioi_logits_original)\n",
        "del(abc_logits_original)"
      ],
      "metadata": {
        "id": "dL5-ITHuKQzx"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## patching fns"
      ],
      "metadata": {
        "id": "IBxzrUmCgsfw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# def patch_head_vectors(\n",
        "#     orig_head_vector: Float[Tensor, \"batch pos head_index d_head\"],\n",
        "#     hook: HookPoint,\n",
        "#     new_cache: ActivationCache,\n",
        "#     orig_cache: ActivationCache,\n",
        "#     head_to_patch: Tuple[int, int],\n",
        "# ) -> Float[Tensor, \"batch pos head_index d_head\"]:\n",
        "#     '''\n",
        "#     '''\n",
        "#     orig_head_vector[...] = orig_cache[hook.name][...]\n",
        "#     if head_to_patch[0] == hook.layer():\n",
        "#         orig_head_vector[:, :, head_to_patch[1]] = new_cache[hook.name][:, :, head_to_patch[1]]\n",
        "#     return orig_head_vector\n",
        "\n",
        "# def patch_head_input(\n",
        "#     orig_activation: Float[Tensor, \"batch pos head_idx d_head\"],\n",
        "#     hook: HookPoint,\n",
        "#     patched_cache: ActivationCache,\n",
        "#     head_list: List[Tuple[int, int]],\n",
        "# ) -> Float[Tensor, \"batch pos head_idx d_head\"]:\n",
        "#     '''\n",
        "#     '''\n",
        "#     heads_to_patch = [head for layer, head in head_list if layer == hook.layer()]\n",
        "#     orig_activation[:, :, heads_to_patch] = patched_cache[hook.name][:, :, heads_to_patch]\n",
        "#     return orig_activation"
      ],
      "metadata": {
        "id": "yXu4jtOUg20q"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "4DZ2SGhfl6mW"
      },
      "outputs": [],
      "source": [
        "# def patch_mlp_vectors(\n",
        "#     orig_MLP_vector: Float[Tensor, \"batch pos d_model\"],\n",
        "#     hook: HookPoint,\n",
        "#     new_cache: ActivationCache,\n",
        "#     orig_cache: ActivationCache,\n",
        "#     layer_to_patch: int,\n",
        "# ) -> Float[Tensor, \"batch pos d_model\"]:\n",
        "#     '''\n",
        "#     '''\n",
        "#     if layer_to_patch == hook.layer():\n",
        "#         orig_MLP_vector[:, :, :] = new_cache[hook.name][:, :, :]\n",
        "#     return orig_MLP_vector\n",
        "\n",
        "# def patch_mlp_input(\n",
        "#     orig_activation: Float[Tensor, \"batch pos d_model\"],\n",
        "#     hook: HookPoint,\n",
        "#     patched_cache: ActivationCache,\n",
        "#     layer_list: List[int],\n",
        "# ) -> Float[Tensor, \"batch pos head_idx d_head\"]:\n",
        "#     '''\n",
        "#     '''\n",
        "#     if hook.layer() in layer_list:\n",
        "#         orig_activation[:, :, :] = patched_cache[hook.name][:, :, :]\n",
        "#     return orig_activation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# only loop thru sender/mlp nodes of circuit"
      ],
      "metadata": {
        "id": "GeskwYZbOY_z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## head to head"
      ],
      "metadata": {
        "id": "8LJmWsjMT6IV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# def circ_path_patch_head_to_heads(\n",
        "#     circuit: List[Tuple[int, int]],\n",
        "#     receiver_heads: List[Tuple[int, int]],\n",
        "#     receiver_input: str,\n",
        "#     model: HookedTransformer,\n",
        "#     patching_metric: Callable,\n",
        "#     new_dataset: Dataset = dataset_2,\n",
        "#     orig_dataset: Dataset = dataset_1,\n",
        "#     new_cache: Optional[ActivationCache] = None,\n",
        "#     orig_cache: Optional[ActivationCache] = None,\n",
        "# ) -> Float[Tensor, \"layer head\"]:\n",
        "#     '''\n",
        "#     Returns:\n",
        "#         tensor of scores of shape (max(receiver_layers), model.cfg.n_heads)\n",
        "#     '''\n",
        "#     assert receiver_input in (\"k\", \"q\", \"v\", \"z\")\n",
        "#     receiver_layers = set(next(zip(*receiver_heads)))  # a set of all layers of receiver heads\n",
        "#     receiver_hook_names = [utils.get_act_name(receiver_input, layer) for layer in receiver_layers]\n",
        "#     receiver_hook_names_filter = lambda name: name in receiver_hook_names\n",
        "\n",
        "#     results = t.zeros(max(receiver_layers), model.cfg.n_heads, device=\"cuda\", dtype=t.float32)\n",
        "\n",
        "#     ### 1. Get activations ###\n",
        "#     z_name_filter = lambda name: name.endswith(\"z\")\n",
        "#     if new_cache is None:\n",
        "#         _, new_cache = model.run_with_cache(\n",
        "#             new_dataset.toks,\n",
        "#             names_filter=z_name_filter,\n",
        "#             return_type=None\n",
        "#         )\n",
        "#     if orig_cache is None:\n",
        "#         _, orig_cache = model.run_with_cache(\n",
        "#             orig_dataset.toks,\n",
        "#             names_filter=z_name_filter,\n",
        "#             return_type=None\n",
        "#         )\n",
        "\n",
        "#     senders = [tup for tup in circuit if tup[0] < receiver_heads[0][0]]  # senders are in layer before receivers\n",
        "\n",
        "#     for (sender_layer, sender_head) in tqdm(senders):\n",
        "#         ### 2. Frozen Clean Run with sender node patched from Corrupted Run ###\n",
        "#         hook_fn = partial(\n",
        "#             patch_head_vectors,\n",
        "#             new_cache=new_cache,\n",
        "#             orig_cache=orig_cache,\n",
        "#             head_to_patch=(sender_layer, sender_head),\n",
        "#         )\n",
        "#         model.add_hook(z_name_filter, hook_fn, level=1)\n",
        "\n",
        "#         _, patched_cache = model.run_with_cache(\n",
        "#             orig_dataset.toks,\n",
        "#             names_filter=receiver_hook_names_filter,\n",
        "#             return_type=None\n",
        "#         )\n",
        "#         # model.reset_hooks(including_permanent=True)\n",
        "#         assert set(patched_cache.keys()) == set(receiver_hook_names)\n",
        "\n",
        "#         ### 3. Clean Run, with patched in received node from step 2 ###\n",
        "#         hook_fn = partial(\n",
        "#             patch_head_input,\n",
        "#             patched_cache=patched_cache,\n",
        "#             head_list=receiver_heads,\n",
        "#         )\n",
        "#         patched_logits = model.run_with_hooks(\n",
        "#             orig_dataset.toks,\n",
        "#             fwd_hooks = [(receiver_hook_names_filter, hook_fn)],\n",
        "#             return_type=\"logits\"\n",
        "#         )\n",
        "\n",
        "#         results[sender_layer, sender_head] = patching_metric(patched_logits)\n",
        "\n",
        "#     return results"
      ],
      "metadata": {
        "id": "GecpRVxKT6Id"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## mlp to mlp"
      ],
      "metadata": {
        "id": "M_b5txIwj7Ql"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "7WqoY8OEj7Qm"
      },
      "outputs": [],
      "source": [
        "# def circ_path_patch_MLPs_to_MLPs(\n",
        "#     mlp_circuit: List[int],\n",
        "#     receiver_layers: List[int],\n",
        "#     model: HookedTransformer,\n",
        "#     patching_metric: Callable,\n",
        "#     new_dataset: Dataset = dataset_2,\n",
        "#     orig_dataset: Dataset = dataset_1,\n",
        "#     new_cache: Optional[ActivationCache] = None,\n",
        "#     orig_cache: Optional[ActivationCache] = None,\n",
        "# ) -> Float[Tensor, \"layer head\"]:\n",
        "#     '''\n",
        "#     Returns:\n",
        "#         tensor of scores of shape (max(receiver_layers), model.cfg.n_heads)\n",
        "#     '''\n",
        "#     # model.reset_hooks()\n",
        "#     receiver_hook_names = [utils.get_act_name('mlp_out', layer) for layer in receiver_layers]  # modify for mlp_out\n",
        "#     receiver_hook_names_filter = lambda name: name in receiver_hook_names\n",
        "\n",
        "#     results = t.zeros(max(receiver_layers), device=\"cuda\", dtype=t.float32)\n",
        "\n",
        "#     ### 1. Get activations ###\n",
        "#     z_name_filter = lambda name: name.endswith(\"mlp_out\")\n",
        "\n",
        "#     if new_cache is None:\n",
        "#         _, new_cache = model.run_with_cache(\n",
        "#             new_dataset.toks,\n",
        "#             names_filter=z_name_filter,\n",
        "#             return_type=None\n",
        "#         )\n",
        "#     if orig_cache is None:\n",
        "#         _, orig_cache = model.run_with_cache(\n",
        "#             orig_dataset.toks,\n",
        "#             names_filter=z_name_filter,\n",
        "#             return_type=None\n",
        "#         )\n",
        "\n",
        "#     sender_mlp_list = [L for L in mlp_circuit if L < max(receiver_layers)]\n",
        "#     for (sender_layer) in sender_mlp_list:\n",
        "#         ### 2. Frozen Clean Run with sender node patched from Corrupted Run ###\n",
        "#         hook_fn = partial(\n",
        "#             patch_mlp_vectors,\n",
        "#             new_cache=new_cache,\n",
        "#             orig_cache=orig_cache,\n",
        "#             layer_to_patch = sender_layer\n",
        "#         )\n",
        "\n",
        "#         model.add_hook(z_name_filter, hook_fn, level=1)\n",
        "\n",
        "#         _, patched_cache = model.run_with_cache(\n",
        "#             orig_dataset.toks,\n",
        "#             names_filter=receiver_hook_names_filter,\n",
        "#             return_type=None\n",
        "#         )\n",
        "#         # model.reset_hooks(including_permanent=True)\n",
        "#         # assert set(patched_cache.keys()) == set(receiver_hook_names)\n",
        "\n",
        "#         ### 3. Clean Run with patched receiver node from step 2 ###\n",
        "#         hook_fn = partial(\n",
        "#             patch_mlp_input,\n",
        "#             patched_cache=patched_cache,\n",
        "#             layer_list=receiver_layers,\n",
        "#         )\n",
        "#         patched_logits = model.run_with_hooks(\n",
        "#             orig_dataset.toks,\n",
        "#             fwd_hooks = [(receiver_hook_names_filter, hook_fn)],\n",
        "#             return_type=\"logits\"\n",
        "#         )\n",
        "\n",
        "#         results[sender_layer] = patching_metric(patched_logits)\n",
        "\n",
        "#     # the result is which sender layers affect ALL the inputted nodes. this is why we just\n",
        "#     # want to pass one node at a time- to see which layers affect just IT.\n",
        "#     # if we want a 'group of nodes under a common type', we'd pass a set of nodes\n",
        "#     return results"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## head to MLP"
      ],
      "metadata": {
        "id": "MAqhd68popR0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "head senders to MLP receiver (circ nodes)"
      ],
      "metadata": {
        "id": "IYWv3k4uopR1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# def circ_path_patch_head_to_mlp(\n",
        "#     circuit: List[Tuple[int, int]],\n",
        "#     receiver_layers: List[int],\n",
        "#     model: HookedTransformer,\n",
        "#     patching_metric: Callable,\n",
        "#     new_dataset: Dataset = dataset_2,\n",
        "#     orig_dataset: Dataset = dataset_1,\n",
        "#     new_cache: Optional[ActivationCache] = None,\n",
        "#     orig_cache: Optional[ActivationCache] = None,\n",
        "# ) -> Float[Tensor, \"layer head\"]:\n",
        "#     '''\n",
        "#     Returns:\n",
        "#         tensor of scores of shape (max(receiver_layers), model.cfg.n_heads)\n",
        "#     '''\n",
        "#     # model.reset_hooks()\n",
        "\n",
        "#     receiver_hook_names = [utils.get_act_name('mlp_out', layer) for layer in receiver_layers]\n",
        "#     receiver_hook_names_filter = lambda name: name in receiver_hook_names\n",
        "\n",
        "#     results = t.zeros(max(receiver_layers), model.cfg.n_heads, device=\"cuda\", dtype=t.float32)\n",
        "\n",
        "#     ### 1. Get activations ###\n",
        "#     z_name_filter = lambda name: name.endswith((\"z\", \"mlpout\"))  # gets same value as just z\n",
        "\n",
        "#     if new_cache is None:\n",
        "#         _, new_cache = model.run_with_cache(\n",
        "#             new_dataset.toks,\n",
        "#             names_filter=z_name_filter,\n",
        "#             return_type=None\n",
        "#         )\n",
        "#     if orig_cache is None:\n",
        "#         _, orig_cache = model.run_with_cache(\n",
        "#             orig_dataset.toks,\n",
        "#             names_filter=z_name_filter,\n",
        "#             return_type=None\n",
        "#         )\n",
        "\n",
        "#     senders = [tup for tup in circuit if tup[0] < receiver_layers[0]]\n",
        "#     for (sender_layer, sender_head) in tqdm(senders):\n",
        "#         ### 2. Frozen Clean Run with sender node patched from Corrupted Run ###\n",
        "#         hook_fn = partial(\n",
        "#             patch_head_vectors,\n",
        "#             new_cache=new_cache,\n",
        "#             orig_cache=orig_cache,\n",
        "#             head_to_patch=(sender_layer, sender_head),\n",
        "#         )\n",
        "\n",
        "#         model.add_hook(z_name_filter, hook_fn, level=1)\n",
        "\n",
        "#         _, patched_cache = model.run_with_cache(\n",
        "#             orig_dataset.toks,\n",
        "#             names_filter=receiver_hook_names_filter,\n",
        "#             return_type=None\n",
        "#         )\n",
        "#         # model.reset_hooks(including_permanent=True)\n",
        "#         # assert set(patched_cache.keys()) == set(receiver_hook_names)\n",
        "\n",
        "#         ### 3. Clean Run with patched receiver node from step 2 ###\n",
        "\n",
        "#         hook_fn = partial(\n",
        "#             patch_mlp_input,\n",
        "#             patched_cache=patched_cache,\n",
        "#             layer_list=receiver_layers,\n",
        "#         )\n",
        "#         patched_logits = model.run_with_hooks(\n",
        "#             orig_dataset.toks,\n",
        "#             fwd_hooks = [(receiver_hook_names_filter, hook_fn)],\n",
        "#             return_type=\"logits\"\n",
        "#         )\n",
        "\n",
        "#         results[sender_layer, sender_head] = patching_metric(patched_logits)\n",
        "\n",
        "#     return results"
      ],
      "metadata": {
        "id": "RaTv9OYbopR1"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MLP to head"
      ],
      "metadata": {
        "id": "ok__ApdwYsSu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "MLP senders to head receiver (circ nodes)"
      ],
      "metadata": {
        "id": "tKFYikeHYsSv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# def circ_path_patch_mlp_to_head(\n",
        "#     mlp_circuit: List[int],\n",
        "#     receiver_heads: List[Tuple[int, int]],\n",
        "#     receiver_input: str,\n",
        "#     model: HookedTransformer,\n",
        "#     patching_metric: Callable,\n",
        "#     new_dataset: Dataset = dataset_2,\n",
        "#     orig_dataset: Dataset = dataset_1,\n",
        "#     new_cache: Optional[ActivationCache] = None,\n",
        "#     orig_cache: Optional[ActivationCache] = None,\n",
        "# ) -> Float[Tensor, \"layer head\"]:\n",
        "#     '''\n",
        "#     Returns:\n",
        "#         tensor of scores of shape (max(receiver_layers), model.cfg.n_heads)\n",
        "#     '''\n",
        "#     # model.reset_hooks() # doesn't make diff if comment out or not\n",
        "\n",
        "#     assert receiver_input in (\"k\", \"q\", \"v\", \"z\")  # we can run get_path_patch_head_to_heads() 3 times for k, q, v!\n",
        "#     receiver_layers = set(next(zip(*receiver_heads)))\n",
        "#     receiver_hook_names = [utils.get_act_name(receiver_input, layer) for layer in receiver_layers]\n",
        "#     receiver_hook_names_filter = lambda name: name in receiver_hook_names\n",
        "\n",
        "#     results = t.zeros(max(receiver_layers), device=\"cuda\", dtype=t.float32)\n",
        "\n",
        "#     ### 1. Get activations ###\n",
        "#     z_name_filter = lambda name: name.endswith((\"z\", \"mlpout\"))  # gets same value as just mlp out\n",
        "\n",
        "#     if new_cache is None:\n",
        "#         _, new_cache = model.run_with_cache(\n",
        "#             new_dataset.toks,\n",
        "#             names_filter=z_name_filter,\n",
        "#             return_type=None\n",
        "#         )\n",
        "#     if orig_cache is None:\n",
        "#         _, orig_cache = model.run_with_cache(\n",
        "#             orig_dataset.toks,\n",
        "#             names_filter=z_name_filter,\n",
        "#             return_type=None\n",
        "#         )\n",
        "\n",
        "#     sender_mlp_list = [L for L in mlp_circuit if L < receiver_heads[0][0]]\n",
        "#     for (sender_layer) in sender_mlp_list:\n",
        "#         ### 2. Frozen Clean Run with sender node patched from Corrupted Run ###\n",
        "#         hook_fn = partial(\n",
        "#             patch_mlp_vectors,\n",
        "#             new_cache=new_cache,\n",
        "#             orig_cache=orig_cache,\n",
        "#             layer_to_patch = sender_layer\n",
        "#         )\n",
        "\n",
        "#         model.add_hook(z_name_filter, hook_fn, level=1)\n",
        "\n",
        "#         _, patched_cache = model.run_with_cache(\n",
        "#             orig_dataset.toks,\n",
        "#             names_filter=receiver_hook_names_filter,\n",
        "#             return_type=None\n",
        "#         )\n",
        "#         # model.reset_hooks(including_permanent=True)\n",
        "#         assert set(patched_cache.keys()) == set(receiver_hook_names)\n",
        "\n",
        "#         ### 3. Clean Run with patched receiver node from step 2 ###\n",
        "#         hook_fn = partial(\n",
        "#             patch_head_input,\n",
        "#             patched_cache=patched_cache,\n",
        "#             head_list=receiver_heads,\n",
        "#         )\n",
        "#         patched_logits = model.run_with_hooks(\n",
        "#             orig_dataset.toks,\n",
        "#             fwd_hooks = [(receiver_hook_names_filter, hook_fn)],\n",
        "#             return_type=\"logits\"\n",
        "#         )\n",
        "\n",
        "#         results[sender_layer] = patching_metric(patched_logits)\n",
        "\n",
        "#     return results"
      ],
      "metadata": {
        "id": "uXVDECcvYsSv"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# loop backw"
      ],
      "metadata": {
        "id": "c54cqFf2lT2F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## get circuit"
      ],
      "metadata": {
        "id": "no8gv3Hgka8Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get the node ablated circuit without edges ablated."
      ],
      "metadata": {
        "id": "tmFnku5GOeVn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "heads_not_ablate = [(0, 1), (1, 5), (4, 4), (4, 10), (5, 0), (6, 1), (6, 6), (6, 10), (7, 11), (8, 1), (8, 6), (8, 8), (8, 9), (9, 1)]\n",
        "mlps_not_ablate = [0, 1, 2, 3, 4, 6, 7, 8, 9, 10, 11]\n",
        "\n",
        "orig_score = ioi_average_logit_diff\n",
        "\n",
        "model.reset_hooks(including_permanent=True)  #must do this after running with mean ablation hook\n",
        "abl_model = add_ablation_hook_MLP_head(model, dataset_2, heads_not_ablate, mlps_not_ablate)\n",
        "\n",
        "new_logits = model(dataset_1.toks)\n",
        "new_score = logits_to_ave_logit_diff_2(new_logits, dataset_1)\n",
        "circ_score = (100 * new_score / orig_score).item()\n",
        "print(f\"(cand circuit / full) %: {circ_score:.4f}\")\n",
        "del(new_logits)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vLVucYAnkdKK",
        "outputId": "c74f97ee-7042-4849-a27c-4e557cbb4d83"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(cand circuit / full) %: 81.0075\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## head to head"
      ],
      "metadata": {
        "id": "HLA7GH89vZY2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "qkv_to_HH = {} # qkv to dict\n",
        "\n",
        "for head_type in [\"q\", \"k\", \"v\"]:\n",
        "    head_to_head_results = {}\n",
        "    for head in heads_not_ablate:\n",
        "        print(head_type, head)\n",
        "        model.reset_hooks()\n",
        "        model = add_ablation_hook_MLP_head(model, dataset_2, heads_not_ablate, mlps_not_ablate)\n",
        "\n",
        "        result = circ_path_patch_head_to_heads(\n",
        "            circuit = heads_not_ablate,\n",
        "            receiver_heads = [head],\n",
        "            receiver_input = head_type,\n",
        "            model = model,\n",
        "            patching_metric = ioi_metric_3\n",
        "        )\n",
        "        head_to_head_results[head] = result\n",
        "    qkv_to_HH[head_type] = head_to_head_results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LfncU3j7vWWC",
        "outputId": "534c9208-1d8c-4ca4-d767-8a2424231465"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "q (0, 1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "0it [00:00, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "q (1, 5)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 1/1 [00:01<00:00,  1.12s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "q (4, 4)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 2/2 [00:02<00:00,  1.13s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "q (4, 10)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 2/2 [00:02<00:00,  1.12s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "q (5, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 4/4 [00:04<00:00,  1.12s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "q (6, 1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 5/5 [00:05<00:00,  1.12s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "q (6, 6)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 5/5 [00:05<00:00,  1.13s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "q (6, 10)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 5/5 [00:05<00:00,  1.13s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "q (7, 11)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 8/8 [00:09<00:00,  1.13s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "q (8, 1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 9/9 [00:10<00:00,  1.12s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "q (8, 6)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 9/9 [00:10<00:00,  1.13s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "q (8, 8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 9/9 [00:10<00:00,  1.13s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "q (8, 9)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 9/9 [00:10<00:00,  1.13s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "q (9, 1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 13/13 [00:14<00:00,  1.12s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "k (0, 1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "0it [00:00, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "k (1, 5)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 1/1 [00:01<00:00,  1.12s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "k (4, 4)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 2/2 [00:02<00:00,  1.13s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "k (4, 10)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 2/2 [00:02<00:00,  1.13s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "k (5, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 4/4 [00:04<00:00,  1.13s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "k (6, 1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 5/5 [00:05<00:00,  1.13s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "k (6, 6)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 5/5 [00:05<00:00,  1.13s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "k (6, 10)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 5/5 [00:05<00:00,  1.13s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "k (7, 11)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 8/8 [00:09<00:00,  1.13s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "k (8, 1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 9/9 [00:10<00:00,  1.13s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "k (8, 6)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 9/9 [00:10<00:00,  1.13s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "k (8, 8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 9/9 [00:10<00:00,  1.13s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "k (8, 9)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 9/9 [00:10<00:00,  1.13s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "k (9, 1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 13/13 [00:14<00:00,  1.13s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "v (0, 1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "0it [00:00, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "v (1, 5)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 1/1 [00:01<00:00,  1.13s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "v (4, 4)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 2/2 [00:02<00:00,  1.13s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "v (4, 10)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 2/2 [00:02<00:00,  1.13s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "v (5, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 4/4 [00:04<00:00,  1.13s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "v (6, 1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 5/5 [00:05<00:00,  1.13s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "v (6, 6)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 5/5 [00:05<00:00,  1.13s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "v (6, 10)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 5/5 [00:05<00:00,  1.13s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "v (7, 11)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 8/8 [00:09<00:00,  1.13s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "v (8, 1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 9/9 [00:10<00:00,  1.13s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "v (8, 6)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 9/9 [00:10<00:00,  1.13s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "v (8, 8)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 9/9 [00:10<00:00,  1.13s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "v (8, 9)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 9/9 [00:10<00:00,  1.13s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "v (9, 1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 13/13 [00:14<00:00,  1.13s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "head_to_head_adjList = {}\n",
        "for head_type in [\"q\", \"k\", \"v\"]:\n",
        "    for head in heads_not_ablate:\n",
        "        result = qkv_to_HH[head_type][head]\n",
        "        filtered_indices = (result < 0.8) & (result != 0.0)\n",
        "        rows, cols = filtered_indices.nonzero(as_tuple=True)\n",
        "        sender_nodes = list(zip(rows.tolist(), cols.tolist()))\n",
        "        head_with_type = head + (head_type,)\n",
        "        head_to_head_adjList[head_with_type] = sender_nodes"
      ],
      "metadata": {
        "id": "VE2IpLNpHgeb"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## mlp to mlp"
      ],
      "metadata": {
        "id": "ccROLT75vT7y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mlp_to_mlp_results = {}\n",
        "\n",
        "# for layer in range(11, 0, -1):\n",
        "for layer in reversed(mlps_not_ablate):\n",
        "    print(layer)\n",
        "    model.reset_hooks()\n",
        "    model = add_ablation_hook_MLP_head(model, dataset_2, heads_not_ablate, mlps_not_ablate)\n",
        "    result = circ_path_patch_MLPs_to_MLPs(\n",
        "        mlp_circuit = mlps_not_ablate,\n",
        "        receiver_layers = [layer],\n",
        "        model = model,\n",
        "        patching_metric = ioi_metric_3\n",
        "    )\n",
        "    mlp_to_mlp_results[layer] = result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nD4bTlmHlVks",
        "outputId": "10285e50-f0f2-43e0-d2a7-f38625dcdab7"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11\n",
            "10\n",
            "9\n",
            "8\n",
            "7\n",
            "6\n",
            "4\n",
            "3\n",
            "2\n",
            "1\n",
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mlp_to_mlp_adjList = {}\n",
        "for mlp in mlps_not_ablate:\n",
        "    result = mlp_to_mlp_results[mlp]\n",
        "    filtered_indices = (result < 0.80) & (result != 0.0)\n",
        "    filtered_indices = filtered_indices.nonzero(as_tuple=True)[0]\n",
        "    mlp_to_mlp_adjList[mlp] = filtered_indices.tolist()"
      ],
      "metadata": {
        "id": "NN0Vd9eIrpL9"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## head to mlp"
      ],
      "metadata": {
        "id": "qStvFtFrBHTE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "head_to_mlp_results = {}\n",
        "\n",
        "# for layer in range(11, 0, -1):\n",
        "for layer in reversed(mlps_not_ablate):\n",
        "    print(layer)\n",
        "    model.reset_hooks()\n",
        "    model = add_ablation_hook_MLP_head(model, dataset_2, heads_not_ablate, mlps_not_ablate)\n",
        "    result = circ_path_patch_head_to_mlp(\n",
        "        circuit = heads_not_ablate,\n",
        "        receiver_layers = [layer],\n",
        "        model = model,\n",
        "        patching_metric = ioi_metric_3\n",
        "    )\n",
        "    head_to_mlp_results[layer] = result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RkeGOxibBMIs",
        "outputId": "e74d0286-c349-4b93-f99a-05299fce348e"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 14/14 [00:15<00:00,  1.13s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 14/14 [00:15<00:00,  1.13s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 13/13 [00:14<00:00,  1.13s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 9/9 [00:10<00:00,  1.13s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 8/8 [00:09<00:00,  1.13s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 5/5 [00:05<00:00,  1.13s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 2/2 [00:02<00:00,  1.13s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 2/2 [00:02<00:00,  1.13s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 2/2 [00:02<00:00,  1.13s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 1/1 [00:01<00:00,  1.13s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "0it [00:00, ?it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "head_to_mlp_adjList = {}\n",
        "for layer in mlps_not_ablate:\n",
        "    result = head_to_mlp_results[layer]\n",
        "    filtered_indices = (result < 0.8) & (result != 0.0)\n",
        "    rows, cols = filtered_indices.nonzero(as_tuple=True)\n",
        "    sender_nodes = list(zip(rows.tolist(), cols.tolist()))\n",
        "    head_to_mlp_adjList[layer] = sender_nodes"
      ],
      "metadata": {
        "id": "9sSF5sgSCQfn"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## mlp to head"
      ],
      "metadata": {
        "id": "y1DQqZXrEi4o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "qkv_mlp_to_HH = {} # qkv to dict\n",
        "\n",
        "for head_type in [\"q\", \"k\", \"v\"]:\n",
        "    mlp_to_head_results = {}\n",
        "    for head in heads_not_ablate:\n",
        "        print(head_type, head)\n",
        "        model.reset_hooks()\n",
        "        model = add_ablation_hook_MLP_head(model, dataset_2, heads_not_ablate, mlps_not_ablate)\n",
        "\n",
        "        result = circ_path_patch_mlp_to_head(\n",
        "            mlp_circuit = mlps_not_ablate,\n",
        "            receiver_heads = [head],\n",
        "            receiver_input = head_type,\n",
        "            model = model,\n",
        "            patching_metric = ioi_metric_3\n",
        "        )\n",
        "        mlp_to_head_results[head] = result\n",
        "    qkv_mlp_to_HH[head_type] = mlp_to_head_results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4GdPAN1wEuhc",
        "outputId": "28c91f1b-6772-400d-b026-4a4717a846fa"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "q (0, 1)\n",
            "q (1, 5)\n",
            "q (4, 4)\n",
            "q (4, 10)\n",
            "q (5, 0)\n",
            "q (6, 1)\n",
            "q (6, 6)\n",
            "q (6, 10)\n",
            "q (7, 11)\n",
            "q (8, 1)\n",
            "q (8, 6)\n",
            "q (8, 8)\n",
            "q (8, 9)\n",
            "q (9, 1)\n",
            "k (0, 1)\n",
            "k (1, 5)\n",
            "k (4, 4)\n",
            "k (4, 10)\n",
            "k (5, 0)\n",
            "k (6, 1)\n",
            "k (6, 6)\n",
            "k (6, 10)\n",
            "k (7, 11)\n",
            "k (8, 1)\n",
            "k (8, 6)\n",
            "k (8, 8)\n",
            "k (8, 9)\n",
            "k (9, 1)\n",
            "v (0, 1)\n",
            "v (1, 5)\n",
            "v (4, 4)\n",
            "v (4, 10)\n",
            "v (5, 0)\n",
            "v (6, 1)\n",
            "v (6, 6)\n",
            "v (6, 10)\n",
            "v (7, 11)\n",
            "v (8, 1)\n",
            "v (8, 6)\n",
            "v (8, 8)\n",
            "v (8, 9)\n",
            "v (9, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mlp_to_head_adjList = {}\n",
        "for head_type in [\"q\", \"k\", \"v\"]:\n",
        "    for head in heads_not_ablate:\n",
        "        result = qkv_mlp_to_HH[head_type][head]\n",
        "        filtered_indices = (result < 0.8) & (result != 0.0)\n",
        "        filtered_indices = filtered_indices.nonzero(as_tuple=True)[0]\n",
        "        head_with_type = head + (head_type,)\n",
        "        mlp_to_head_adjList[head_with_type] = filtered_indices.tolist()"
      ],
      "metadata": {
        "id": "OKDLGm2KPlJ2"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# save graph files to free up memory"
      ],
      "metadata": {
        "id": "u1VIlinotlLf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(task + \"_head_to_head_results.pkl\", \"wb\") as file:\n",
        "    pickle.dump(head_to_head_results, file)\n",
        "files.download(task + \"_head_to_head_results.pkl\")\n",
        "\n",
        "with open(task + \"_mlp_to_mlp_results.pkl\", \"wb\") as file:\n",
        "    pickle.dump(mlp_to_mlp_results, file)\n",
        "files.download(task + \"_mlp_to_mlp_results.pkl\")\n",
        "\n",
        "with open(task + \"_head_to_mlp_results.pkl\", \"wb\") as file:\n",
        "    pickle.dump(head_to_mlp_results, file)\n",
        "files.download(task + \"_head_to_mlp_results.pkl\")\n",
        "\n",
        "with open(task + \"_mlp_to_head_results.pkl\", \"wb\") as file:\n",
        "    pickle.dump(mlp_to_head_results, file)\n",
        "files.download(task + \"_mlp_to_head_results.pkl\")\n",
        "\n",
        "del(head_to_head_results)\n",
        "del(mlp_to_mlp_results)\n",
        "del(head_to_mlp_results)\n",
        "del(mlp_to_head_results)"
      ],
      "metadata": {
        "id": "U8Fosqu6rt3_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "9785d7f7-0e32-4143-8bbe-75aedd0bddd8"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_72c9322c-42fc-4128-8716-39c9cf18706a\", \"numerals_head_to_head_results.pkl\", 8138)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_88219577-3c5f-4541-9ced-c12eff49b37d\", \"numerals_mlp_to_mlp_results.pkl\", 3557)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_5c626ae2-f1db-4172-a814-de948ce5ce98\", \"numerals_head_to_mlp_results.pkl\", 6285)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_5904455e-a440-43f1-a4b6-3f69988a6914\", \"numerals_mlp_to_head_results.pkl\", 4562)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# resid post"
      ],
      "metadata": {
        "id": "xyMeogkrRdVJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## head to resid"
      ],
      "metadata": {
        "id": "ZT1l8tATRna6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# def get_path_patch_head_to_final_resid_post(\n",
        "#     circuit: List[Tuple[int, int]],\n",
        "#     model: HookedTransformer,\n",
        "#     patching_metric: Callable,\n",
        "#     new_dataset: Dataset = dataset_2,\n",
        "#     orig_dataset: Dataset = dataset_1,\n",
        "# ) -> Float[Tensor, \"layer head\"]:\n",
        "#     '''\n",
        "#     Returns:\n",
        "\n",
        "#     '''\n",
        "#     # model.reset_hooks()\n",
        "#     results = t.zeros(model.cfg.n_layers, model.cfg.n_heads, device=\"cuda\", dtype=t.float32)\n",
        "\n",
        "#     resid_post_hook_name = utils.get_act_name(\"resid_post\", model.cfg.n_layers - 1)\n",
        "#     resid_post_name_filter = lambda name: name == resid_post_hook_name\n",
        "\n",
        "\n",
        "#     ### 1. Get activations ###\n",
        "#     z_name_filter = lambda name: name.endswith(\"z\")\n",
        "\n",
        "#     _, new_cache = model.run_with_cache(\n",
        "#         new_dataset.toks,\n",
        "#         names_filter=z_name_filter,\n",
        "#         return_type=None\n",
        "#     )\n",
        "\n",
        "#     _, orig_cache = model.run_with_cache(\n",
        "#         orig_dataset.toks,\n",
        "#         names_filter=z_name_filter,\n",
        "#         return_type=None\n",
        "#     )\n",
        "\n",
        "#     for (sender_layer, sender_head) in tqdm(circuit):\n",
        "#         ### 2. Frozen Clean Run with sender node patched from Corrupted Run ###\n",
        "#         hook_fn = partial(\n",
        "#             patch_head_vectors,\n",
        "#             new_cache=new_cache,\n",
        "#             orig_cache=orig_cache,\n",
        "#             head_to_patch=(sender_layer, sender_head),\n",
        "#         )\n",
        "#         model.add_hook(z_name_filter, hook_fn)\n",
        "\n",
        "#         _, patched_cache = model.run_with_cache(\n",
        "#             orig_dataset.toks,\n",
        "#             names_filter=resid_post_name_filter,\n",
        "#             return_type=None\n",
        "#         )\n",
        "\n",
        "#         assert set(patched_cache.keys()) == {resid_post_hook_name}\n",
        "\n",
        "#         ### 3. Clean Run with patched receiver node from step 2 ###\n",
        "#         patched_logits = model.unembed(model.ln_final(patched_cache[resid_post_hook_name]))\n",
        "\n",
        "#         results[sender_layer, sender_head] = patching_metric(patched_logits)\n",
        "\n",
        "#     return results"
      ],
      "metadata": {
        "id": "DqvrtttURoeL"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.reset_hooks()\n",
        "model = add_ablation_hook_MLP_head(model, dataset_2, heads_not_ablate, mlps_not_ablate)\n",
        "\n",
        "path_patch_head_to_final_resid_post = get_path_patch_head_to_final_resid_post(heads_not_ablate, model, ioi_metric_3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4m3Qf4FHTzwE",
        "outputId": "44898795-361e-4796-eb33-42c107683ca2"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 14/14 [00:09<00:00,  1.52it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path_patch_head_to_final_resid_post.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aLruYArCUbjd",
        "outputId": "746784e3-d22c-4662-f792-738907645590"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([12, 12])"
            ]
          },
          "metadata": {},
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "heads_to_resid = {}\n",
        "result = path_patch_head_to_final_resid_post\n",
        "filtered_indices = (result < 0.8) & (result != 0.0)\n",
        "rows, cols = filtered_indices.nonzero(as_tuple=True)\n",
        "heads_to_resid['resid'] = list(zip(rows.tolist(), cols.tolist()))"
      ],
      "metadata": {
        "id": "81vDVv78Re-A"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## mlp to resid"
      ],
      "metadata": {
        "id": "gdBVxmksWbO_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# def get_path_patch_mlp_to_final_resid_post(\n",
        "#     mlp_circuit: List[int],\n",
        "#     model: HookedTransformer,\n",
        "#     patching_metric: Callable,\n",
        "#     new_dataset: Dataset = dataset_2,\n",
        "#     orig_dataset: Dataset = dataset_1,\n",
        "# ) -> Float[Tensor, \"layer head\"]:\n",
        "#     '''\n",
        "#     Performs path patching (see algorithm in appendix B of IOI paper), with:\n",
        "\n",
        "#         sender head = (each head, looped through, one at a time)\n",
        "#         receiver node = final value of residual stream\n",
        "\n",
        "#     Returns:\n",
        "#         tensor of metric values for every possible sender head\n",
        "#     '''\n",
        "#     # model.reset_hooks()\n",
        "#     results = t.zeros(model.cfg.n_layers, device=\"cuda\", dtype=t.float32) #model.cfg.n_heads,\n",
        "\n",
        "#     resid_post_hook_name = utils.get_act_name(\"resid_post\", model.cfg.n_layers - 1)\n",
        "#     resid_post_name_filter = lambda name: name == resid_post_hook_name\n",
        "\n",
        "\n",
        "#     ### 1. Get activations ###\n",
        "#     z_name_filter = lambda name: name.endswith((\"z\", \"mlp_out\"))\n",
        "\n",
        "#     _, new_cache = model.run_with_cache(\n",
        "#         new_dataset.toks,\n",
        "#         names_filter=z_name_filter,\n",
        "#         return_type=None\n",
        "#     )\n",
        "\n",
        "#     _, orig_cache = model.run_with_cache(\n",
        "#         orig_dataset.toks,\n",
        "#         names_filter=z_name_filter,\n",
        "#         return_type=None\n",
        "#     )\n",
        "\n",
        "#     for sender_layer in mlp_circuit:\n",
        "#         ### 2. Frozen Clean Run with sender node patched from Corrupted Run ###\n",
        "#         hook_fn = partial(\n",
        "#             patch_mlp_vectors,\n",
        "#             new_cache=new_cache,\n",
        "#             orig_cache=orig_cache,\n",
        "#             layer_to_patch = sender_layer\n",
        "#         )\n",
        "#         model.add_hook(z_name_filter, hook_fn)\n",
        "\n",
        "#         _, patched_cache = model.run_with_cache(\n",
        "#             orig_dataset.toks,\n",
        "#             names_filter=resid_post_name_filter,\n",
        "#             return_type=None\n",
        "#         )\n",
        "\n",
        "#         assert set(patched_cache.keys()) == {resid_post_hook_name}\n",
        "\n",
        "#         ### 3. Clean Run with patched receiver node from step 2 ###\n",
        "\n",
        "#         patched_logits = model.unembed(model.ln_final(patched_cache[resid_post_hook_name]))\n",
        "\n",
        "#         results[sender_layer] = patching_metric(patched_logits)\n",
        "\n",
        "#     return results"
      ],
      "metadata": {
        "id": "tjz_J2HMWexY"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.reset_hooks()\n",
        "model = add_ablation_hook_MLP_head(model, dataset_2, heads_not_ablate, mlps_not_ablate)\n",
        "\n",
        "path_patch_mlp_to_final_resid_post = get_path_patch_mlp_to_final_resid_post(mlps_not_ablate, model, ioi_metric_3)"
      ],
      "metadata": {
        "id": "oSbfi1kPZqI6"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path_patch_mlp_to_final_resid_post.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5828cda9-df61-469e-c902-97f7d37be299",
        "id": "ENP8nod0ZqI7"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([12])"
            ]
          },
          "metadata": {},
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mlps_to_resid = {}\n",
        "result = path_patch_mlp_to_final_resid_post\n",
        "filtered_indices = (result < 0.8) & (result != 0.0)\n",
        "filtered_indices = filtered_indices.nonzero(as_tuple=True)[0]\n",
        "mlps_to_resid['resid'] = filtered_indices.tolist()"
      ],
      "metadata": {
        "id": "idRsYRx2ZqI7"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# filter out nodes with no ingoing edges"
      ],
      "metadata": {
        "id": "o8sGuo1wb10E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "head_to_head_adjList = {node: neighbors for node, neighbors in head_to_head_adjList.items() if neighbors}"
      ],
      "metadata": {
        "id": "L5BLSrZAb5sX"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mlp_to_head_adjList = {node: neighbors for node, neighbors in mlp_to_head_adjList.items() if neighbors}"
      ],
      "metadata": {
        "id": "fXuZqTiFcL0i"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# save graph files"
      ],
      "metadata": {
        "id": "Scg7MWoGmkFk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(task + \"_head_to_head_adjList.pkl\", \"wb\") as file:\n",
        "    pickle.dump(head_to_head_adjList, file)\n",
        "files.download(task + \"_head_to_head_adjList.pkl\")\n",
        "\n",
        "with open(task + \"_mlp_to_mlp_adjList.pkl\", \"wb\") as file:\n",
        "    pickle.dump(mlp_to_mlp_adjList, file)\n",
        "files.download(task + \"_mlp_to_mlp_adjList.pkl\")\n",
        "\n",
        "with open(task + \"_head_to_mlp_adjList.pkl\", \"wb\") as file:\n",
        "    pickle.dump(head_to_mlp_adjList, file)\n",
        "files.download(task + \"_head_to_mlp_adjList.pkl\")\n",
        "\n",
        "with open(task + \"_mlp_to_head_adjList.pkl\", \"wb\") as file:\n",
        "    pickle.dump(mlp_to_head_adjList, file)\n",
        "files.download(task + \"_mlp_to_head_adjList.pkl\")\n",
        "\n",
        "with open(task + \"_heads_to_resid.pkl\", \"wb\") as file:\n",
        "    pickle.dump(heads_to_resid, file)\n",
        "files.download(task + \"_heads_to_resid.pkl\")\n",
        "\n",
        "with open(task + \"_mlps_to_resid.pkl\", \"wb\") as file:\n",
        "    pickle.dump(mlps_to_resid, file)\n",
        "files.download(task + \"_mlps_to_resid.pkl\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "210f9170-447e-45ac-8094-eb29caaf7716",
        "id": "NIJrXZOupjVx"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_f9482936-cd6c-49ce-8164-ef8a8e319c61\", \"numerals_head_to_head_adjList.pkl\", 507)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_e5a65a05-2732-400e-87f6-8dc5ecebff58\", \"numerals_mlp_to_mlp_adjList.pkl\", 175)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_f0f829ca-2be2-4477-8ef9-7f96dc4db605\", \"numerals_head_to_mlp_adjList.pkl\", 290)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_c5085f4c-6955-4fcf-b45c-246d8c384b39\", \"numerals_mlp_to_head_adjList.pkl\", 315)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_4c8a4a96-338e-435d-ab0f-7283e904282a\", \"numerals_heads_to_resid.pkl\", 81)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_8b1b89d8-c24a-4c82-90f6-df31171cf984\", \"numerals_mlps_to_resid.pkl\", 49)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(task + \"_heads_to_resid_results.pkl\", \"wb\") as file:\n",
        "    pickle.dump(path_patch_head_to_final_resid_post, file)\n",
        "files.download(task + \"_heads_to_resid_results.pkl\")\n",
        "\n",
        "with open(task + \"_mlps_to_resid_results.pkl\", \"wb\") as file:\n",
        "    pickle.dump(path_patch_mlp_to_final_resid_post, file)\n",
        "files.download(task + \"_mlps_to_resid_results.pkl\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "clO-UbgvpENB",
        "outputId": "bbf3337e-f430-4f52-f679-57522ecf8059"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_8dd7b660-ee2c-4d0b-a1a7-656d1936ad4e\", \"numerals_heads_to_resid_results.pkl\", 980)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_c84807c4-6f00-4c65-a8fc-a7d5c1500ff1\", \"numerals_mlps_to_resid_results.pkl\", 448)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# graph plot"
      ],
      "metadata": {
        "id": "qNC5dz11VcRG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## plot qkv"
      ],
      "metadata": {
        "id": "ST-TgrqgVgqT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# with open(task + \"_head_to_head_adjList.pkl\", \"rb\") as file:\n",
        "#     head_to_head_adjList = pickle.load(file)\n",
        "\n",
        "# with open(task + \"_mlp_to_mlp_adjList.pkl\", \"rb\") as file:\n",
        "#     mlp_to_mlp_adjList = pickle.load(file)\n",
        "\n",
        "# with open(task + \"_head_to_mlp_adjList.pkl\", \"rb\") as file:\n",
        "#     head_to_mlp_adjList = pickle.load(file)\n",
        "\n",
        "# with open(task + \"_mlp_to_head_adjList.pkl\", \"rb\") as file:\n",
        "#     mlp_to_head_adjList = pickle.load(file)\n",
        "\n",
        "# with open(task + \"_heads_to_resid.pkl\", \"rb\") as file:\n",
        "#     heads_to_resid = pickle.load(file)\n",
        "\n",
        "# with open(task + \"_mlps_to_resid.pkl\", \"rb\") as file:\n",
        "#     mlps_to_resid = pickle.load(file)"
      ],
      "metadata": {
        "id": "0jQ5s9Ji5OB_"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from graphviz import Digraph, Source\n",
        "from IPython.display import display\n",
        "from google.colab import files\n",
        "\n",
        "def plot_graph_adjacency_qkv(head_to_head_adjList, mlp_to_mlp_adjList, head_to_mlp_adjList,\n",
        "                             mlp_to_head_adjList, heads_to_resid, mlps_to_resid,\n",
        "                             filename=\"circuit_graph\", highlighted_nodes=None):\n",
        "    dot = Digraph()\n",
        "    dot.attr(ranksep='0.45', nodesep='0.11')  # vert height- ranksep, nodesep- w\n",
        "\n",
        "    dot.node('resid_post', color=\"#ffcccb\", style='filled')\n",
        "\n",
        "    for node in mlp_to_mlp_adjList.keys():\n",
        "        sender_name = \"MLP \" + str(node)\n",
        "        dot.node(sender_name, color=\"#ffcccb\", style='filled')\n",
        "\n",
        "    for node in head_to_head_adjList.keys():\n",
        "        sender_name = f\"{node[0]} , {node[1]} {node[2]}\"\n",
        "        dot.node(sender_name, color=\"#ffcccb\", style='filled')\n",
        "        sender_name = f\"{node[0]} , {node[1]}\"\n",
        "        dot.node(sender_name, color=\"#ffcccb\", style='filled')\n",
        "\n",
        "    edges_added = []\n",
        "    # for every q k v node, plot an edge to output node\n",
        "    for node in head_to_head_adjList.keys():\n",
        "        sender_name = f\"{node[0]} , {node[1]} {node[2]}\"\n",
        "        receiver_name = f\"{node[0]} , {node[1]}\"\n",
        "        dot.edge(sender_name, receiver_name, color = 'red')\n",
        "        edges_added.append((sender_name, receiver_name))\n",
        "\n",
        "    for node in mlp_to_head_adjList.keys():\n",
        "        sender_name = f\"{node[0]} , {node[1]} {node[2]}\"\n",
        "        dot.node(sender_name, color=\"#ffcccb\", style='filled')\n",
        "        sender_name = f\"{node[0]} , {node[1]}\"\n",
        "        dot.node(sender_name, color=\"#ffcccb\", style='filled')\n",
        "\n",
        "    # for every q k v node, plot an edge to output node\n",
        "    for node in mlp_to_head_adjList.keys():\n",
        "        sender_name = f\"{node[0]} , {node[1]} {node[2]}\"\n",
        "        receiver_name = f\"{node[0]} , {node[1]}\"\n",
        "        if (sender_name, receiver_name) not in edges_added:\n",
        "            dot.edge(sender_name, receiver_name, color = 'red')\n",
        "\n",
        "    def loop_adjList(adjList):\n",
        "        for end_node, start_nodes_list in adjList.items():\n",
        "            if isinstance(end_node, int):\n",
        "                receiver_name = \"MLP \" + str(end_node)\n",
        "            elif isinstance(end_node, tuple):\n",
        "                if len(end_node) == 3:\n",
        "                    receiver_name = f\"{end_node[0]} , {end_node[1]} {end_node[2]}\"\n",
        "                elif len(end_node) == 2:\n",
        "                    receiver_name = f\"{end_node[0]} , {end_node[1]}\"\n",
        "            else:\n",
        "                receiver_name = 'resid_post'\n",
        "            for start in start_nodes_list:\n",
        "                if isinstance(start, int):\n",
        "                    sender_name = \"MLP \" + str(start)\n",
        "                elif isinstance(start, tuple):\n",
        "                    if len(start) == 3:\n",
        "                        sender_name = f\"{start[0]} , {start[1]} {start[2]}\"\n",
        "                    elif len(start) == 2:\n",
        "                        sender_name = f\"{start[0]} , {start[1]}\"\n",
        "                dot.node(sender_name, color=\"#ffcccb\", style='filled')\n",
        "                dot.node(receiver_name, color=\"#ffcccb\", style='filled')\n",
        "                dot.edge(sender_name, receiver_name, color = 'red')\n",
        "\n",
        "    loop_adjList(head_to_head_adjList)\n",
        "    loop_adjList(mlp_to_mlp_adjList)\n",
        "    loop_adjList(head_to_mlp_adjList)\n",
        "    loop_adjList(mlp_to_head_adjList)\n",
        "    loop_adjList(heads_to_resid)\n",
        "    loop_adjList(mlps_to_resid)\n",
        "\n",
        "    # Display the graph in Colab\n",
        "    # display(Source(dot.source))\n",
        "\n",
        "    # Save the graph to a file\n",
        "    dot.format = 'png'  # You can change this to 'pdf', 'png', etc. based on your needs\n",
        "    dot.render(filename)\n",
        "    files.download(filename + \".png\")"
      ],
      "metadata": {
        "id": "WNWW7NhPVgqT"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_graph_adjacency_qkv(head_to_head_adjList, mlp_to_mlp_adjList, head_to_mlp_adjList,\n",
        "                         mlp_to_head_adjList, heads_to_resid, mlps_to_resid, filename=\"qkv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "d298efab-db97-4297-9228-cacc24602cf7",
        "id": "aS-TFDTOVgqU"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_801714e4-7a8f-4100-bbea-cdb5577005ba\", \"qkv.png\", 862345)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## rewrite no qkv fn"
      ],
      "metadata": {
        "id": "5zQfhV5QVgqU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_graph_adjacency(head_to_head_adjList, mlp_to_mlp_adjList, head_to_mlp_adjList,\n",
        "                             mlp_to_head_adjList, heads_to_resid, mlps_to_resid,\n",
        "                             filename=\"circuit_graph\", highlighted_nodes=None):\n",
        "    dot = Digraph()\n",
        "    dot.attr(ranksep='0.45', nodesep='0.11')  # vert height- ranksep, nodesep- w\n",
        "\n",
        "    def loop_adjList(adjList):\n",
        "        for end_node, start_nodes_list in adjList.items():\n",
        "            if isinstance(end_node, int):\n",
        "                receiver_name = \"MLP \" + str(end_node)\n",
        "            elif isinstance(end_node, tuple):\n",
        "                receiver_name = f\"{end_node[0]} , {end_node[1]}\"\n",
        "            else:\n",
        "                receiver_name = 'resid_post'\n",
        "            for start in start_nodes_list:\n",
        "                if isinstance(start, int):\n",
        "                    sender_name = \"MLP \" + str(start)\n",
        "                elif isinstance(start, tuple):\n",
        "                    sender_name = f\"{start[0]} , {start[1]}\"\n",
        "                dot.node(sender_name, color=\"#ffcccb\", style='filled')\n",
        "                dot.node(receiver_name, color=\"#ffcccb\", style='filled')\n",
        "                dot.edge(sender_name, receiver_name, color = 'red')\n",
        "\n",
        "    loop_adjList(head_to_head_adjList)\n",
        "    loop_adjList(mlp_to_mlp_adjList)\n",
        "    loop_adjList(head_to_mlp_adjList)\n",
        "    loop_adjList(mlp_to_head_adjList)\n",
        "    loop_adjList(heads_to_resid)\n",
        "    loop_adjList(mlps_to_resid)\n",
        "\n",
        "    # Save the graph to a file\n",
        "    dot.format = 'png'  # You can change this to 'pdf', 'png', etc. based on your needs\n",
        "    dot.render(filename)\n",
        "    files.download(filename + \".png\")"
      ],
      "metadata": {
        "id": "4Cg85tfNVgqU"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_graph_adjacency(head_to_head_adjList, mlp_to_mlp_adjList, head_to_mlp_adjList,\n",
        "                         mlp_to_head_adjList, heads_to_resid, mlps_to_resid, filename=\"no qkv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "39cf91c0-2a91-4cd6-cf9e-1968280815ed",
        "id": "jlB5soNZVgqU"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_8f0b80e7-3d1b-4b52-ae4d-9d767fae5afe\", \"no qkv.png\", 639433)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}