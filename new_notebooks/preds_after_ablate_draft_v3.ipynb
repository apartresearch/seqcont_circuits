{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DcZG9rm2IAiA"
      },
      "source": [
        "# Setup\n",
        "(No need to change anything)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rMcpSDdjIAiA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed876942-d2aa-4bba-812b-f80f95a85add"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running as a Colab notebook\n",
            "Collecting git+https://github.com/neelnanda-io/TransformerLens.git\n",
            "  Cloning https://github.com/neelnanda-io/TransformerLens.git to /tmp/pip-req-build-0o5jhvht\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/neelnanda-io/TransformerLens.git /tmp/pip-req-build-0o5jhvht\n",
            "  Resolved https://github.com/neelnanda-io/TransformerLens.git to commit ce82675a8e89b6d5e6229a89620c843c794f3b04\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting accelerate>=0.23.0 (from transformer-lens==0.0.0)\n",
            "  Downloading accelerate-0.25.0-py3-none-any.whl (265 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m265.7/265.7 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting beartype<0.15.0,>=0.14.1 (from transformer-lens==0.0.0)\n",
            "  Downloading beartype-0.14.1-py3-none-any.whl (739 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m739.7/739.7 kB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting datasets>=2.7.1 (from transformer-lens==0.0.0)\n",
            "  Downloading datasets-2.15.0-py3-none-any.whl (521 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m521.2/521.2 kB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting einops>=0.6.0 (from transformer-lens==0.0.0)\n",
            "  Downloading einops-0.7.0-py3-none-any.whl (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fancy-einsum>=0.0.3 (from transformer-lens==0.0.0)\n",
            "  Downloading fancy_einsum-0.0.3-py3-none-any.whl (6.2 kB)\n",
            "Collecting jaxtyping>=0.2.11 (from transformer-lens==0.0.0)\n",
            "  Downloading jaxtyping-0.2.24-py3-none-any.whl (38 kB)\n",
            "Collecting numpy>=1.24 (from transformer-lens==0.0.0)\n",
            "  Downloading numpy-1.26.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m42.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.10/dist-packages (from transformer-lens==0.0.0) (1.5.3)\n",
            "Requirement already satisfied: rich>=12.6.0 in /usr/local/lib/python3.10/dist-packages (from transformer-lens==0.0.0) (13.7.0)\n",
            "Collecting torch!=2.0,!=2.1.0,>=1.10 (from transformer-lens==0.0.0)\n",
            "  Downloading torch-2.1.1-cp310-cp310-manylinux1_x86_64.whl (670.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m670.2/670.2 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.10/dist-packages (from transformer-lens==0.0.0) (4.66.1)\n",
            "Requirement already satisfied: transformers>=4.25.1 in /usr/local/lib/python3.10/dist-packages (from transformer-lens==0.0.0) (4.35.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from transformer-lens==0.0.0) (4.5.0)\n",
            "Collecting wandb>=0.13.5 (from transformer-lens==0.0.0)\n",
            "  Downloading wandb-0.16.1-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m92.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.23.0->transformer-lens==0.0.0) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.23.0->transformer-lens==0.0.0) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.23.0->transformer-lens==0.0.0) (6.0.1)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.23.0->transformer-lens==0.0.0) (0.19.4)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.23.0->transformer-lens==0.0.0) (0.4.1)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer-lens==0.0.0) (9.0.0)\n",
            "Collecting pyarrow-hotfix (from datasets>=2.7.1->transformer-lens==0.0.0)\n",
            "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets>=2.7.1->transformer-lens==0.0.0)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer-lens==0.0.0) (2.31.0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer-lens==0.0.0) (3.4.1)\n",
            "Collecting multiprocess (from datasets>=2.7.1->transformer-lens==0.0.0)\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer-lens==0.0.0) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer-lens==0.0.0) (3.9.1)\n",
            "Collecting typeguard<3,>=2.13.3 (from jaxtyping>=0.2.11->transformer-lens==0.0.0)\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->transformer-lens==0.0.0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->transformer-lens==0.0.0) (2023.3.post1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=12.6.0->transformer-lens==0.0.0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=12.6.0->transformer-lens==0.0.0) (2.16.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch!=2.0,!=2.1.0,>=1.10->transformer-lens==0.0.0) (3.13.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch!=2.0,!=2.1.0,>=1.10->transformer-lens==0.0.0) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch!=2.0,!=2.1.0,>=1.10->transformer-lens==0.0.0) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch!=2.0,!=2.1.0,>=1.10->transformer-lens==0.0.0) (3.1.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch!=2.0,!=2.1.0,>=1.10->transformer-lens==0.0.0)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m70.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch!=2.0,!=2.1.0,>=1.10->transformer-lens==0.0.0)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m76.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch!=2.0,!=2.1.0,>=1.10->transformer-lens==0.0.0)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m99.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch!=2.0,!=2.1.0,>=1.10->transformer-lens==0.0.0)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch!=2.0,!=2.1.0,>=1.10->transformer-lens==0.0.0)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch!=2.0,!=2.1.0,>=1.10->transformer-lens==0.0.0)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch!=2.0,!=2.1.0,>=1.10->transformer-lens==0.0.0)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch!=2.0,!=2.1.0,>=1.10->transformer-lens==0.0.0)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch!=2.0,!=2.1.0,>=1.10->transformer-lens==0.0.0)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.18.1 (from torch!=2.0,!=2.1.0,>=1.10->transformer-lens==0.0.0)\n",
            "  Downloading nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl (209.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.8/209.8 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch!=2.0,!=2.1.0,>=1.10->transformer-lens==0.0.0)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch!=2.0,!=2.1.0,>=1.10->transformer-lens==0.0.0) (2.1.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch!=2.0,!=2.1.0,>=1.10->transformer-lens==0.0.0)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.3.101-py3-none-manylinux1_x86_64.whl (20.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.5/20.5 MB\u001b[0m \u001b[31m96.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.1->transformer-lens==0.0.0) (2023.6.3)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.1->transformer-lens==0.0.0) (0.15.0)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer-lens==0.0.0) (8.1.7)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb>=0.13.5->transformer-lens==0.0.0)\n",
            "  Downloading GitPython-3.1.40-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.6/190.6 kB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentry-sdk>=1.0.0 (from wandb>=0.13.5->transformer-lens==0.0.0)\n",
            "  Downloading sentry_sdk-1.38.0-py2.py3-none-any.whl (252 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m252.8/252.8 kB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb>=0.13.5->transformer-lens==0.0.0)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting setproctitle (from wandb>=0.13.5->transformer-lens==0.0.0)\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer-lens==0.0.0) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer-lens==0.0.0) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer-lens==0.0.0) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb>=0.13.5->transformer-lens==0.0.0) (1.16.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer-lens==0.0.0) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer-lens==0.0.0) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer-lens==0.0.0) (1.9.3)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer-lens==0.0.0) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer-lens==0.0.0) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer-lens==0.0.0) (4.0.3)\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer-lens==0.0.0)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=12.6.0->transformer-lens==0.0.0) (0.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=2.7.1->transformer-lens==0.0.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=2.7.1->transformer-lens==0.0.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=2.7.1->transformer-lens==0.0.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=2.7.1->transformer-lens==0.0.0) (2023.11.17)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch!=2.0,!=2.1.0,>=1.10->transformer-lens==0.0.0) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch!=2.0,!=2.1.0,>=1.10->transformer-lens==0.0.0) (1.3.0)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer-lens==0.0.0)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Building wheels for collected packages: transformer-lens\n",
            "  Building wheel for transformer-lens (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformer-lens: filename=transformer_lens-0.0.0-py3-none-any.whl size=118964 sha256=a41697fddfb7a0f1f60d43d2eec2ff20c9aa08409a7870e3d636f9e7617edcf1\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-pf_r0dtc/wheels/8a/1e/37/ffb9c15454a1725b13a9d9f5e74fb91725048884ad734b8c1f\n",
            "Successfully built transformer-lens\n",
            "Installing collected packages: typeguard, smmap, setproctitle, sentry-sdk, pyarrow-hotfix, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, fancy-einsum, einops, docker-pycreds, dill, beartype, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, jaxtyping, gitdb, nvidia-cusolver-cu12, GitPython, wandb, torch, datasets, accelerate, transformer-lens\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.23.5\n",
            "    Uninstalling numpy-1.23.5:\n",
            "      Successfully uninstalled numpy-1.23.5\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.1.0+cu118\n",
            "    Uninstalling torch-2.1.0+cu118:\n",
            "      Successfully uninstalled torch-2.1.0+cu118\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires fastapi, which is not installed.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "lida 0.0.10 requires uvicorn, which is not installed.\n",
            "cupy-cuda11x 11.0.0 requires numpy<1.26,>=1.20, but you have numpy 1.26.2 which is incompatible.\n",
            "torchaudio 2.1.0+cu118 requires torch==2.1.0, but you have torch 2.1.1 which is incompatible.\n",
            "torchdata 0.7.0 requires torch==2.1.0, but you have torch 2.1.1 which is incompatible.\n",
            "torchtext 0.16.0 requires torch==2.1.0, but you have torch 2.1.1 which is incompatible.\n",
            "torchvision 0.16.0+cu118 requires torch==2.1.0, but you have torch 2.1.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed GitPython-3.1.40 accelerate-0.25.0 beartype-0.14.1 datasets-2.15.0 dill-0.3.7 docker-pycreds-0.4.0 einops-0.7.0 fancy-einsum-0.0.3 gitdb-4.0.11 jaxtyping-0.2.24 multiprocess-0.70.15 numpy-1.26.2 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.18.1 nvidia-nvjitlink-cu12-12.3.101 nvidia-nvtx-cu12-12.1.105 pyarrow-hotfix-0.6 sentry-sdk-1.38.0 setproctitle-1.3.3 smmap-5.0.1 torch-2.1.1 transformer-lens-0.0.0 typeguard-2.13.3 wandb-0.16.1\n"
          ]
        }
      ],
      "source": [
        "# Janky code to do different setup when run in a Colab notebook vs VSCode\n",
        "DEBUG_MODE = False\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "    print(\"Running as a Colab notebook\")\n",
        "    %pip install git+https://github.com/neelnanda-io/TransformerLens.git\n",
        "    # Install another version of node that makes PySvelte work way faster\n",
        "    # !curl -fsSL https://deb.nodesource.com/setup_16.x | sudo -E bash -; sudo apt-get install -y nodejs\n",
        "    # %pip install git+https://github.com/neelnanda-io/PySvelte.git\n",
        "except:\n",
        "    IN_COLAB = False\n",
        "    print(\"Running as a Jupyter notebook - intended for development only!\")\n",
        "    from IPython import get_ipython\n",
        "\n",
        "    ipython = get_ipython()\n",
        "    # Code to automatically update the HookedTransformer code as its edited without restarting the kernel\n",
        "    ipython.magic(\"load_ext autoreload\")\n",
        "    ipython.magic(\"autoreload 2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Z6b1n2tvIAiD"
      },
      "outputs": [],
      "source": [
        "# Import stuff\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import einops\n",
        "from fancy_einsum import einsum\n",
        "import tqdm.notebook as tqdm\n",
        "import random\n",
        "from pathlib import Path\n",
        "import plotly.express as px\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from jaxtyping import Float, Int\n",
        "from typing import List, Union, Optional\n",
        "from functools import partial\n",
        "import copy\n",
        "\n",
        "import itertools\n",
        "from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer\n",
        "import dataclasses\n",
        "import datasets\n",
        "from IPython.display import HTML"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "zuhzYxbsIAiE"
      },
      "outputs": [],
      "source": [
        "import transformer_lens\n",
        "import transformer_lens.utils as utils\n",
        "from transformer_lens.hook_points import (\n",
        "    HookedRootModule,\n",
        "    HookPoint,\n",
        ")  # Hooking utilities\n",
        "from transformer_lens import HookedTransformer, HookedTransformerConfig, FactoredMatrix, ActivationCache"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hccba0v-IAiF"
      },
      "source": [
        "We turn automatic differentiation off, to save GPU memory, as this notebook focuses on model inference not model training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "cFMTUcQiIAiF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e98c342-a6c4-43ec-d5ab-b4e4e4909c87"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.autograd.grad_mode.set_grad_enabled at 0x7e462e5e5b10>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "torch.set_grad_enabled(False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Model"
      ],
      "metadata": {
        "id": "OLkInsdjyHMx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "xLwDyosvIAiJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8069f03-3c5c-4bae-99b6-1b42084f059b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded pretrained model gpt2-small into HookedTransformer\n"
          ]
        }
      ],
      "source": [
        "model = HookedTransformer.from_pretrained(\n",
        "    \"gpt2-small\",\n",
        "    center_unembed=True,\n",
        "    center_writing_weights=True,\n",
        "    fold_ln=True,\n",
        "    refactor_factored_attn_matrices=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import functions from repo"
      ],
      "metadata": {
        "id": "Z4iJEGh6b56v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/callummcdougall/ARENA_2.0.git"
      ],
      "metadata": {
        "id": "Fdh5--MfYw7-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7425dd3-af68-4399-8e73-086418c53a0a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ARENA_2.0'...\n",
            "remote: Enumerating objects: 9262, done.\u001b[K\n",
            "remote: Counting objects: 100% (1974/1974), done.\u001b[K\n",
            "remote: Compressing objects: 100% (343/343), done.\u001b[K\n",
            "remote: Total 9262 (delta 1725), reused 1731 (delta 1627), pack-reused 7288\u001b[K\n",
            "Receiving objects: 100% (9262/9262), 156.79 MiB | 14.59 MiB/s, done.\n",
            "Resolving deltas: 100% (5620/5620), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd ARENA_2.0/chapter1_transformers/exercises/part3_indirect_object_identification"
      ],
      "metadata": {
        "id": "fhmqg_vsng4S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a173ce63-c829-44c4-9d0c-51e988bb8592"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/ARENA_2.0/chapter1_transformers/exercises/part3_indirect_object_identification\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import ioi_circuit_extraction as ioi_circuit_extraction"
      ],
      "metadata": {
        "id": "OT0Sn571ZnkV"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ioi_dataset import IOIDataset"
      ],
      "metadata": {
        "id": "QfLqx5QkO5hV"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate dataset with multiple prompts"
      ],
      "metadata": {
        "id": "6Fuq8XW770vX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Dataset:\n",
        "    def __init__(self, prompts, pos_dict, tokenizer, S1_is_first=False):\n",
        "        self.prompts = prompts\n",
        "        self.tokenizer = tokenizer\n",
        "        self.N = len(prompts)\n",
        "        self.max_len = max(\n",
        "            [\n",
        "                len(self.tokenizer(prompt[\"text\"]).input_ids)\n",
        "                for prompt in self.prompts\n",
        "            ]\n",
        "        )\n",
        "        # all_ids = [prompt[\"TEMPLATE_IDX\"] for prompt in self.ioi_prompts]\n",
        "        all_ids = [0 for prompt in self.prompts] # only 1 template\n",
        "        all_ids_ar = np.array(all_ids)\n",
        "        self.groups = []\n",
        "        for id in list(set(all_ids)):\n",
        "            self.groups.append(np.where(all_ids_ar == id)[0])\n",
        "\n",
        "        texts = [ prompt[\"text\"] for prompt in self.prompts ]\n",
        "        self.toks = torch.Tensor(self.tokenizer(texts, padding=True).input_ids).type(\n",
        "            torch.int\n",
        "        )\n",
        "        self.corr_tokenIDs = [\n",
        "            self.tokenizer.encode(\" \" + prompt[\"corr\"])[0] for prompt in self.prompts\n",
        "        ]\n",
        "        self.incorr_tokenIDs = [\n",
        "            self.tokenizer.encode(\" \" + prompt[\"incorr\"])[0] for prompt in self.prompts\n",
        "        ]\n",
        "\n",
        "        # word_idx: for every prompt, find the token index of each target token and \"end\"\n",
        "        # word_idx is a tensor with an element for each prompt. The element is the targ token's ind at that prompt\n",
        "        self.word_idx = {}\n",
        "        for targ in [key for key in self.prompts[0].keys() if (key != 'text' and key != 'corr' and key != 'incorr')]:\n",
        "            targ_lst = []\n",
        "            for prompt in self.prompts:\n",
        "                input_text = prompt[\"text\"]\n",
        "                tokens = model.tokenizer.tokenize(input_text)\n",
        "                # if S1_is_first and targ == \"S1\":  # only use this if first token doesn't have space Ġ in front\n",
        "                #     target_token = prompt[targ]\n",
        "                # else:\n",
        "                #     target_token = \"Ġ\" + prompt[targ]\n",
        "                # target_index = tokens.index(target_token)\n",
        "                target_index = pos_dict[targ]\n",
        "                targ_lst.append(target_index)\n",
        "            self.word_idx[targ] = torch.tensor(targ_lst)\n",
        "\n",
        "        targ_lst = []\n",
        "        for prompt in self.prompts:\n",
        "            input_text = prompt[\"text\"]\n",
        "            tokens = self.tokenizer.tokenize(input_text)\n",
        "            end_token_index = len(tokens) - 1\n",
        "            targ_lst.append(end_token_index)\n",
        "        self.word_idx[\"end\"] = torch.tensor(targ_lst)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.N"
      ],
      "metadata": {
        "id": "4wXBNWj5FwVn"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos_dict = {\n",
        "    'S1': 0,\n",
        "    'S2': 1,\n",
        "    'S3': 2,\n",
        "    'S4': 3,\n",
        "}"
      ],
      "metadata": {
        "id": "kS_Tlrb_70vg"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_prompts_list(x ,y):\n",
        "    prompts_list = []\n",
        "    for i in range(x, y):\n",
        "        prompt_dict = {\n",
        "            'S1': str(i),\n",
        "            'S2': str(i+1),\n",
        "            'S3': str(i+2),\n",
        "            'S4': str(i+3),\n",
        "            'corr': str(i+4),\n",
        "            'incorr': str(i+3),\n",
        "            'text': f\"{i} {i+1} {i+2} {i+3}\"\n",
        "        }\n",
        "        prompts_list.append(prompt_dict)\n",
        "    return prompts_list\n",
        "\n",
        "prompts_list = generate_prompts_list(1, 101)\n",
        "dataset = Dataset(prompts_list, pos_dict, model.tokenizer, S1_is_first=True)"
      ],
      "metadata": {
        "id": "u0NPSKcZ1iDe"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def generate_prompts_list_corr(x ,y):\n",
        "    prompts_list = []\n",
        "    for i in range(x, y):\n",
        "        r1 = random.randint(1, 100)\n",
        "        r2 = random.randint(1, 100)\n",
        "        while True:\n",
        "            r3 = random.randint(1, 100)\n",
        "            r4 = random.randint(1, 100)\n",
        "            if r4 - 1 != r3:\n",
        "                break\n",
        "        prompt_dict = {\n",
        "            'S1': str(r1),\n",
        "            'S2': str(r2),\n",
        "            'S3': str(r3),\n",
        "            'S4': str(r4),\n",
        "            'corr': str(i+4),\n",
        "            'incorr': str(i+3),\n",
        "            'text': f\"{r1} {r2} {r3} {r4}\"\n",
        "        }\n",
        "        prompts_list.append(prompt_dict)\n",
        "    return prompts_list\n",
        "\n",
        "prompts_list_2 = generate_prompts_list_corr(1, 101)\n",
        "# prompts_list_2 = generate_prompts_list_corr(1, 2)\n",
        "dataset_2 = Dataset(prompts_list_2, pos_dict, model.tokenizer, S1_is_first=True)"
      ],
      "metadata": {
        "id": "dzzLlCqZS_wl"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_2 = Dataset(prompts_list_2, pos_dict, model.tokenizer, S1_is_first=True)"
      ],
      "metadata": {
        "id": "msu6D4p_feW5"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get preds fns"
      ],
      "metadata": {
        "id": "Q74PH3CiSgw8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_batch_dim(\n",
        "    tensor: Float[torch.Tensor, \"1 ...\"]\n",
        ") -> Float[torch.Tensor, \"...\"]:\n",
        "    \"\"\"\n",
        "    Removes the first dimension of a tensor if it is size 1, otherwise returns the tensor unchanged\n",
        "    \"\"\"\n",
        "    if tensor.shape[0] == 1:\n",
        "        return tensor.squeeze(0)\n",
        "    else:\n",
        "        return tensor"
      ],
      "metadata": {
        "id": "-wK53DqEvBxf"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def test_prompt(\n",
        "#     prompt: str,\n",
        "#     answer: str,\n",
        "#     model,\n",
        "#     orig_model,\n",
        "#     prepend_space_to_answer: bool = True,\n",
        "#     print_details: bool = True,\n",
        "#     prepend_bos: bool = True,\n",
        "#     top_k: int = 10,\n",
        "# ):\n",
        "#     \"\"\"\n",
        "#     Function to test whether a model can give the correct answer to a prompt. Intended for exploratory analysis, so it prints things out rather than returning things.\n",
        "\n",
        "#     Works for multi-token answers and multi-token prompts.\n",
        "\n",
        "#     Will always print the ranks of the answer tokens, and if print_details will print the logit and prob for the answer tokens and the top k tokens returned for each answer position.\n",
        "#     \"\"\"\n",
        "#     if prepend_space_to_answer and not answer.startswith(\" \"):\n",
        "#         answer = \" \" + answer\n",
        "#     # GPT-2 often treats the first token weirdly, so lets give it a resting position\n",
        "#     tokens = orig_model.to_tokens(prompt + answer, prepend_bos=prepend_bos)\n",
        "\n",
        "\n",
        "#     prompt_str_tokens = orig_model.to_str_tokens(prompt, prepend_bos=prepend_bos)\n",
        "#     answer_str_tokens = orig_model.to_str_tokens(answer, prepend_bos=False)\n",
        "#     prompt_length = len(prompt_str_tokens)\n",
        "#     answer_length = len(answer_str_tokens)\n",
        "#     if print_details:\n",
        "#         print(\"Tokenized prompt:\", prompt_str_tokens)\n",
        "#         print(\"Tokenized answer:\", answer_str_tokens)\n",
        "\n",
        "#     logits = remove_batch_dim(model(tokens))\n",
        "\n",
        "#     probs = logits.softmax(dim=-1)\n",
        "#     answer_ranks = []\n",
        "#     for index in range(prompt_length, prompt_length + answer_length):\n",
        "#         answer_token = tokens[0, index]\n",
        "#         answer_str_token = answer_str_tokens[index - prompt_length]\n",
        "#         # Offset by 1 because models predict the NEXT token\n",
        "#         token_probs = probs[index - 1]\n",
        "#         sorted_token_probs, sorted_token_values = token_probs.sort(descending=True) # sorted_token_values are the indices; the indices correspond to token num of vocab\n",
        "#         # Janky way to get the index of the token in the sorted list - I couldn't find a better way?\n",
        "#         correct_rank = torch.arange(len(sorted_token_values))[\n",
        "#             (sorted_token_values == answer_token).cpu()\n",
        "#         ].item()\n",
        "#         answer_ranks.append((answer_str_token, correct_rank))\n",
        "#         if print_details:\n",
        "#             # String formatting syntax - the first number gives the number of characters to pad to, the second number gives the number of decimal places.\n",
        "#             # rprint gives rich text printing\n",
        "#             print(\n",
        "#                 f\"Performance on answer token:\\n[b]Rank: {correct_rank: <8} Logit: {logits[index-1, answer_token].item():5.2f} Prob: {token_probs[answer_token].item():6.2%} Token: |{answer_str_token}|[/b]\"\n",
        "#             )\n",
        "#             for i in range(top_k):\n",
        "#                 print(\n",
        "#                     f\"Top {i}th token. Logit: {logits[index-1, sorted_token_values[i]].item():5.2f} Prob: {sorted_token_probs[i].item():6.2%} Token: |{orig_model.to_string(sorted_token_values[i])}|\"\n",
        "#                 )\n",
        "#     print(f\"[b]Ranks of the answer tokens:[/b] {answer_ranks}\")"
      ],
      "metadata": {
        "id": "Hba_G6MBShaH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def test_prompt(\n",
        "#     prompt: str,\n",
        "#     answer: str,\n",
        "#     model,  # Can't give type hint due to circular imports\n",
        "#     prepend_space_to_answer: Optional[bool] = True,\n",
        "#     print_details: Optional[bool] = True,\n",
        "#     prepend_bos: Optional[bool] = USE_DEFAULT_VALUE,\n",
        "#     top_k: Optional[int] = 10,\n",
        "# ) -> None:\n",
        "#     \"\"\"Test if the Model Can Give the Correct Answer to a Prompt.\n",
        "\n",
        "#     Intended for exploratory analysis. Prints out the performance on the answer (rank, logit, prob),\n",
        "#     as well as the top k tokens. Works for multi-token prompts and multi-token answers.\n",
        "\n",
        "#     Warning:\n",
        "\n",
        "#     This will print the results (it does not return them).\n",
        "\n",
        "#     Examples:\n",
        "\n",
        "#     >>> from transformer_lens import HookedTransformer, utils\n",
        "#     >>> model = HookedTransformer.from_pretrained(\"tiny-stories-1M\")\n",
        "#     Loaded pretrained model tiny-stories-1M into HookedTransformer\n",
        "\n",
        "#     >>> prompt = \"Why did the elephant cross the\"\n",
        "#     >>> answer = \"road\"\n",
        "#     >>> utils.test_prompt(prompt, answer, model)\n",
        "#     Tokenized prompt: ['<|endoftext|>', 'Why', ' did', ' the', ' elephant', ' cross', ' the']\n",
        "#     Tokenized answer: [' road']\n",
        "#     Performance on answer token:\n",
        "#     Rank: 2        Logit: 14.24 Prob:  3.51% Token: | road|\n",
        "#     Top 0th token. Logit: 14.51 Prob:  4.59% Token: | ground|\n",
        "#     Top 1th token. Logit: 14.41 Prob:  4.18% Token: | tree|\n",
        "#     Top 2th token. Logit: 14.24 Prob:  3.51% Token: | road|\n",
        "#     Top 3th token. Logit: 14.22 Prob:  3.45% Token: | car|\n",
        "#     Top 4th token. Logit: 13.92 Prob:  2.55% Token: | river|\n",
        "#     Top 5th token. Logit: 13.79 Prob:  2.25% Token: | street|\n",
        "#     Top 6th token. Logit: 13.77 Prob:  2.21% Token: | k|\n",
        "#     Top 7th token. Logit: 13.75 Prob:  2.16% Token: | hill|\n",
        "#     Top 8th token. Logit: 13.64 Prob:  1.92% Token: | swing|\n",
        "#     Top 9th token. Logit: 13.46 Prob:  1.61% Token: | park|\n",
        "#     Ranks of the answer tokens: [(' road', 2)]\n",
        "\n",
        "#     Args:\n",
        "#         prompt:\n",
        "#             The prompt string, e.g. \"Why did the elephant cross the\".\n",
        "#         answer:\n",
        "#             The answer, e.g. \"road\". Note that if you set prepend_space_to_answer to False, you need\n",
        "#             to think about if you have a space before the answer here (as e.g. in this example the\n",
        "#             answer may really be \" road\" if the prompt ends without a trailing space).\n",
        "#         model:\n",
        "#             The model.\n",
        "#         prepend_space_to_answer:\n",
        "#             Whether or not to prepend a space to the answer. Note this will only ever prepend a\n",
        "#             space if the answer doesn't already start with one.\n",
        "#         print_details:\n",
        "#             Print the prompt (as a string but broken up by token), answer and top k tokens (all\n",
        "#             with logit, rank and probability).\n",
        "#         prepend_bos:\n",
        "#             Overrides self.cfg.default_prepend_bos if set. Whether to prepend\n",
        "#             the BOS token to the input (applicable when input is a string). Models generally learn\n",
        "#             to use the BOS token as a resting place for attention heads (i.e. a way for them to be\n",
        "#             \"turned off\"). This therefore often improves performance slightly.\n",
        "#         top_k:\n",
        "#             Top k tokens to print details of (when print_details is set to True).\n",
        "\n",
        "#     Returns:\n",
        "#         None (just prints the results directly).\n",
        "#     \"\"\"\n",
        "#     if prepend_space_to_answer and not answer.startswith(\" \"):\n",
        "#         answer = \" \" + answer\n",
        "#     # GPT-2 often treats the first token weirdly, so lets give it a resting position\n",
        "#     prompt_tokens = model.to_tokens(prompt, prepend_bos=prepend_bos)\n",
        "#     answer_tokens = model.to_tokens(answer, prepend_bos=False)\n",
        "#     tokens = torch.cat((prompt_tokens, answer_tokens), dim=1)\n",
        "#     prompt_str_tokens = model.to_str_tokens(prompt, prepend_bos=prepend_bos)\n",
        "#     answer_str_tokens = model.to_str_tokens(answer, prepend_bos=False)\n",
        "#     prompt_length = len(prompt_str_tokens)\n",
        "#     answer_length = len(answer_str_tokens)\n",
        "#     if print_details:\n",
        "#         print(\"Tokenized prompt:\", prompt_str_tokens)\n",
        "#         print(\"Tokenized answer:\", answer_str_tokens)\n",
        "#     logits = remove_batch_dim(model(tokens))\n",
        "#     probs = logits.softmax(dim=-1)\n",
        "#     answer_ranks = []\n",
        "#     for index in range(prompt_length, prompt_length + answer_length):\n",
        "#         answer_token = tokens[0, index]\n",
        "#         answer_str_token = answer_str_tokens[index - prompt_length]\n",
        "#         # Offset by 1 because models predict the NEXT token\n",
        "#         token_probs = probs[index - 1]\n",
        "#         sorted_token_probs, sorted_token_values = token_probs.sort(descending=True)\n",
        "#         # Janky way to get the index of the token in the sorted list - I couldn't find a better way?\n",
        "#         correct_rank = torch.arange(len(sorted_token_values))[\n",
        "#             (sorted_token_values == answer_token).cpu()\n",
        "#         ].item()\n",
        "#         answer_ranks.append((answer_str_token, correct_rank))\n",
        "#         if print_details:\n",
        "#             # String formatting syntax - the first number gives the number of characters to pad to, the second number gives the number of decimal places.\n",
        "#             # rprint gives rich text printing\n",
        "#             rprint(\n",
        "#                 f\"Performance on answer token:\\n[b]Rank: {correct_rank: <8} Logit: {logits[index-1, answer_token].item():5.2f} Prob: {token_probs[answer_token].item():6.2%} Token: |{answer_str_token}|[/b]\"\n",
        "#             )\n",
        "#             for i in range(top_k):\n",
        "#                 print(\n",
        "#                     f\"Top {i}th token. Logit: {logits[index-1, sorted_token_values[i]].item():5.2f} Prob: {sorted_token_probs[i].item():6.2%} Token: |{model.to_string(sorted_token_values[i])}|\"\n",
        "#                 )\n",
        "#     rprint(f\"[b]Ranks of the answer tokens:[/b] {answer_ranks}\")"
      ],
      "metadata": {
        "id": "6OfqM2mkaT5A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test_prompt(\n",
        "#     prompt = \"1 2 3 4\",\n",
        "#     answer: \" 5\",\n",
        "#     model,\n",
        "#     prepend_space_to_answer= True,\n",
        "#     print_details = True,\n",
        "#     prepend_bos = True,\n",
        "#     top_k = 10,\n",
        "# )"
      ],
      "metadata": {
        "id": "htAmfQQQaWlG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt = \"1 2 3 4\"\n",
        "# answer = \" 5\"\n",
        "\n",
        "# prompt_str_tokens = model.to_str_tokens(prompt, prepend_bos=True)\n",
        "# answer_str_tokens = model.to_str_tokens(answer, prepend_bos=False)\n",
        "# prompt_length = len(prompt_str_tokens)\n",
        "# answer_length = len(answer_str_tokens)\n",
        "\n",
        "# print(\"Tokenized prompt:\", prompt_str_tokens)\n",
        "# print(\"Tokenized answer:\", answer_str_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8SdB_gJ2YVjc",
        "outputId": "4d8a0003-6e97-4e52-81e9-03683eed8597"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized prompt: ['<|endoftext|>', '1', ' 2', ' 3', ' 4']\n",
            "Tokenized answer: [' 5']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MLP ablation fns"
      ],
      "metadata": {
        "id": "G9FQY3H3zkFV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import Tensor\n",
        "\n",
        "def logits_to_ave_logit_diff(logits: Float[Tensor, \"batch seq d_vocab\"], dataset: Dataset, per_prompt=False):\n",
        "    '''\n",
        "    Returns logit difference between the correct and incorrect answer.\n",
        "\n",
        "    If per_prompt=True, return the array of differences rather than the average.\n",
        "    '''\n",
        "\n",
        "    # Only the final logits are relevant for the answer\n",
        "    # Get the logits corresponding to the indirect object / subject tokens respectively\n",
        "    corr_logits: Float[Tensor, \"batch\"] = logits[range(logits.size(0)), dataset.word_idx[\"end\"], dataset.corr_tokenIDs]\n",
        "    incorr_logits: Float[Tensor, \"batch\"] = logits[range(logits.size(0)), dataset.word_idx[\"end\"], dataset.incorr_tokenIDs]\n",
        "    # Find logit difference\n",
        "    answer_logit_diff = corr_logits - incorr_logits\n",
        "    return answer_logit_diff if per_prompt else answer_logit_diff.mean()"
      ],
      "metadata": {
        "id": "ZOTRN8KnheFO"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Dict, Tuple, List\n",
        "from jaxtyping import Float, Bool\n",
        "import torch as t\n",
        "\n",
        "# lst = [(layer, head) for layer in range(12) for head in range(12)]\n",
        "lst = [layer for layer in range(12)]\n",
        "CIRCUIT = {\n",
        "    \"number mover\": lst,\n",
        "    # \"number mover 4\": lst,\n",
        "    \"number mover 3\": lst,\n",
        "    \"number mover 2\": lst,\n",
        "    \"number mover 1\": lst,\n",
        "}\n",
        "\n",
        "SEQ_POS_TO_KEEP = {\n",
        "    \"number mover\": \"end\",\n",
        "    # \"number mover 4\": \"S4\",\n",
        "    \"number mover 3\": \"S3\",\n",
        "    \"number mover 2\": \"S2\",\n",
        "    \"number mover 1\": \"S1\",\n",
        "}\n",
        "\n",
        "def logits_to_ave_logit_diff(logits: Float[Tensor, \"batch seq d_vocab\"], dataset: Dataset, per_prompt=False):\n",
        "    '''\n",
        "    Returns logit difference between the correct and incorrect answer.\n",
        "\n",
        "    If per_prompt=True, return the array of differences rather than the average.\n",
        "    '''\n",
        "\n",
        "    # Only the final logits are relevant for the answer\n",
        "    # Get the logits corresponding to the indirect object / subject tokens respectively\n",
        "    corr_logits: Float[Tensor, \"batch\"] = logits[range(logits.size(0)), dataset.word_idx[\"end\"], dataset.corr_tokenIDs]\n",
        "    incorr_logits: Float[Tensor, \"batch\"] = logits[range(logits.size(0)), dataset.word_idx[\"end\"], dataset.incorr_tokenIDs]\n",
        "    # Find logit difference\n",
        "    answer_logit_diff = corr_logits - incorr_logits\n",
        "    return answer_logit_diff if per_prompt else answer_logit_diff.mean()"
      ],
      "metadata": {
        "id": "tIZoAvLqz1Sd"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_means_by_template_MLP(\n",
        "    means_dataset: Dataset,\n",
        "    model: HookedTransformer\n",
        ") -> Float[Tensor, \"layer batch seq head_idx d_head\"]:\n",
        "    '''\n",
        "    Returns the mean of each head's output over the means dataset. This mean is\n",
        "    computed separately for each group of prompts with the same template (these\n",
        "    are given by means_dataset.groups).\n",
        "    '''\n",
        "    # Cache the outputs of every head\n",
        "    _, means_cache = model.run_with_cache(\n",
        "        means_dataset.toks.long(),\n",
        "        return_type=None,\n",
        "        # names_filter=lambda name: name.endswith(\"z\"),\n",
        "    )\n",
        "    # Create tensor to store means\n",
        "    n_layers, d_model = model.cfg.n_layers, model.cfg.d_model\n",
        "    batch, seq_len = len(means_dataset), means_dataset.max_len\n",
        "    means = t.zeros(size=(n_layers, batch, seq_len, d_model), device=model.cfg.device)\n",
        "\n",
        "    # Get set of different templates for this data\n",
        "    for layer in range(n_layers):\n",
        "        mlp_output_for_this_layer: Float[Tensor, \"batch seq d_model\"] = means_cache[utils.get_act_name(\"mlp_out\", layer)]\n",
        "        for template_group in means_dataset.groups:  # here, we only have one group\n",
        "            mlp_output_for_this_template = mlp_output_for_this_layer[template_group]\n",
        "            # aggregate all batches\n",
        "            mlp_output_means_for_this_template = einops.reduce(mlp_output_for_this_template, \"batch seq d_model -> seq d_model\", \"mean\")\n",
        "            means[layer, template_group] = mlp_output_means_for_this_template\n",
        "            # at layer, each batch ind is tempalte group (a tensor of size seq d_model)\n",
        "            # is assigned the SAME mean, \"mlp_output_means_for_this_template\"\n",
        "\n",
        "    return means"
      ],
      "metadata": {
        "id": "McmRZoY7Wudl"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_mlp_outputs_and_posns_to_keep(\n",
        "    means_dataset: Dataset,\n",
        "    model: HookedTransformer,\n",
        "    circuit: Dict[str, List[int]],  # Adjusted to hold list of layers instead of (layer, head) tuples\n",
        "    seq_pos_to_keep: Dict[str, str],\n",
        ") -> Dict[int, Bool[Tensor, \"batch seq\"]]:  # Adjusted the return type to \"batch seq\"\n",
        "    '''\n",
        "    Returns a dictionary mapping layers to a boolean mask giving the indices of the\n",
        "    MLP output which *shouldn't* be mean-ablated.\n",
        "\n",
        "    The output of this function will be used for the hook function that does ablation.\n",
        "    '''\n",
        "    mlp_outputs_and_posns_to_keep = {}\n",
        "    batch, seq = len(means_dataset), means_dataset.max_len\n",
        "\n",
        "    for layer in range(model.cfg.n_layers):\n",
        "        mask = t.zeros(size=(batch, seq))\n",
        "\n",
        "        for (mlp_type, layer_list) in circuit.items():\n",
        "            seq_pos = seq_pos_to_keep[mlp_type]\n",
        "            indices = means_dataset.word_idx[seq_pos]\n",
        "            if layer in layer_list:  # Check if the current layer is in the layer list for this mlp_type\n",
        "                mask[:, indices] = 1\n",
        "\n",
        "        mlp_outputs_and_posns_to_keep[layer] = mask.bool()\n",
        "\n",
        "    return mlp_outputs_and_posns_to_keep"
      ],
      "metadata": {
        "id": "MH4KI_wCu7M-"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def hook_fn_mask_mlp_out(\n",
        "    mlp_out: Float[Tensor, \"batch seq d_mlp\"],\n",
        "    hook: HookPoint,\n",
        "    mlp_outputs_and_posns_to_keep: Dict[int, Bool[Tensor, \"batch seq\"]],\n",
        "    means: Float[Tensor, \"layer batch seq d_mlp\"],\n",
        ") -> Float[Tensor, \"batch seq d_mlp\"]:\n",
        "    '''\n",
        "    Hook function which masks the MLP output of a transformer layer.\n",
        "\n",
        "    mlp_outputs_and_posns_to_keep\n",
        "        Dict created with the get_mlp_outputs_and_posns_to_keep function. This tells\n",
        "        us where to mask.\n",
        "\n",
        "    means\n",
        "        Tensor of mean MLP output values of the means_dataset over each group of prompts\n",
        "        with the same template. This tells us what values to mask with.\n",
        "    '''\n",
        "    # Get the mask for this layer, adapted for MLP output structure\n",
        "    mask_for_this_layer = mlp_outputs_and_posns_to_keep[hook.layer()].unsqueeze(-1).to(mlp_out.device)\n",
        "\n",
        "    # Set MLP output values to the mean where necessary\n",
        "    mlp_out = t.where(mask_for_this_layer, mlp_out, means[hook.layer()])\n",
        "\n",
        "    return mlp_out"
      ],
      "metadata": {
        "id": "fXWq7V0Mv0F4"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_mean_ablation_hook_MLP(\n",
        "    model: HookedTransformer,\n",
        "    means_dataset: Dataset,\n",
        "    circuit: Dict[str, List[Tuple[int, int]]] = CIRCUIT,\n",
        "    seq_pos_to_keep: Dict[str, str] = SEQ_POS_TO_KEEP,\n",
        "    is_permanent: bool = True,\n",
        ") -> HookedTransformer:\n",
        "    '''\n",
        "    Adds a permanent hook to the model, which ablates according to the circuit and\n",
        "    seq_pos_to_keep dictionaries.\n",
        "\n",
        "    In other words, when the model is run on ioi_dataset, every head's output will\n",
        "    be replaced with the mean over means_dataset for sequences with the same template,\n",
        "    except for a subset of heads and sequence positions as specified by the circuit\n",
        "    and seq_pos_to_keep dicts.\n",
        "    '''\n",
        "\n",
        "    model.reset_hooks(including_permanent=True)\n",
        "\n",
        "    # Compute the mean of each head's output on the ABC dataset, grouped by template\n",
        "    means = compute_means_by_template_MLP(means_dataset, model)\n",
        "\n",
        "    # Convert this into a boolean map\n",
        "    mlp_outputs_and_posns_to_keep = get_mlp_outputs_and_posns_to_keep(means_dataset, model, circuit, seq_pos_to_keep)\n",
        "\n",
        "    # Get a hook function which will patch in the mean z values for each head, at\n",
        "    # all positions which aren't important for the circuit\n",
        "    hook_fn = partial(\n",
        "        hook_fn_mask_mlp_out,\n",
        "        mlp_outputs_and_posns_to_keep=mlp_outputs_and_posns_to_keep,\n",
        "        means=means\n",
        "    )\n",
        "\n",
        "    # Apply hook\n",
        "    model.add_hook(lambda name: name.endswith(\"mlp_out\"), hook_fn, is_permanent=True)\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "sJlawX18v-yD"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mean_ablate_by_lst_MLP(lst, model, orig_score, print_output=True):\n",
        "    CIRCUIT = {\n",
        "        \"number mover\": lst,\n",
        "        # \"number mover 4\": lst,\n",
        "        \"number mover 3\": lst,\n",
        "        \"number mover 2\": lst,\n",
        "        \"number mover 1\": lst,\n",
        "    }\n",
        "\n",
        "    SEQ_POS_TO_KEEP = {\n",
        "        \"number mover\": \"end\",\n",
        "        # \"number mover 4\": \"S4\",\n",
        "        \"number mover 3\": \"S3\",\n",
        "        \"number mover 2\": \"S2\",\n",
        "        \"number mover 1\": \"S1\",\n",
        "    }\n",
        "\n",
        "    model.reset_hooks(including_permanent=True)  #must do this after running with mean ablation hook\n",
        "\n",
        "    # ioi_logits_original, ioi_cache = model.run_with_cache(dataset.toks)\n",
        "\n",
        "    model = add_mean_ablation_hook_MLP(model, means_dataset=dataset_2, circuit=CIRCUIT, seq_pos_to_keep=SEQ_POS_TO_KEEP)\n",
        "    new_logits = model(dataset.toks)\n",
        "\n",
        "    # orig_score = logits_to_ave_logit_diff_2(ioi_logits_original, dataset)\n",
        "    new_score = logits_to_ave_logit_diff(new_logits, dataset)\n",
        "    if print_output:\n",
        "        # print(f\"Average logit difference (IOI dataset, using entire model): {orig_score:.4f}\")\n",
        "        # print(f\"Average logit difference (IOI dataset, only using circuit): {new_score:.4f}\")\n",
        "        print(f\"Average logit difference (circuit / full) %: {100 * new_score / orig_score:.4f}\")\n",
        "    # return new_score\n",
        "    return 100 * new_score / orig_score"
      ],
      "metadata": {
        "id": "Ko6itvH15NtO"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# run outside fns"
      ],
      "metadata": {
        "id": "0Lvddi6GwT0B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## add ablation hooks"
      ],
      "metadata": {
        "id": "S6hoY8qjyM40"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lst = list(range(9))\n",
        "\n",
        "CIRCUIT = {\n",
        "    \"number mover\": lst,\n",
        "    # \"number mover 4\": lst,\n",
        "    \"number mover 3\": lst,\n",
        "    \"number mover 2\": lst,\n",
        "    \"number mover 1\": lst,\n",
        "}\n",
        "\n",
        "SEQ_POS_TO_KEEP = {\n",
        "    \"number mover\": \"end\",\n",
        "    # \"number mover 4\": \"S4\",\n",
        "    \"number mover 3\": \"S3\",\n",
        "    \"number mover 2\": \"S2\",\n",
        "    \"number mover 1\": \"S1\",\n",
        "}"
      ],
      "metadata": {
        "id": "V06TRL2P_Iq2"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.reset_hooks(including_permanent=True)\n",
        "logits_orig, orig_cache = model.run_with_cache(dataset.toks)\n",
        "orig_score = logits_to_ave_logit_diff(logits_orig, dataset)"
      ],
      "metadata": {
        "id": "oug8qZm81f-B"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.reset_hooks(including_permanent=True)\n",
        "means = compute_means_by_template_MLP(dataset_2, model)\n",
        "mlp_outputs_and_posns_to_keep = get_mlp_outputs_and_posns_to_keep(dataset_2, model, CIRCUIT, SEQ_POS_TO_KEEP)\n",
        "\n",
        "# Get a hook function which will patch in the mean MLP output values for each MLP layer, at\n",
        "# all positions which aren't important for the circuit\n",
        "hook_fn = partial(\n",
        "    hook_fn_mask_mlp_out,\n",
        "    mlp_outputs_and_posns_to_keep = mlp_outputs_and_posns_to_keep,\n",
        "    means=means\n",
        ")\n",
        "\n",
        "# Apply hook\n",
        "model.add_hook(lambda name: name.endswith(\"mlp_out\"), hook_fn, is_permanent=True)"
      ],
      "metadata": {
        "id": "UToVh5yDGUf4"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # run entire dataset through\n",
        "\n",
        "# new_logits = model(dataset.toks)\n",
        "# # new_logits.size()\n",
        "\n",
        "# new_score = logits_to_ave_logit_diff(new_logits, dataset)\n",
        "\n",
        "# print(f\"Average logit difference (dataset, using entire model): {orig_score:.4f}\")\n",
        "# print(f\"Average logit difference (dataset, only using circuit): {new_score:.4f}\")\n",
        "# print(f\"Average logit difference (circuit / full) %: {100 * new_score / orig_score:.4f}\")"
      ],
      "metadata": {
        "id": "UGUdVXYDyV9W"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## run 1 prompt and get pred tokens"
      ],
      "metadata": {
        "id": "fmEZD51bybHU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The hook operates on the seq len of the means dataset (which has 4 tokens). Thus, don't prepend (so set prepend_bos=False in prompt_tokens) or add the answer to it, else that's two extra tokens! OR, preprend to every sample in the means dataset!"
      ],
      "metadata": {
        "id": "Fi7s-Sax5ms1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"1 2 3 4\"\n",
        "answer = \" 5\"\n",
        "\n",
        "if not answer.startswith(\" \"):\n",
        "    answer = \" \" + answer\n",
        "# GPT-2 often treats the first token weirdly, so lets give it a resting position\n",
        "prompt_tokens = model.to_tokens(prompt, prepend_bos=False)\n",
        "answer_tokens = model.to_tokens(answer, prepend_bos=False)\n",
        "prompt_str_tokens = model.to_str_tokens(prompt, prepend_bos=False)\n",
        "answer_str_tokens = model.to_str_tokens(answer, prepend_bos=False)\n",
        "prompt_length = len(prompt_str_tokens)\n",
        "answer_length = len(answer_str_tokens)\n",
        "\n",
        "print(\"Tokenized prompt:\", prompt_str_tokens)\n",
        "print(\"Tokenized answer:\", answer_str_tokens)\n",
        "logits = remove_batch_dim(model(prompt_tokens))\n",
        "probs = logits.softmax(dim=-1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TwCEVPQq4tOn",
        "outputId": "83712ce0-4ff6-422e-a293-7abb023c67df"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized prompt: ['1', ' 2', ' 3', ' 4']\n",
            "Tokenized answer: [' 5']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# logits.size()\n",
        "# probs.size()"
      ],
      "metadata": {
        "id": "JkovZzSGzVj4"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logits = logits[0]\n",
        "probs = probs[0]"
      ],
      "metadata": {
        "id": "D3n57GHbla2n"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = torch.cat((prompt_tokens, answer_tokens), dim=1)"
      ],
      "metadata": {
        "id": "JTF7ypuqzixE"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_k = 5\n",
        "\n",
        "answer_ranks = []\n",
        "for index in range(prompt_length, prompt_length + answer_length):\n",
        "    answer_token = tokens[0, index]\n",
        "    answer_str_token = answer_str_tokens[index - prompt_length]\n",
        "    # Offset by 1 because models predict the NEXT token\n",
        "    token_probs = probs[index - 1]\n",
        "    sorted_token_probs, sorted_token_values = token_probs.sort(descending=True) # sorted_token_values are the indices; the indices correspond to token num of vocab\n",
        "    # Janky way to get the index of the token in the sorted list - I couldn't find a better way?\n",
        "    correct_rank = torch.arange(len(sorted_token_values))[\n",
        "        (sorted_token_values == answer_token).cpu()\n",
        "    ].item()\n",
        "    answer_ranks.append((answer_str_token, correct_rank))\n",
        "\n",
        "    # String formatting syntax - the first number gives the number of characters to pad to, the second number gives the number of decimal places.\n",
        "    # rprint gives rich text printing\n",
        "    print(\n",
        "        f\"Performance on answer token:\\n[b]Rank: {correct_rank: <8} Logit: {logits[index-1, answer_token].item():5.2f} Prob: {token_probs[answer_token].item():6.2%} Token: |{answer_str_token}|[/b]\"\n",
        "    )\n",
        "    for i in range(top_k):\n",
        "        print(\n",
        "            f\"Top {i}th token. Logit: {logits[index-1, sorted_token_values[i]].item():5.2f} Prob: {sorted_token_probs[i].item():6.2%} Token: |{model.to_string(sorted_token_values[i])}|\"\n",
        "        )\n",
        "print(f\"[b]Ranks of the answer tokens:[/b] {answer_ranks}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c36d85b8-2166-46d4-a956-2c3f6ff23e2e",
        "id": "1qJPl5a57Lm-"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performance on answer token:\n",
            "[b]Rank: 1        Logit: 14.77 Prob: 16.71% Token: | 5|[/b]\n",
            "Top 0th token. Logit: 15.17 Prob: 25.05% Token: | 4|\n",
            "Top 1th token. Logit: 14.77 Prob: 16.71% Token: | 5|\n",
            "Top 2th token. Logit: 14.15 Prob:  9.02% Token: | 3|\n",
            "Top 3th token. Logit: 13.89 Prob:  6.98% Token: | 6|\n",
            "Top 4th token. Logit: 13.85 Prob:  6.66% Token: | 1|\n",
            "[b]Ranks of the answer tokens:[/b] [(' 5', 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# get pred tokens fn"
      ],
      "metadata": {
        "id": "AvlNok6k0YK9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Four key changes: prepend False, model takes in just prompt_tokens (no answer), slice first element of first dim of logits"
      ],
      "metadata": {
        "id": "0drtuACW1hdH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_prompt(\n",
        "    prompt: str,\n",
        "    answer: str,\n",
        "    model,  # Can't give type hint due to circular imports\n",
        "    prepend_space_to_answer: Optional[bool] = True,\n",
        "    print_details: Optional[bool] = True,\n",
        "    prepend_bos: Optional[bool] = False, # key change\n",
        "    top_k: Optional[int] = 10,\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        prompt:\n",
        "            The prompt string, e.g. \"Why did the elephant cross the\".\n",
        "        answer:\n",
        "            The answer, e.g. \"road\". Note that if you set prepend_space_to_answer to False, you need\n",
        "            to think about if you have a space before the answer here (as e.g. in this example the\n",
        "            answer may really be \" road\" if the prompt ends without a trailing space).\n",
        "        model:\n",
        "            The model.\n",
        "        prepend_space_to_answer:\n",
        "            Whether or not to prepend a space to the answer. Note this will only ever prepend a\n",
        "            space if the answer doesn't already start with one.\n",
        "        print_details:\n",
        "            Print the prompt (as a string but broken up by token), answer and top k tokens (all\n",
        "            with logit, rank and probability).\n",
        "        prepend_bos:\n",
        "            Overrides self.cfg.default_prepend_bos if set. Whether to prepend\n",
        "            the BOS token to the input (applicable when input is a string). Models generally learn\n",
        "            to use the BOS token as a resting place for attention heads (i.e. a way for them to be\n",
        "            \"turned off\"). This therefore often improves performance slightly.\n",
        "        top_k:\n",
        "            Top k tokens to print details of (when print_details is set to True).\n",
        "\n",
        "    Returns:\n",
        "        None (just prints the results directly).\n",
        "    \"\"\"\n",
        "    if prepend_space_to_answer and not answer.startswith(\" \"):\n",
        "        answer = \" \" + answer\n",
        "    # GPT-2 often treats the first token weirdly, so lets give it a resting position\n",
        "    prompt_tokens = model.to_tokens(prompt, prepend_bos=prepend_bos)\n",
        "    answer_tokens = model.to_tokens(answer, prepend_bos=False)\n",
        "    tokens = torch.cat((prompt_tokens, answer_tokens), dim=1)\n",
        "    prompt_str_tokens = model.to_str_tokens(prompt, prepend_bos=prepend_bos)\n",
        "    answer_str_tokens = model.to_str_tokens(answer, prepend_bos=False)\n",
        "    prompt_length = len(prompt_str_tokens)\n",
        "    answer_length = len(answer_str_tokens)\n",
        "    if print_details:\n",
        "        print(\"Tokenized prompt:\", prompt_str_tokens)\n",
        "        print(\"Tokenized answer:\", answer_str_tokens)\n",
        "    # logits = remove_batch_dim(model(tokens))  # key change\n",
        "    logits = remove_batch_dim(model(prompt_tokens))  # key change\n",
        "    probs = logits.softmax(dim=-1)\n",
        "\n",
        "    # key changes\n",
        "    logits = logits[0]\n",
        "    probs = probs[0]\n",
        "\n",
        "    answer_ranks = []\n",
        "    for index in range(prompt_length, prompt_length + answer_length):\n",
        "        answer_token = tokens[0, index]\n",
        "        answer_str_token = answer_str_tokens[index - prompt_length]\n",
        "        # Offset by 1 because models predict the NEXT token\n",
        "        token_probs = probs[index - 1]\n",
        "        sorted_token_probs, sorted_token_values = token_probs.sort(descending=True)\n",
        "        # Janky way to get the index of the token in the sorted list - I couldn't find a better way?\n",
        "        correct_rank = torch.arange(len(sorted_token_values))[\n",
        "            (sorted_token_values == answer_token).cpu()\n",
        "        ].item()\n",
        "        answer_ranks.append((answer_str_token, correct_rank))\n",
        "        if print_details:\n",
        "            # String formatting syntax - the first number gives the number of characters to pad to, the second number gives the number of decimal places.\n",
        "            # rprint gives rich text printing\n",
        "            print(\n",
        "                f\"Performance on answer token:\\n[b]Rank: {correct_rank: <8} Logit: {logits[index-1, answer_token].item():5.2f} Prob: {token_probs[answer_token].item():6.2%} Token: |{answer_str_token}|[/b]\"\n",
        "            )\n",
        "            for i in range(top_k):\n",
        "                print(\n",
        "                    f\"Top {i}th token. Logit: {logits[index-1, sorted_token_values[i]].item():5.2f} Prob: {sorted_token_probs[i].item():6.2%} Token: |{model.to_string(sorted_token_values[i])}|\"\n",
        "                )\n",
        "    print(f\"[b]Ranks of the answer tokens:[/b] {answer_ranks}\")"
      ],
      "metadata": {
        "id": "zbFmLmyr0an3"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lst = [layer for layer in range(12) if layer !=9]\n",
        "CIRCUIT = {\n",
        "    \"number mover\": lst,\n",
        "    \"number mover 3\": lst,\n",
        "    \"number mover 2\": lst,\n",
        "    \"number mover 1\": lst,\n",
        "}\n",
        "\n",
        "SEQ_POS_TO_KEEP = {\n",
        "    \"number mover\": \"end\",\n",
        "    \"number mover 3\": \"S3\",\n",
        "    \"number mover 2\": \"S2\",\n",
        "    \"number mover 1\": \"S1\",\n",
        "}\n",
        "\n",
        "model = add_mean_ablation_hook_MLP(model, means_dataset=dataset_2, circuit=CIRCUIT, seq_pos_to_keep=SEQ_POS_TO_KEEP)\n",
        "\n",
        "test_prompt(\n",
        "    prompt = \"1 2 3 4\",\n",
        "    answer = \" 5\",\n",
        "    model = model,\n",
        "    prepend_space_to_answer= True,\n",
        "    print_details = True,\n",
        "    prepend_bos = False,\n",
        "    top_k = 10,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k18zY1Pk0yke",
        "outputId": "6dc29ed5-ef84-43a3-ae65-3732f54b80c5"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized prompt: ['1', ' 2', ' 3', ' 4']\n",
            "Tokenized answer: [' 5']\n",
            "Performance on answer token:\n",
            "[b]Rank: 1        Logit: 14.04 Prob: 23.80% Token: | 5|[/b]\n",
            "Top 0th token. Logit: 14.18 Prob: 27.48% Token: | 4|\n",
            "Top 1th token. Logit: 14.04 Prob: 23.80% Token: | 5|\n",
            "Top 2th token. Logit: 13.14 Prob:  9.67% Token: | 6|\n",
            "Top 3th token. Logit: 12.47 Prob:  4.95% Token: | 1|\n",
            "Top 4th token. Logit: 12.34 Prob:  4.33% Token: | 3|\n",
            "Top 5th token. Logit: 11.93 Prob:  2.87% Token: | 2|\n",
            "Top 6th token. Logit: 11.71 Prob:  2.31% Token: | 0|\n",
            "Top 7th token. Logit: 11.09 Prob:  1.24% Token: | 7|\n",
            "Top 8th token. Logit: 10.96 Prob:  1.09% Token: | 8|\n",
            "Top 9th token. Logit: 10.28 Prob:  0.55% Token: |/|\n",
            "[b]Ranks of the answer tokens:[/b] [(' 5', 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def add_MLP_hook_by_lst(model, lst):\n",
        "    CIRCUIT = {\n",
        "        \"number mover\": lst,\n",
        "        \"number mover 3\": lst,\n",
        "        \"number mover 2\": lst,\n",
        "        \"number mover 1\": lst,\n",
        "    }\n",
        "\n",
        "    SEQ_POS_TO_KEEP = {\n",
        "        \"number mover\": \"end\",\n",
        "        \"number mover 3\": \"S3\",\n",
        "        \"number mover 2\": \"S2\",\n",
        "        \"number mover 1\": \"S1\",\n",
        "    }\n",
        "\n",
        "    model = add_mean_ablation_hook_MLP(model, means_dataset=dataset_2, circuit=CIRCUIT, seq_pos_to_keep=SEQ_POS_TO_KEEP)\n",
        "    return model"
      ],
      "metadata": {
        "id": "_Jn_-rM02dlX"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ablated_model = add_MLP_hook_by_lst(model, [])\n",
        "\n",
        "test_prompt(\n",
        "    prompt = \"1 2 3 4\",\n",
        "    answer = \" 5\",\n",
        "    model = ablated_model,\n",
        "    prepend_space_to_answer= True,\n",
        "    print_details = True,\n",
        "    prepend_bos = False,\n",
        "    top_k = 10,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KDWgGrOg4WP-",
        "outputId": "c1849e81-fe26-4a66-8768-312a924152a0"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized prompt: ['1', ' 2', ' 3', ' 4']\n",
            "Tokenized answer: [' 5']\n",
            "Performance on answer token:\n",
            "[b]Rank: 6        Logit: 13.14 Prob:  3.28% Token: | 5|[/b]\n",
            "Top 0th token. Logit: 13.72 Prob:  5.86% Token: |.|\n",
            "Top 1th token. Logit: 13.70 Prob:  5.76% Token: | 1|\n",
            "Top 2th token. Logit: 13.49 Prob:  4.65% Token: | 3|\n",
            "Top 3th token. Logit: 13.43 Prob:  4.40% Token: | 4|\n",
            "Top 4th token. Logit: 13.33 Prob:  3.97% Token: | 2|\n",
            "Top 5th token. Logit: 13.33 Prob:  3.96% Token: | 0|\n",
            "Top 6th token. Logit: 13.14 Prob:  3.28% Token: | 5|\n",
            "Top 7th token. Logit: 13.03 Prob:  2.95% Token: | 6|\n",
            "Top 8th token. Logit: 12.75 Prob:  2.23% Token: | 8|\n",
            "Top 9th token. Logit: 12.71 Prob:  2.14% Token: | 7|\n",
            "[b]Ranks of the answer tokens:[/b] [(' 5', 6)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ablated_model = add_MLP_hook_by_lst(model, [layer for layer in range(12)])\n",
        "\n",
        "test_prompt(\n",
        "    prompt = \"1 2 3 4\",\n",
        "    answer = \" 5\",\n",
        "    model = ablated_model,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Jwsvq2N4nfi",
        "outputId": "a8e67309-dcc5-4aef-9f49-4348236f1a76"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized prompt: ['1', ' 2', ' 3', ' 4']\n",
            "Tokenized answer: [' 5']\n",
            "Performance on answer token:\n",
            "[b]Rank: 0        Logit: 16.81 Prob: 92.39% Token: | 5|[/b]\n",
            "Top 0th token. Logit: 16.81 Prob: 92.39% Token: | 5|\n",
            "Top 1th token. Logit: 11.62 Prob:  0.51% Token: | 4|\n",
            "Top 2th token. Logit: 11.34 Prob:  0.39% Token: | 1|\n",
            "Top 3th token. Logit: 11.23 Prob:  0.35% Token: | 6|\n",
            "Top 4th token. Logit: 11.10 Prob:  0.30% Token: | 3|\n",
            "Top 5th token. Logit: 10.88 Prob:  0.24% Token: | 0|\n",
            "Top 6th token. Logit: 10.85 Prob:  0.24% Token: |/|\n",
            "Top 7th token. Logit: 10.22 Prob:  0.13% Token: | 7|\n",
            "Top 8th token. Logit: 10.10 Prob:  0.11% Token: |\n",
            "|\n",
            "Top 9th token. Logit:  9.94 Prob:  0.10% Token: |5|\n",
            "[b]Ranks of the answer tokens:[/b] [(' 5', 0)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# run using only fns"
      ],
      "metadata": {
        "id": "zkx8xD8dwWOL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lst = [layer for layer in range(12) if layer !=9]\n",
        "perc_of_orig = mean_ablate_by_lst_MLP(lst, model, orig_score, print_output=False).item()\n",
        "perc_of_orig"
      ],
      "metadata": {
        "id": "NDt2mk245r94",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c8fd3f0-682b-42d2-dcb3-c1fc7e2285bd"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7.348504066467285"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ablated_model = add_MLP_hook_by_lst(model, lst)\n",
        "\n",
        "test_prompt(\n",
        "    prompt = \"1 2 3 4\",\n",
        "    answer = \" 5\",\n",
        "    model = ablated_model,\n",
        "    prepend_space_to_answer= True,\n",
        "    print_details = True,\n",
        "    prepend_bos = False,\n",
        "    top_k = 10,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LxfsGtqJ2WTz",
        "outputId": "084b893e-1a99-430d-dc56-ee92097231bd"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized prompt: ['1', ' 2', ' 3', ' 4']\n",
            "Tokenized answer: [' 5']\n",
            "Performance on answer token:\n",
            "[b]Rank: 1        Logit: 14.04 Prob: 23.80% Token: | 5|[/b]\n",
            "Top 0th token. Logit: 14.18 Prob: 27.48% Token: | 4|\n",
            "Top 1th token. Logit: 14.04 Prob: 23.80% Token: | 5|\n",
            "Top 2th token. Logit: 13.14 Prob:  9.67% Token: | 6|\n",
            "Top 3th token. Logit: 12.47 Prob:  4.95% Token: | 1|\n",
            "Top 4th token. Logit: 12.34 Prob:  4.33% Token: | 3|\n",
            "Top 5th token. Logit: 11.93 Prob:  2.87% Token: | 2|\n",
            "Top 6th token. Logit: 11.71 Prob:  2.31% Token: | 0|\n",
            "Top 7th token. Logit: 11.09 Prob:  1.24% Token: | 7|\n",
            "Top 8th token. Logit: 10.96 Prob:  1.09% Token: | 8|\n",
            "Top 9th token. Logit: 10.28 Prob:  0.55% Token: |/|\n",
            "[b]Ranks of the answer tokens:[/b] [(' 5', 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is faster if use GPU (as there's several runs)"
      ],
      "metadata": {
        "id": "JpC_GdHK0M9Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(12):\n",
        "    lst = [layer for layer in range(12) if layer != i]\n",
        "    perc_of_orig = mean_ablate_by_lst_MLP(lst, model, orig_score, print_output=False).item()\n",
        "    print(i, perc_of_orig)"
      ],
      "metadata": {
        "id": "I9SR5ETh6BWw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a34c9673-8828-430e-809e-4bcffff9cef6"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 40.28706359863281\n",
            "1 66.07421875\n",
            "2 68.1141128540039\n",
            "3 85.263916015625\n",
            "4 68.15135192871094\n",
            "5 86.04444122314453\n",
            "6 70.00276184082031\n",
            "7 82.18880462646484\n",
            "8 80.56534576416016\n",
            "9 7.348504066467285\n",
            "10 67.42327117919922\n",
            "11 74.48482513427734\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mlp_to_not_ablate = [0, 1, 2, 4, 6, 9, 10, 11]\n",
        "perc_of_orig = mean_ablate_by_lst_MLP(mlp_to_not_ablate, model, orig_score, print_output=False).item()\n",
        "print(perc_of_orig)"
      ],
      "metadata": {
        "id": "kszdCRW_6cmS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c00378fb-8b39-4960-a28f-a49d25190937"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "42.85197830200195\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ablated_model = add_MLP_hook_by_lst(model, mlp_to_not_ablate)\n",
        "\n",
        "test_prompt(\n",
        "    prompt = \"1 2 3 4\",\n",
        "    answer = \" 5\",\n",
        "    model = ablated_model,\n",
        "    prepend_space_to_answer= True,\n",
        "    print_details = True,\n",
        "    prepend_bos = False,\n",
        "    top_k = 10,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lEZZYceZ3EFr",
        "outputId": "a631a4b7-25ca-4cd2-e327-c713add5f8ba"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized prompt: ['1', ' 2', ' 3', ' 4']\n",
            "Tokenized answer: [' 5']\n",
            "Performance on answer token:\n",
            "[b]Rank: 0        Logit: 15.52 Prob: 43.16% Token: | 5|[/b]\n",
            "Top 0th token. Logit: 15.52 Prob: 43.16% Token: | 5|\n",
            "Top 1th token. Logit: 13.90 Prob:  8.54% Token: | 4|\n",
            "Top 2th token. Logit: 13.64 Prob:  6.65% Token: | 3|\n",
            "Top 3th token. Logit: 13.31 Prob:  4.78% Token: | 1|\n",
            "Top 4th token. Logit: 13.18 Prob:  4.19% Token: | 6|\n",
            "Top 5th token. Logit: 13.08 Prob:  3.77% Token: | 0|\n",
            "Top 6th token. Logit: 12.98 Prob:  3.41% Token: | 2|\n",
            "Top 7th token. Logit: 12.35 Prob:  1.83% Token: | 7|\n",
            "Top 8th token. Logit: 11.93 Prob:  1.20% Token: |.|\n",
            "Top 9th token. Logit: 11.91 Prob:  1.17% Token: | -|\n",
            "[b]Ranks of the answer tokens:[/b] [(' 5', 0)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MLP and Head together fns"
      ],
      "metadata": {
        "id": "DlpH0Wib-v1j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_heads_and_posns_to_keep(\n",
        "    means_dataset: Dataset,\n",
        "    model: HookedTransformer,\n",
        "    circuit: Dict[str, List[Tuple[int, int]]],\n",
        "    seq_pos_to_keep: Dict[str, str],\n",
        ") -> Dict[int, Bool[Tensor, \"batch seq head\"]]:\n",
        "    '''\n",
        "    Returns a dictionary mapping layers to a boolean mask giving the indices of the\n",
        "    z output which *shouldn't* be mean-ablated.\n",
        "\n",
        "    The output of this function will be used for the hook function that does ablation.\n",
        "    '''\n",
        "    heads_and_posns_to_keep = {}\n",
        "    batch, seq, n_heads = len(means_dataset), means_dataset.max_len, model.cfg.n_heads\n",
        "\n",
        "    for layer in range(model.cfg.n_layers):\n",
        "\n",
        "        mask = t.zeros(size=(batch, seq, n_heads))\n",
        "\n",
        "        for (head_type, head_list) in circuit.items():\n",
        "            seq_pos = seq_pos_to_keep[head_type]\n",
        "            indices = means_dataset.word_idx[seq_pos] # modify this for key vs query pos. curr, this is query\n",
        "            for (layer_idx, head_idx) in head_list:\n",
        "                if layer_idx == layer:\n",
        "                    mask[:, indices, head_idx] = 1\n",
        "\n",
        "        heads_and_posns_to_keep[layer] = mask.bool()\n",
        "\n",
        "    return heads_and_posns_to_keep\n",
        "\n",
        "def hook_fn_mask_z(\n",
        "    z: Float[Tensor, \"batch seq head d_head\"],\n",
        "    hook: HookPoint,\n",
        "    heads_and_posns_to_keep: Dict[int, Bool[Tensor, \"batch seq head\"]],\n",
        "    means: Float[Tensor, \"layer batch seq head d_head\"],\n",
        ") -> Float[Tensor, \"batch seq head d_head\"]:\n",
        "    '''\n",
        "    Hook function which masks the z output of a transformer head.\n",
        "\n",
        "    heads_and_posns_to_keep\n",
        "        Dict created with the get_heads_and_posns_to_keep function. This tells\n",
        "        us where to mask.\n",
        "\n",
        "    means\n",
        "        Tensor of mean z values of the means_dataset over each group of prompts\n",
        "        with the same template. This tells us what values to mask with.\n",
        "    '''\n",
        "    # Get the mask for this layer, and add d_head=1 dimension so it broadcasts correctly\n",
        "    mask_for_this_layer = heads_and_posns_to_keep[hook.layer()].unsqueeze(-1).to(z.device)\n",
        "\n",
        "    # Set z values to the mean\n",
        "    z = t.where(mask_for_this_layer, z, means[hook.layer()])\n",
        "\n",
        "    return z\n",
        "\n",
        "def compute_means_by_template(\n",
        "    means_dataset: Dataset,\n",
        "    model: HookedTransformer\n",
        ") -> Float[Tensor, \"layer batch seq head_idx d_head\"]:\n",
        "    '''\n",
        "    Returns the mean of each head's output over the means dataset. This mean is\n",
        "    computed separately for each group of prompts with the same template (these\n",
        "    are given by means_dataset.groups).\n",
        "    '''\n",
        "    # Cache the outputs of every head\n",
        "    _, means_cache = model.run_with_cache(\n",
        "        means_dataset.toks.long(),\n",
        "        return_type=None,\n",
        "        names_filter=lambda name: name.endswith(\"z\"),\n",
        "    )\n",
        "    # Create tensor to store means\n",
        "    n_layers, n_heads, d_head = model.cfg.n_layers, model.cfg.n_heads, model.cfg.d_head\n",
        "    batch, seq_len = len(means_dataset), means_dataset.max_len\n",
        "    means = t.zeros(size=(n_layers, batch, seq_len, n_heads, d_head), device=model.cfg.device)\n",
        "\n",
        "    # Get set of different templates for this data\n",
        "    for layer in range(model.cfg.n_layers):\n",
        "        z_for_this_layer: Float[Tensor, \"batch seq head d_head\"] = means_cache[utils.get_act_name(\"z\", layer)]\n",
        "        for template_group in means_dataset.groups:\n",
        "            z_for_this_template = z_for_this_layer[template_group]\n",
        "            z_means_for_this_template = einops.reduce(z_for_this_template, \"batch seq head d_head -> seq head d_head\", \"mean\")\n",
        "            means[layer, template_group] = z_means_for_this_template\n",
        "\n",
        "    return means\n",
        "\n",
        "def add_mean_ablation_hook(\n",
        "    model: HookedTransformer,\n",
        "    means_dataset: Dataset,\n",
        "    circuit: Dict[str, List[Tuple[int, int]]] = CIRCUIT,\n",
        "    seq_pos_to_keep: Dict[str, str] = SEQ_POS_TO_KEEP,\n",
        "    is_permanent: bool = True,\n",
        ") -> HookedTransformer:\n",
        "    '''\n",
        "    Adds a permanent hook to the model, which ablates according to the circuit and\n",
        "    seq_pos_to_keep dictionaries.\n",
        "\n",
        "    In other words, when the model is run on ioi_dataset, every head's output will\n",
        "    be replaced with the mean over means_dataset for sequences with the same template,\n",
        "    except for a subset of heads and sequence positions as specified by the circuit\n",
        "    and seq_pos_to_keep dicts.\n",
        "    '''\n",
        "\n",
        "    model.reset_hooks(including_permanent=True)\n",
        "\n",
        "    # Compute the mean of each head's output on the ABC dataset, grouped by template\n",
        "    means = compute_means_by_template(means_dataset, model)\n",
        "\n",
        "    # Convert this into a boolean map\n",
        "    heads_and_posns_to_keep = get_heads_and_posns_to_keep(means_dataset, model, circuit, seq_pos_to_keep)\n",
        "\n",
        "    # Get a hook function which will patch in the mean z values for each head, at\n",
        "    # all positions which aren't important for the circuit\n",
        "    hook_fn = partial(\n",
        "        hook_fn_mask_z,\n",
        "        heads_and_posns_to_keep=heads_and_posns_to_keep,\n",
        "        means=means\n",
        "    )\n",
        "\n",
        "    # Apply hook\n",
        "    model.add_hook(lambda name: name.endswith(\"z\"), hook_fn, is_permanent=is_permanent)\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "cE7xLtws_Pnt"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_mean_ablation_hook_MLP_head(\n",
        "    model: HookedTransformer,\n",
        "    means_dataset: Dataset,\n",
        "    heads_lst, mlp_lst,\n",
        "    is_permanent: bool = True,\n",
        ") -> HookedTransformer:\n",
        "    CIRCUIT = {\n",
        "        \"number mover\": heads_lst,\n",
        "        \"number mover 3\": heads_lst,\n",
        "        \"number mover 2\": heads_lst,\n",
        "        \"number mover 1\": heads_lst,\n",
        "    }\n",
        "\n",
        "    SEQ_POS_TO_KEEP = {\n",
        "        \"number mover\": \"end\",\n",
        "        \"number mover 3\": \"S3\",\n",
        "        \"number mover 2\": \"S2\",\n",
        "        \"number mover 1\": \"S1\",\n",
        "    }\n",
        "\n",
        "    model.reset_hooks(including_permanent=True)\n",
        "\n",
        "    # Compute the mean of each head's output on the ABC dataset, grouped by template\n",
        "    means = compute_means_by_template(means_dataset, model)\n",
        "\n",
        "    # Convert this into a boolean map\n",
        "    heads_and_posns_to_keep = get_heads_and_posns_to_keep(means_dataset, model, CIRCUIT, SEQ_POS_TO_KEEP)\n",
        "\n",
        "    # Get a hook function which will patch in the mean z values for each head, at\n",
        "    # all positions which aren't important for the circuit\n",
        "    hook_fn = partial(\n",
        "        hook_fn_mask_z,\n",
        "        heads_and_posns_to_keep=heads_and_posns_to_keep,\n",
        "        means=means\n",
        "    )\n",
        "\n",
        "    # Apply hook\n",
        "    model.add_hook(lambda name: name.endswith(\"z\"), hook_fn, is_permanent=is_permanent)\n",
        "\n",
        "    ########################\n",
        "    CIRCUIT = {\n",
        "        \"number mover\": mlp_lst,\n",
        "        \"number mover 3\": mlp_lst,\n",
        "        \"number mover 2\": mlp_lst,\n",
        "        \"number mover 1\": mlp_lst,\n",
        "    }\n",
        "\n",
        "    SEQ_POS_TO_KEEP = {\n",
        "        \"number mover\": \"end\",\n",
        "        \"number mover 3\": \"S3\",\n",
        "        \"number mover 2\": \"S2\",\n",
        "        \"number mover 1\": \"S1\",\n",
        "    }\n",
        "\n",
        "    # Compute the mean of each head's output on the ABC dataset, grouped by template\n",
        "    means = compute_means_by_template_MLP(means_dataset, model)\n",
        "\n",
        "    # Convert this into a boolean map\n",
        "    mlp_outputs_and_posns_to_keep = get_mlp_outputs_and_posns_to_keep(means_dataset, model, CIRCUIT, SEQ_POS_TO_KEEP)\n",
        "\n",
        "    # Get a hook function which will patch in the mean z values for each head, at\n",
        "    # all positions which aren't important for the circuit\n",
        "    hook_fn = partial(\n",
        "        hook_fn_mask_mlp_out,\n",
        "        mlp_outputs_and_posns_to_keep=mlp_outputs_and_posns_to_keep,\n",
        "        means=means\n",
        "    )\n",
        "\n",
        "    # Apply hook\n",
        "    model.add_hook(lambda name: name.endswith(\"mlp_out\"), hook_fn, is_permanent=True)\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "mnGaDWe__CyA"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# run MLP and Head together"
      ],
      "metadata": {
        "id": "VDfzJNjP66tK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "heads_not_ablate = [(layer, head) for layer in range(12) for head in range(12)]\n",
        "mlps_not_ablate = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
        "\n",
        "ablated_model = add_mean_ablation_hook_MLP_head(model, dataset_2, heads_not_ablate, mlps_not_ablate)\n",
        "new_logits = ablated_model(dataset.toks)\n",
        "new_score = logits_to_ave_logit_diff(new_logits, dataset)\n",
        "print(f\"Average logit difference (circuit / full) %: {100 * new_score / orig_score:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pfjSc0rEALmb",
        "outputId": "76b9b4e7-6720-455d-bad7-e7fecccb865a"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average logit difference (circuit / full) %: 100.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get rid of last layer\n",
        "\n",
        "heads_not_ablate = [(layer, head) for layer in range(11) for head in range(12)]\n",
        "mlps_not_ablate = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
        "\n",
        "ablated_model = add_mean_ablation_hook_MLP_head(model, dataset_2, heads_not_ablate, mlps_not_ablate)\n",
        "new_logits = ablated_model(dataset.toks)\n",
        "new_score = logits_to_ave_logit_diff(new_logits, dataset)\n",
        "print(f\"Average logit difference (circuit / full) %: {100 * new_score / orig_score:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RHbM_oeSAsbO",
        "outputId": "3aceac21-27bf-431a-d755-c7d14760394f"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average logit difference (circuit / full) %: 75.3683\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_prompt(\n",
        "    prompt = \"1 2 3 4\",\n",
        "    answer = \" 5\",\n",
        "    model = ablated_model,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m_vGlhEv6s8H",
        "outputId": "5358d2d4-c2a7-42f2-9232-51fd59e1fadb"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized prompt: ['1', ' 2', ' 3', ' 4']\n",
            "Tokenized answer: [' 5']\n",
            "Performance on answer token:\n",
            "[b]Rank: 0        Logit: 17.24 Prob: 85.22% Token: | 5|[/b]\n",
            "Top 0th token. Logit: 17.24 Prob: 85.22% Token: | 5|\n",
            "Top 1th token. Logit: 13.36 Prob:  1.77% Token: | 6|\n",
            "Top 2th token. Logit: 13.25 Prob:  1.57% Token: | 4|\n",
            "Top 3th token. Logit: 12.70 Prob:  0.91% Token: | 1|\n",
            "Top 4th token. Logit: 12.68 Prob:  0.89% Token: | 3|\n",
            "Top 5th token. Logit: 12.36 Prob:  0.65% Token: | 7|\n",
            "Top 6th token. Logit: 12.19 Prob:  0.55% Token: |.|\n",
            "Top 7th token. Logit: 12.16 Prob:  0.53% Token: | 0|\n",
            "Top 8th token. Logit: 11.74 Prob:  0.35% Token: |/|\n",
            "Top 9th token. Logit: 11.45 Prob:  0.26% Token: | 9|\n",
            "[b]Ranks of the answer tokens:[/b] [(' 5', 0)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try circ: https://colab.research.google.com/drive/1N777jopL9usz4T7jgBllP2FG4wD9je3g#scrollTo=aZw403xx7s2x"
      ],
      "metadata": {
        "id": "pNoF3ZLt9Rjn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# try circ found by iteration 80%\n",
        "\n",
        "heads_not_ablate = [(0, 1), (0, 2), (0, 5), (0, 7), (0, 8), (0, 10), (1, 0), (1, 1), (1, 5), (1, 7), (1, 11), (2, 0), (2, 1), (2, 2), (2, 3), (2, 6), (2, 8), (2, 9), (2, 10), (2, 11), (3, 3), (3, 4), (3, 5), (3, 7), (3, 8), (3, 9), (3, 11), (4, 4), (4, 10), (5, 1), (5, 4), (5, 6), (5, 8), (5, 11), (6, 4), (6, 6), (6, 7), (6, 8), (6, 9), (6, 10), (6, 11), (7, 11), (9, 1)]\n",
        "mlps_not_ablate = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
        "\n",
        "model = add_mean_ablation_hook_MLP_head(model, dataset_2, heads_not_ablate, mlps_not_ablate)\n",
        "new_logits = model(dataset.toks)\n",
        "new_score = logits_to_ave_logit_diff(new_logits, dataset)\n",
        "print(f\"Average logit difference (circuit / full) %: {100 * new_score / orig_score:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ps_SUBDB4Zj",
        "outputId": "9c9c54f4-a123-4a83-ac6d-f0c0993f4f7a"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average logit difference (circuit / full) %: 80.9106\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_prompt(\n",
        "    prompt = \"1 2 3 4\",\n",
        "    answer = \" 5\",\n",
        "    model = ablated_model,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G2OPygOM7Mc9",
        "outputId": "24ddfac2-fb93-4810-ff6b-b8982e528dd5"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized prompt: ['1', ' 2', ' 3', ' 4']\n",
            "Tokenized answer: [' 5']\n",
            "Performance on answer token:\n",
            "[b]Rank: 0        Logit: 14.08 Prob: 19.40% Token: | 5|[/b]\n",
            "Top 0th token. Logit: 14.08 Prob: 19.40% Token: | 5|\n",
            "Top 1th token. Logit: 13.05 Prob:  6.95% Token: | 4|\n",
            "Top 2th token. Logit: 13.04 Prob:  6.82% Token: | 6|\n",
            "Top 3th token. Logit: 12.55 Prob:  4.18% Token: | 1|\n",
            "Top 4th token. Logit: 12.54 Prob:  4.15% Token: | 3|\n",
            "Top 5th token. Logit: 12.48 Prob:  3.91% Token: | 0|\n",
            "Top 6th token. Logit: 12.42 Prob:  3.70% Token: | 7|\n",
            "Top 7th token. Logit: 12.16 Prob:  2.85% Token: | 2|\n",
            "Top 8th token. Logit: 12.11 Prob:  2.71% Token: |.|\n",
            "Top 9th token. Logit: 11.65 Prob:  1.71% Token: | 8|\n",
            "[b]Ranks of the answer tokens:[/b] [(' 5', 0)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# output top preds vars not print"
      ],
      "metadata": {
        "id": "XBHql2Yf9pDM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_top_preds(\n",
        "    prompt: str,\n",
        "    answer: str,\n",
        "    model,  # Can't give type hint due to circular imports\n",
        "    prepend_space_to_answer: Optional[bool] = True,\n",
        "    print_details: Optional[bool] = True,\n",
        "    prepend_bos: Optional[bool] = False, # key change\n",
        "    top_k: Optional[int] = 10,\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        prompt:\n",
        "            The prompt string, e.g. \"Why did the elephant cross the\".\n",
        "        answer:\n",
        "            The answer, e.g. \"road\". Note that if you set prepend_space_to_answer to False, you need\n",
        "            to think about if you have a space before the answer here (as e.g. in this example the\n",
        "            answer may really be \" road\" if the prompt ends without a trailing space).\n",
        "        model:\n",
        "            The model.\n",
        "        prepend_space_to_answer:\n",
        "            Whether or not to prepend a space to the answer. Note this will only ever prepend a\n",
        "            space if the answer doesn't already start with one.\n",
        "        print_details:\n",
        "            Print the prompt (as a string but broken up by token), answer and top k tokens (all\n",
        "            with logit, rank and probability).\n",
        "        prepend_bos:\n",
        "            Overrides self.cfg.default_prepend_bos if set. Whether to prepend\n",
        "            the BOS token to the input (applicable when input is a string). Models generally learn\n",
        "            to use the BOS token as a resting place for attention heads (i.e. a way for them to be\n",
        "            \"turned off\"). This therefore often improves performance slightly.\n",
        "        top_k:\n",
        "            Top k tokens to print details of (when print_details is set to True).\n",
        "\n",
        "    Returns:\n",
        "        None (just prints the results directly).\n",
        "    \"\"\"\n",
        "    if prepend_space_to_answer and not answer.startswith(\" \"):\n",
        "        answer = \" \" + answer\n",
        "    # GPT-2 often treats the first token weirdly, so lets give it a resting position\n",
        "    prompt_tokens = model.to_tokens(prompt, prepend_bos=prepend_bos)\n",
        "    answer_tokens = model.to_tokens(answer, prepend_bos=False)\n",
        "    tokens = torch.cat((prompt_tokens, answer_tokens), dim=1)\n",
        "    prompt_str_tokens = model.to_str_tokens(prompt, prepend_bos=prepend_bos)\n",
        "    answer_str_tokens = model.to_str_tokens(answer, prepend_bos=False)\n",
        "    prompt_length = len(prompt_str_tokens)\n",
        "    answer_length = len(answer_str_tokens)\n",
        "    # if print_details:\n",
        "    #     print(\"Tokenized prompt:\", prompt_str_tokens)\n",
        "    #     print(\"Tokenized answer:\", answer_str_tokens)\n",
        "    # logits = remove_batch_dim(model(tokens))  # key change\n",
        "    logits = remove_batch_dim(model(prompt_tokens))  # key change\n",
        "    probs = logits.softmax(dim=-1)\n",
        "\n",
        "    # key changes\n",
        "    logits = logits[0]\n",
        "    probs = probs[0]\n",
        "\n",
        "    answer_ranks = []\n",
        "\n",
        "    # from end of input to how long answer is (if answer is 1 token, this is just 1)\n",
        "    for index in range(prompt_length, prompt_length + answer_length):\n",
        "        answer_token = tokens[0, index]\n",
        "        answer_str_token = answer_str_tokens[index - prompt_length]\n",
        "        # Offset by 1 because models predict the NEXT token\n",
        "        token_probs = probs[index - 1]\n",
        "        sorted_token_probs, sorted_token_values = token_probs.sort(descending=True)\n",
        "        # Janky way to get the index of the token in the sorted list - I couldn't find a better way?\n",
        "        correct_rank = torch.arange(len(sorted_token_values))[\n",
        "            (sorted_token_values == answer_token).cpu()\n",
        "        ].item()\n",
        "        answer_ranks.append((answer_str_token, correct_rank))\n",
        "        # if print_details:\n",
        "        #     # String formatting syntax - the first number gives the number of characters to pad to, the second number gives the number of decimal places.\n",
        "        #     # rprint gives rich text printing\n",
        "        #     print(\n",
        "        #         f\"Performance on answer token:\\n[b]Rank: {correct_rank: <8} Logit: {logits[index-1, answer_token].item():5.2f} Prob: {token_probs[answer_token].item():6.2%} Token: |{answer_str_token}|[/b]\"\n",
        "        #     )\n",
        "        #     for i in range(top_k):\n",
        "        #         print(\n",
        "        #             f\"Top {i}th token. Logit: {logits[index-1, sorted_token_values[i]].item():5.2f} Prob: {sorted_token_probs[i].item():6.2%} Token: |{model.to_string(sorted_token_values[i])}|\"\n",
        "        #         )\n",
        "    # print(f\"[b]Ranks of the answer tokens:[/b] {answer_ranks}\")\n",
        "\n",
        "    return logits[index-1, sorted_token_values[:top_k]], sorted_token_probs[:top_k], [model.to_string(tok) for tok in sorted_token_values[:top_k]]"
      ],
      "metadata": {
        "id": "oL9IZv3P9yDG"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# try circ found by iteration 80%\n",
        "\n",
        "heads_not_ablate = [(0, 1), (0, 2), (0, 5), (0, 7), (0, 8), (0, 10), (1, 0), (1, 1), (1, 5), (1, 7), (1, 11), (2, 0), (2, 1), (2, 2), (2, 3), (2, 6), (2, 8), (2, 9), (2, 10), (2, 11), (3, 3), (3, 4), (3, 5), (3, 7), (3, 8), (3, 9), (3, 11), (4, 4), (4, 10), (5, 1), (5, 4), (5, 6), (5, 8), (5, 11), (6, 4), (6, 6), (6, 7), (6, 8), (6, 9), (6, 10), (6, 11), (7, 11), (9, 1)]\n",
        "mlps_not_ablate = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
        "\n",
        "ablated_model = add_mean_ablation_hook_MLP_head(model, dataset_2, heads_not_ablate, mlps_not_ablate)\n",
        "new_logits = ablated_model(dataset.toks)\n",
        "new_score = logits_to_ave_logit_diff(new_logits, dataset)\n",
        "print(f\"Average logit difference (circuit / full) %: {100 * new_score / orig_score:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c9c54f4-a123-4a83-ac6d-f0c0993f4f7a",
        "id": "MQSQjlOs-1eo"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average logit difference (circuit / full) %: 80.9106\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logs, probs, toks = get_top_preds(\n",
        "    prompt = \"1 2 3 4\",\n",
        "    answer = \" 5\",\n",
        "    model = ablated_model,\n",
        ")\n",
        "probs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9defe692-7545-4073-8771-f9f4af3f2b25",
        "id": "2jkfOW-G-1eo"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.1940, 0.0695, 0.0682, 0.0418, 0.0415, 0.0391, 0.0370, 0.0285, 0.0271,\n",
              "        0.0171], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "toks"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d3EreDq1_FDr",
        "outputId": "b9dd714e-721a-426d-bebb-4df23e04c59b"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[' 5', ' 4', ' 6', ' 1', ' 3', ' 0', ' 7', ' 2', '.', ' 8']"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## test on loop"
      ],
      "metadata": {
        "id": "4qnIvlsI_-1p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "heads_not_ablate =  [(0, 1), (0, 2), (0, 5), (0, 7), (0, 8), (0, 10), (1, 0), (1, 1), (1, 5), (1, 7), (1, 11), (2, 0), (2, 1), (2, 2), (2, 3), (2, 6), (2, 8), (2, 9), (2, 10), (2, 11), (3, 3), (3, 4), (3, 5), (3, 7), (3, 8), (3, 9), (3, 11), (4, 4), (4, 10), (5, 1), (5, 4), (5, 6), (5, 8), (5, 11), (6, 4), (6, 6), (6, 7), (6, 8), (6, 9), (6, 10), (6, 11), (7, 11), (9, 1)]\n",
        "\n",
        "for i in range(12):\n",
        "    mlps_not_ablate = [layer for layer in range(12) if layer != i]\n",
        "    ablated_model = add_mean_ablation_hook_MLP_head(model, dataset_2, heads_not_ablate, mlps_not_ablate)\n",
        "    new_logits = ablated_model(dataset.toks)\n",
        "    new_score = logits_to_ave_logit_diff(new_logits, dataset)\n",
        "    print(i, f\"Average logit difference (circuit / full) %: {100 * new_score / orig_score:.4f}\")\n",
        "    # perc_of_orig = mean_ablate_by_lst_MLP(lst, model, orig_score, print_output=False).item()\n",
        "    # print(i, perc_of_orig)\n",
        "    logs, probs, toks = get_top_preds(prompt = \"1 2 3 4\", answer = \" 5\", model = ablated_model)\n",
        "    print(toks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dZUjCI9aAATM",
        "outputId": "fad2d4d3-a550-46e1-cf96-2500d8a607ca"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 Average logit difference (circuit / full) %: 23.6304\n",
            "['.', ' 0', ' 5', ' 4', ' 1', ' 3', ' 6', ' 2', ' 7', ' 49']\n",
            "1 Average logit difference (circuit / full) %: 45.9230\n",
            "[' 5', ' 4', ' 6', '.', ' 1', ' 0', ' 3', ' 2', ' 7', ' 8']\n",
            "2 Average logit difference (circuit / full) %: 45.6853\n",
            "[' 5', ' 4', '.', ' 6', ' 1', ' 3', ' 0', ' 2', ' 7', ' 8']\n",
            "3 Average logit difference (circuit / full) %: 64.7762\n",
            "[' 5', ' 6', ' 4', ' 7', ' 1', ' 3', ' 0', '.', ' 2', ' 8']\n",
            "4 Average logit difference (circuit / full) %: 53.2802\n",
            "[' 5', '.', ' 4', ' 0', ' 6', ' 1', ' 3', ' 2', ' 7', ' 8']\n",
            "5 Average logit difference (circuit / full) %: 72.4063\n",
            "[' 5', ' 4', ' 6', ' 1', ' 3', ' 0', ' 2', ' 7', '.', ' 8']\n",
            "6 Average logit difference (circuit / full) %: 51.0637\n",
            "[' 5', ' 4', '.', ' 0', ' 6', ' 1', ' 3', ' 2', ' 7', ' 8']\n",
            "7 Average logit difference (circuit / full) %: 49.6890\n",
            "[' 5', ' 4', ' 6', ' 3', ' 1', ' 0', ' 2', '.', ' 7', '/']\n",
            "8 Average logit difference (circuit / full) %: 50.5831\n",
            "[' 5', '.', ' 4', ' 6', ' 0', ' 1', ' 3', ' 7', ' 2', ' 9']\n",
            "9 Average logit difference (circuit / full) %: 12.3830\n",
            "[' 4', ' 5', ' 6', ' 1', ' 3', ' 0', '.', ' 2', ' 7', ' 8']\n",
            "10 Average logit difference (circuit / full) %: 49.6573\n",
            "[' 5', '.', ' 4', ' 6', ' 1', ' 3', ' 0', ' 7', ' 2', ' 8']\n",
            "11 Average logit difference (circuit / full) %: 59.4846\n",
            "[' 5', ' 4', ' 6', ' 3', ' 1', ' 7', ' 0', '.', ' 2', ' 9']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Head Ablation Expm Functions"
      ],
      "metadata": {
        "id": "BHHvz84w70vh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model.reset_hooks(including_permanent=True)\n",
        "# ioi_logits_original, ioi_cache = model.run_with_cache(dataset.toks)\n",
        "# orig_score = logits_to_ave_logit_diff(ioi_logits_original, dataset)"
      ],
      "metadata": {
        "id": "OI3FcmpMaNxB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mean_ablate_by_lst(lst, model, orig_score, print_output=True):\n",
        "    CIRCUIT = {\n",
        "        \"number mover\": lst,\n",
        "        # \"number mover 4\": lst,\n",
        "        \"number mover 3\": lst,\n",
        "        \"number mover 2\": lst,\n",
        "        \"number mover 1\": lst,\n",
        "    }\n",
        "\n",
        "    SEQ_POS_TO_KEEP = {\n",
        "        \"number mover\": \"end\",\n",
        "        # \"number mover 4\": \"S4\",\n",
        "        \"number mover 3\": \"S3\",\n",
        "        \"number mover 2\": \"S2\",\n",
        "        \"number mover 1\": \"S1\",\n",
        "    }\n",
        "\n",
        "    model.reset_hooks(including_permanent=True)  #must do this after running with mean ablation hook\n",
        "\n",
        "    # ioi_logits_original, ioi_cache = model.run_with_cache(dataset.toks)\n",
        "\n",
        "    model = ioi_circuit_extraction.add_mean_ablation_hook(model, means_dataset=dataset_2, circuit=CIRCUIT, seq_pos_to_keep=SEQ_POS_TO_KEEP)\n",
        "    ioi_logits_minimal = model(dataset.toks)\n",
        "\n",
        "    # orig_score = logits_to_ave_logit_diff_2(ioi_logits_original, dataset)\n",
        "    new_score = logits_to_ave_logit_diff(ioi_logits_minimal, dataset)\n",
        "    if print_output:\n",
        "        # print(f\"Average logit difference (IOI dataset, using entire model): {orig_score:.4f}\")\n",
        "        # print(f\"Average logit difference (IOI dataset, only using circuit): {new_score:.4f}\")\n",
        "        print(f\"Average logit difference (circuit / full) %: {100 * new_score / orig_score:.4f}\")\n",
        "    # return new_score\n",
        "    return 100 * new_score / orig_score"
      ],
      "metadata": {
        "id": "QLK5m1Ps70vh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_circuit_forw(curr_circuit=None, orig_score=100, threshold=10):\n",
        "    # threshold is T, a %. if performance is less than T%, allow its removal\n",
        "    if curr_circuit == []:\n",
        "        # Start with full circuit\n",
        "        curr_circuit = [(layer, head) for layer in range(12) for head in range(12)]\n",
        "\n",
        "    for layer in range(0, 12):\n",
        "        for head in range(12):\n",
        "            if (layer, head) not in curr_circuit:\n",
        "                continue\n",
        "\n",
        "            # Copying the curr_circuit so we can iterate over one and modify the other\n",
        "            copy_circuit = curr_circuit.copy()\n",
        "\n",
        "            # Temporarily removing the current tuple from the copied circuit\n",
        "            copy_circuit.remove((layer, head))\n",
        "\n",
        "            new_score = mean_ablate_by_lst(copy_circuit, model, orig_score, print_output=False).item()\n",
        "\n",
        "            # print((layer,head), new_score)\n",
        "            # If the result is less than the threshold, remove the tuple from the original list\n",
        "            if (100 - new_score) < threshold:\n",
        "                curr_circuit.remove((layer, head))\n",
        "\n",
        "                print(\"\\nRemoved:\", (layer, head))\n",
        "                print(new_score)\n",
        "\n",
        "    return curr_circuit, new_score"
      ],
      "metadata": {
        "id": "ybrqaAul70vi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_circuit_backw(curr_circuit=None, orig_score=100, threshold=10):\n",
        "    # threshold is T, a %. if performance is less than T%, allow its removal\n",
        "    if curr_circuit == []:\n",
        "        # Start with full circuit\n",
        "        curr_circuit = [(layer, head) for layer in range(12) for head in range(12)]\n",
        "\n",
        "    for layer in range(11, -1, -1):  # go thru all heads in a layer first\n",
        "        for head in range(12):\n",
        "            if (layer, head) not in curr_circuit:\n",
        "                continue\n",
        "\n",
        "            # Copying the curr_circuit so we can iterate over one and modify the other\n",
        "            copy_circuit = curr_circuit.copy()\n",
        "\n",
        "            # Temporarily removing the current tuple from the copied circuit\n",
        "            copy_circuit.remove((layer, head))\n",
        "\n",
        "            new_score = mean_ablate_by_lst(copy_circuit, model, orig_score, print_output=False).item()\n",
        "\n",
        "            # If the result is less than the threshold, remove the tuple from the original list\n",
        "            if (100 - new_score) < threshold:\n",
        "                curr_circuit.remove((layer, head))\n",
        "\n",
        "                print(\"\\nRemoved:\", (layer, head))\n",
        "                print(new_score)\n",
        "\n",
        "    return curr_circuit, new_score"
      ],
      "metadata": {
        "id": "p7jLJcMH70vi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}